{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_Papers_with_Code_repo_understanding",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lQWcXBxMsRO"
      },
      "source": [
        "import gzip\n",
        "import pandas as pd \n",
        "\n",
        "with gzip.open('/content/links-between-papers-and-code.json.gz') as f:\n",
        "    papers_df = pd.read_json(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qSaDOUc6N1LP",
        "outputId": "fca6454c-39c6-4dd3-ca5f-b3dce3da1ae7"
      },
      "source": [
        "papers_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_url</th>\n",
              "      <th>paper_title</th>\n",
              "      <th>paper_arxiv_id</th>\n",
              "      <th>paper_url_abs</th>\n",
              "      <th>paper_url_pdf</th>\n",
              "      <th>repo_url</th>\n",
              "      <th>is_official</th>\n",
              "      <th>mentioned_in_paper</th>\n",
              "      <th>mentioned_in_github</th>\n",
              "      <th>framework</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://paperswithcode.com/paper/automatic-pos...</td>\n",
              "      <td>Automatic Post-Editing of Machine Translation:...</td>\n",
              "      <td>None</td>\n",
              "      <td>https://aclanthology.org/D18-1341</td>\n",
              "      <td>https://aclanthology.org/D18-1341.pdf</td>\n",
              "      <td>https://github.com/trangvu/ape-npi</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://paperswithcode.com/paper/deep-transfer...</td>\n",
              "      <td>Deep Transferring Quantization</td>\n",
              "      <td>None</td>\n",
              "      <td>https://www.ecva.net/papers/eccv_2020/papers_E...</td>\n",
              "      <td>https://www.ecva.net/papers/eccv_2020/papers_E...</td>\n",
              "      <td>https://github.com/xiezheng-cs/DTQ</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>pytorch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://paperswithcode.com/paper/batch-bayesia...</td>\n",
              "      <td>Batch Bayesian Optimization via Multi-objectiv...</td>\n",
              "      <td>None</td>\n",
              "      <td>https://icml.cc/Conferences/2018/Schedule?show...</td>\n",
              "      <td>http://proceedings.mlr.press/v80/lyu18a/lyu18a...</td>\n",
              "      <td>https://github.com/Alaya-in-Matrix/MACE</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://paperswithcode.com/paper/semantic-inst...</td>\n",
              "      <td>Semantic Instance Segmentation with a Discrimi...</td>\n",
              "      <td>1708.02551</td>\n",
              "      <td>http://arxiv.org/abs/1708.02551v1</td>\n",
              "      <td>http://arxiv.org/pdf/1708.02551v1.pdf</td>\n",
              "      <td>https://github.com/harryhan618/LaneNet</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>pytorch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://paperswithcode.com/paper/misbehaviour-...</td>\n",
              "      <td>Misbehaviour Prediction for Autonomous Driving...</td>\n",
              "      <td>1910.04443</td>\n",
              "      <td>https://arxiv.org/abs/1910.04443v1</td>\n",
              "      <td>https://arxiv.org/pdf/1910.04443v1.pdf</td>\n",
              "      <td>https://github.com/testingautomated-usi/selfor...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124811</th>\n",
              "      <td>https://paperswithcode.com/paper/full-capacity...</td>\n",
              "      <td>Full-Capacity Unitary Recurrent Neural Networks</td>\n",
              "      <td>1611.00035</td>\n",
              "      <td>http://arxiv.org/abs/1611.00035v1</td>\n",
              "      <td>http://arxiv.org/pdf/1611.00035v1.pdf</td>\n",
              "      <td>https://github.com/stwisdom/urnn</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124812</th>\n",
              "      <td>https://paperswithcode.com/paper/sequential-ne...</td>\n",
              "      <td>Sequential Neural Models with Stochastic Layers</td>\n",
              "      <td>1605.07571</td>\n",
              "      <td>http://arxiv.org/abs/1605.07571v2</td>\n",
              "      <td>http://arxiv.org/pdf/1605.07571v2.pdf</td>\n",
              "      <td>https://github.com/marcofraccaro/srnn</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124813</th>\n",
              "      <td>https://paperswithcode.com/paper/a-simple-appr...</td>\n",
              "      <td>A Simple Approach to Sparse Clustering</td>\n",
              "      <td>1602.07277</td>\n",
              "      <td>http://arxiv.org/abs/1602.07277v2</td>\n",
              "      <td>http://arxiv.org/pdf/1602.07277v2.pdf</td>\n",
              "      <td>https://github.com/victorpu/SAS_Hill_Climb</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124814</th>\n",
              "      <td>https://paperswithcode.com/paper/a-convolution...</td>\n",
              "      <td>A convolutional approach to reflection symmetry</td>\n",
              "      <td>1609.05257</td>\n",
              "      <td>http://arxiv.org/abs/1609.05257v1</td>\n",
              "      <td>http://arxiv.org/pdf/1609.05257v1.pdf</td>\n",
              "      <td>https://github.com/cicconet/SymmetryAxes</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124815</th>\n",
              "      <td>https://paperswithcode.com/paper/gans-trained-...</td>\n",
              "      <td>GANs Trained by a Two Time-Scale Update Rule C...</td>\n",
              "      <td>1706.08500</td>\n",
              "      <td>http://arxiv.org/abs/1706.08500v6</td>\n",
              "      <td>http://arxiv.org/pdf/1706.08500v6.pdf</td>\n",
              "      <td>https://github.com/raahii/video-gans-evaluation</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>pytorch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>124816 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                paper_url  ... framework\n",
              "0       https://paperswithcode.com/paper/automatic-pos...  ...        tf\n",
              "1       https://paperswithcode.com/paper/deep-transfer...  ...   pytorch\n",
              "2       https://paperswithcode.com/paper/batch-bayesia...  ...      none\n",
              "3       https://paperswithcode.com/paper/semantic-inst...  ...   pytorch\n",
              "4       https://paperswithcode.com/paper/misbehaviour-...  ...        tf\n",
              "...                                                   ...  ...       ...\n",
              "124811  https://paperswithcode.com/paper/full-capacity...  ...      none\n",
              "124812  https://paperswithcode.com/paper/sequential-ne...  ...      none\n",
              "124813  https://paperswithcode.com/paper/a-simple-appr...  ...      none\n",
              "124814  https://paperswithcode.com/paper/a-convolution...  ...      none\n",
              "124815  https://paperswithcode.com/paper/gans-trained-...  ...   pytorch\n",
              "\n",
              "[124816 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgnYPR5aPMum",
        "outputId": "f7c8a49f-8e94-446b-8c25-4f5f929eb2b2"
      },
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-o9z6yo13\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-o9z6yo13\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (3.2.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (2.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (0.0)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235 kB 947 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pke==1.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (4.62.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2021.10.8)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-1.8.1-py3-none-any.whl size=8764193 sha256=723b1a1b05812683843a5b635b1f8ff50f65a49859466e8699ca5b8f31526933\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t9azm2nz/wheels/fa/b3/09/612ee93bf3ee4164bcd5783e742942cdfc892a86039d3e0a33\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-1.8.1 unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh_3ggI2PZo2",
        "outputId": "4660eb4c-0824-496e-a8a1-985046df9e47"
      },
      "source": [
        "!pip install nltk "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ-4wVSnOHdK"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "import re\n",
        "import pke"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMvsVszKPLZU",
        "outputId": "1ca8e97d-cb60-4430-cd5a-b7454c2df01f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiLoRHZlPeto",
        "outputId": "655359b6-a51c-4cdf-cf5f-fdb2c8b0cc27"
      },
      "source": [
        "def extract_keyphrases(caption, n):\n",
        "    extractor = pke.unsupervised.TextRank() \n",
        "    extractor.load_document(caption)\n",
        "    extractor.candidate_selection()\n",
        "    extractor.candidate_weighting()\n",
        "    keyphrases = extractor.get_n_best(n=n, stemming=False)\n",
        "    print(keyphrases,\"\\n\")\n",
        "    return(keyphrases)\n",
        "    \n",
        "papers_df['abstract_keyphrases'] = papers_df.apply(lambda row: (extract_keyphrases(row['paper_title'],10)),axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('automatic post - editing', 0.4), ('interpreter approach', 0.20000012), ('neural programmer', 0.20000009000000002), ('machine translation', 0.20000005)] \n",
            "\n",
            "[('deep transferring quantization', 0.9999999999999999)] \n",
            "\n",
            "[('multi - objective acquisition ensemble', 0.41666670666666655), ('automated analog circuit design', 0.33333343333333326), ('batch bayesian optimization', 0.24999999999999994)] \n",
            "\n",
            "[('discriminative loss function', 0.5000000499999999), ('semantic instance segmentation', 0.4999999999999999)] \n",
            "\n",
            "[('autonomous driving systems', 0.6000000300000001), ('misbehaviour prediction', 0.4)] \n",
            "\n",
            "[('automatic moderation', 0.3174311865020752), ('offensive memes', 0.3174311565020752), ('hate speech', 0.3174310865020752), ('detection', 0.023853420246887212), ('pixels', 0.023853400246887212)] \n",
            "\n",
            "[('play language models', 0.4195804395804195), ('text generation', 0.27972038972027974), ('simple approach', 0.27972034972027976), ('plug', 0.020979020979020983)] \n",
            "\n",
            "[('phylogenetic trees', 0.25000011), ('observational data', 0.25000008), ('gene functions', 0.25000005), ('automatic annotation', 0.25000002)] \n",
            "\n",
            "[('attentional generative adversarial networks', 0.9638555022554806), ('generation', 0.312849614316057), ('text', 0.03614464774451953)] \n",
            "\n",
            "[('level scene', 0.48191616827999995), ('robust low', 0.4819160982799999), ('level', 0.24095808913999997), ('high', 0.036167913440000005)] \n",
            "\n",
            "[('high fidelity speech synthesis', 0.6666666666666665), ('adversarial networks', 0.3333333833333333)] \n",
            "\n",
            "[('sne', 0.50000004), ('t', 0.50000002)] \n",
            "\n",
            "[('measurement calculus', 1.00000001)] \n",
            "\n",
            "[('stochastic neighborhood embedding', 0.5825243318446601), ('efficient algorithms', 0.3883495145631066), ('t', 0.029126243592233012)] \n",
            "\n",
            "[('visual question answering', 0.5357144057142799), ('image captioning', 0.357142947142853), ('attention', 0.026785784285716787), ('top', 0.026785754285716785), ('up', 0.026785734285716786), ('bottom', 0.026785714285716786)] \n",
            "\n",
            "[('coherent story visualization', 0.9523809823809526), ('character', 0.04761904761904763)] \n",
            "\n",
            "[('muonic x - rays', 0.43715856994535546), ('dirac equation solver', 0.3278688824590164), ('elemental analysis', 0.21857930497267755), ('mudirac', 0.016393442622950824)] \n",
            "\n",
            "[('english news translation', 0.4285714985714287), ('automatic chinese', 0.2857143257142857), ('human parity', 0.2857142957142857)] \n",
            "\n",
            "[('light gated recurrent units', 0.6666666666666665), ('speech recognition', 0.3333333833333333)] \n",
            "\n",
            "[('mnist dataset classification utilizing k', 0.5464480874316942), ('window metric', 0.21857935497267755), ('nn classifier', 0.21857929497267756), ('sliding', 0.016393542622950823)] \n",
            "\n",
            "[('unsupervised domain adaptation', 0.5000000399999999), ('maximum classifier discrepancy', 0.4999999999999999)] \n",
            "\n",
            "[('visual question answering', 0.5357144057142799), ('image captioning', 0.357142947142853), ('attention', 0.026785784285716787), ('top', 0.026785754285716785), ('up', 0.026785734285716786), ('bottom', 0.026785714285716786)] \n",
            "\n",
            "[('deep bidirectional transformers', 0.3680982195092026), ('pre - training', 0.36809817950920254), ('language understanding', 0.245398873006135), ('bert', 0.018404907975460127)] \n",
            "\n",
            "[('policy trust region method', 0.6349201439374402), ('continuous control', 0.31746015696872015), ('trust', 0.20604404385707908), ('off', 0.023809994546919844), ('pcl', 0.023809964546919842)] \n",
            "\n",
            "[('enriching word vectors', 0.6000000000000001), ('subword information', 0.40000004)] \n",
            "\n",
            "[('global covid-19 scenario analysis', 0.43715854994535547), ('compartmental gaussian processes', 0.32786901245901645), ('policy assessment', 0.21857936497267755), ('lockdown', 0.016393502622950824)] \n",
            "\n",
            "[('textual entailment', 1.00000001)] \n",
            "\n",
            "[('sparse quadratic classification rules', 0.5714285714285715), ('linear dimension reduction', 0.42857147857142863)] \n",
            "\n",
            "[('dynamic eye movement matching', 0.49079756571884947), ('neuro gesture', 0.24539887285942485), ('visualization tool', 0.24539884285942484), ('report', 0.01840490856230069)] \n",
            "\n",
            "[('region proposal networks', 0.3614459031325301), ('time object detection', 0.36144586313253013), ('faster r', 0.24096385542168675), ('real', 0.01807234915662651), ('cnn', 0.01807231915662651)] \n",
            "\n",
            "[('reinforcement learning', 0.8160738519637752), ('policy', 0.061308802678741676), ('value', 0.06130878267874167), ('gap', 0.06130876267874168)] \n",
            "\n",
            "[('motion segmentation', 0.48191615827999995), ('trifocal tensor', 0.4819161282799999), ('usage', 0.03616786344000001)] \n",
            "\n",
            "[('maximum independent set problem', 0.9638554422554806), ('branching', 0.03614460774451953)] \n",
            "\n",
            "[('stereo fusion', 0.46509703192617835), ('deep lidar', 0.4650970019261783), ('aware', 0.03490305807382164), ('noise', 0.034903038073821634)] \n",
            "\n",
            "[('discrete latent variable models', 0.5369128516778525), ('unbiased gradient estimates', 0.4026846237583893), ('variance', 0.02013426818791947), ('low', 0.02013424818791947), ('rebar', 0.02013422818791947)] \n",
            "\n",
            "[('end speech recognition toolkit', 0.6201294125682378), ('non -', 0.31006470128411884), ('end', 0.108746846742745), ('streaming', 0.023268732049214428), ('production', 0.023268712049214428), ('wenet', 0.023268692049214428)] \n",
            "\n",
            "[('person re - identification', 0.4444444944444443), ('guided saliency feature', 0.3333333333333333), ('crowded scenes', 0.2222223222222222)] \n",
            "\n",
            "[('d saliency detection', 0.3333334233333333), ('accurate rgb', 0.2222222822222222), ('stream architecture', 0.22222225222222222), ('asymmetric two', 0.2222222222222222)] \n",
            "\n",
            "[('aware relevance estimation', 0.47619055619047623), ('adaptive video summarization', 0.4761904961904762), ('quality', 0.023809583809523815), ('query', 0.023809523809523815)] \n",
            "\n",
            "[('optimal sample size planning', 0.869398034643053), ('test', 0.03265061133923674), ('whitney', 0.03265059133923674), ('mann', 0.03265057133923674), ('wilcoxon', 0.03265055133923674)] \n",
            "\n",
            "[('multi - alternative decision making', 0.4166667666666666), ('finite sampling capacity', 0.25000002999999993), ('accumulator models', 0.16666673666666665), ('optimal allocation', 0.16666666666666666)] \n",
            "\n",
            "[('derivative free neural network', 0.43715857994535545), ('pairwise profile alignment', 0.32786892245901644), ('dynamic programming', 0.21857927497267757), ('function', 0.016393462622950824)] \n",
            "\n",
            "[('complex number representation', 0.46511642906976747), ('correct planar graph', 0.4651163490697675), ('efficient', 0.023255853953488375), ('slam', 0.023255833953488375), ('cpl', 0.023255813953488375)] \n",
            "\n",
            "[('dynamic convolution', 0.8160738519637752), ('span', 0.061308792678741675), ('bert', 0.06130877267874167), ('convbert', 0.06130874267874167)] \n",
            "\n",
            "[('domain representations', 0.4346991073215265), ('emoji occurrences', 0.4346990473215265), ('sarcasm', 0.03265066133923674), ('emotion', 0.03265064133923674), ('sentiment', 0.03265062133923674), ('millions', 0.03265050133923674)] \n",
            "\n",
            "[('colorectal tissue classification', 0.41958047958041955), ('convolutional networks', 0.27972037972027974), ('stain normalization', 0.2797203097202797), ('importance', 0.020979030979020985)] \n",
            "\n",
            "[('shot object detection', 0.9523809823809526), ('few', 0.04761905761904763)] \n",
            "\n",
            "[('non - count symmetries', 0.6201292825682378), ('graphical models', 0.3100647612841188), ('prob', 0.023268792049214428), ('multi', 0.02326876204921443), ('boolean', 0.02326874204921443)] \n",
            "\n",
            "[('adaptive imagination', 0.9301939438523567), ('optimization', 0.06980613614764326)] \n",
            "\n",
            "[('action recognition', 0.46509705192617834), ('dropgraph module', 0.4650969919261783), ('skeleton', 0.03490309807382164), ('gcn', 0.034903048073821635)] \n",
            "\n",
            "[('categorical reparameterization', 0.8694941647232974), ('softmax', 0.06525296763835123), ('gumbel', 0.06525294763835122)] \n",
            "\n",
            "[('active memory replace attention', 1.0000000099999995)] \n",
            "\n",
            "[('natural gradient boosting', 0.5825242918446601), ('probabilistic prediction', 0.38834957456310665), ('ngboost', 0.02912621359223301)] \n",
            "\n",
            "[('end learning', 0.8694941847232974), ('end', 0.4347470823616487), ('cars', 0.06525299763835123), ('self', 0.06525296763835123)] \n",
            "\n",
            "[('reverse time attention mechanism', 0.5479452954794518), ('interpretable predictive model', 0.41095893410958895), ('healthcare', 0.020548015205479456), ('retain', 0.020547945205479454)] \n",
            "\n",
            "[('collaborative deep reinforcement learning', 0.9999999999999996)] \n",
            "\n",
            "[('graph lowering compiler techniques', 0.6666666866666665), ('neural networks', 0.33333340333333333)] \n",
            "\n",
            "[('minimalistic confocal image simulator', 0.9638554322554806), ('confocalgn', 0.03614459774451953)] \n",
            "\n",
            "[('step delay stochastic gradient descent', 0.684931566849315), ('distributed training', 0.273972722739726), ('sgd', 0.020547965205479454), ('od', 0.020547945205479454)] \n",
            "\n",
            "[('culex mosquito population dynamics', 0.9302216517581678), ('rainfall', 0.03488921412091613), ('effects', 0.03488919412091613)] \n",
            "\n",
            "[('deep regression', 0.50000004), ('comprehensive analysis', 0.50000001)] \n",
            "\n",
            "[('scale dataset', 0.3119368063934057), ('blackbird dataset', 0.3119367463934057), ('aggressive flight', 0.27972040972027973), ('uav perception', 0.27972037972027974), ('large', 0.020979070979020985)] \n",
            "\n",
            "[('brain networks', 0.33333340333333333), ('sample test', 0.3333333733333333), ('kernel two', 0.3333333433333333)] \n",
            "\n",
            "[('time semantic segmentation', 0.6955718325224596), ('bilateral segmentation network', 0.5850329604453935), ('real', 0.028301946792452834), ('bisenet', 0.028301886792452834)] \n",
            "\n",
            "[('online asynchronous', 0.9301939238523567), ('regression', 0.06980610614764327)] \n",
            "\n",
            "[('information interaction profile', 0.6000000000000001), ('choice adoption', 0.40000004)] \n",
            "\n",
            "[('bayesian learning', 0.3333334233333333), ('reliable molecular', 0.3333333733333333), ('benchmark study', 0.3333333433333333), ('learning', 0.16666673666666665)] \n",
            "\n",
            "[('universal sentence encoder', 0.9999999999999999)] \n",
            "\n",
            "[('parallel data', 0.50000003), ('word translation', 0.5)] \n",
            "\n",
            "[('stable gaussian process', 0.41958041958041953), ('lagrange systems', 0.27972036972027975), ('tracking control', 0.2797203197202797), ('euler', 0.020979090979020985)] \n",
            "\n",
            "[('brief review', 0.33333340333333333), ('machine learning', 0.3333333633333333), ('directional statistics', 0.3333333333333333)] \n",
            "\n",
            "[('neural network training', 0.5660378158490562), ('tunable subnetwork', 0.37735849056603776), ('parallelism', 0.028301946792452834), ('model', 0.028301926792452834)] \n",
            "\n",
            "[('encoding variational bayes', 0.9523809723809525), ('auto', 0.04761904761904763)] \n",
            "\n",
            "[('heart failure prediction', 0.3278689724590164), ('electronic health records', 0.3278689024590164), ('medical concept representation', 0.3278688524590164), ('application', 0.016393542622950823)] \n",
            "\n",
            "[('russian named entity recognition', 0.4733689730096304), ('crf model', 0.23668449650481524), ('hybrid bi', 0.23668444650481524), ('task', 0.01775423132691311), ('lstm', 0.01775417132691311), ('application', 0.01775411132691311)] \n",
            "\n",
            "[('end dialog control', 0.35465017602533827), ('hybrid code networks', 0.3225806451612903), ('reinforcement learning', 0.21505393344086027), ('efficient end', 0.2150538234408602), ('supervised', 0.01612918225806452), ('practical', 0.01612907225806452)] \n",
            "\n",
            "[('mscoco image', 0.8694942547232974), ('challenge', 0.06525303763835123), ('lessons', 0.06525295763835123)] \n",
            "\n",
            "[('speech', 0.50000003), ('wavenet', 0.50000001)] \n",
            "\n",
            "[('deep bidirectional transformers', 0.3680982195092026), ('pre - training', 0.36809817950920254), ('language understanding', 0.245398873006135), ('bert', 0.018404907975460127)] \n",
            "\n",
            "[('simple neural network module', 0.6666666766666666), ('relational reasoning', 0.33333339333333334)] \n",
            "\n",
            "[('designing neural network architectures', 0.6666666666666665), ('reinforcement learning', 0.3333333833333333)] \n",
            "\n",
            "[('unmanned aerial vehicle', 0.4878049380487806), ('day object tracking', 0.4878048980487806), ('all', 0.02439024390243903)] \n",
            "\n",
            "[('suspicious news detection using micro blog text', 1.0)] \n",
            "\n",
            "[('free hydrodynamic simulation methods', 0.6201293825682378), ('new class', 0.3100646712841188), ('mesh', 0.023268772049214428), ('accurate', 0.023268752049214428), ('gizmo', 0.023268692049214428)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('relation extraction', 0.48191614827999996), ('deep residual', 0.48191607827999994), ('weakly', 0.03616788344000001)] \n",
            "\n",
            "[('simultaneous deep learning', 0.5504587955963303), ('friendly spaces', 0.3669725270642202), ('clustering', 0.02752305577981652), ('means', 0.027522965779816524), ('k', 0.027522945779816524)] \n",
            "\n",
            "[('quantum internet software', 0.9090909690909091), ('simulator', 0.04545457545454546), ('simulaqron', 0.04545454545454546)] \n",
            "\n",
            "[('multi - label image classification', 0.684931506849315), ('knowledge distillation', 0.27397266273972604), ('detection', 0.020548065205479454), ('weakly', 0.020548035205479456)] \n",
            "\n",
            "[('adaptive coverage', 0.8694942047232974), ('valid', 0.06525293763835123), ('classification', 0.06525291763835123)] \n",
            "\n",
            "[('timit phone recognition task', 0.6504065703319131), ('recurrent dnns', 0.3252032501659565), ('ensembles', 0.024390289502130586)] \n",
            "\n",
            "[('mask r', 0.9301939238523567), ('cnn', 0.06980610614764327)] \n",
            "\n",
            "[('margin gaussian process approach', 0.40000009000000003), ('unsupervised visual domain adaptation', 0.4), ('deep max', 0.20000006)] \n",
            "\n",
            "[('classical computations', 0.32519474665675024), ('hybrid quantum', 0.3251947166567502), ('automatic differentiation', 0.3251946866567502), ('pennylane', 0.024416000029749512)] \n",
            "\n",
            "[('network', 1.0)] \n",
            "\n",
            "[('proximal policy optimization algorithms', 0.9999999999999996)] \n",
            "\n",
            "[('bayesian analysis', 0.4493875473271151), ('multiple classifiers', 0.4493875173271151), ('tutorial', 0.03374177511525659), ('change', 0.033741745115256586), ('time', 0.03374171511525659)] \n",
            "\n",
            "[('feature pyramid networks', 0.6000000000000001), ('object detection', 0.40000004)] \n",
            "\n",
            "[('fast direct methods', 0.6000000000000001), ('gaussian processes', 0.40000004)] \n",
            "\n",
            "[('quantum measurements', 1.00000003)] \n",
            "\n",
            "[('astrophysical neutrino source population', 0.5000000399999999), ('icecube data', 0.25000009), ('bayesian constraints', 0.25)] \n",
            "\n",
            "[('end neural speaker embedding system', 0.8950605237519993), ('deep speaker', 0.3870090815234777), ('end', 0.10896527736226147)] \n",
            "\n",
            "[('scalable online learning algorithms', 0.9302216617581679), ('library', 0.03488922412091613), ('sol', 0.03488919412091613)] \n",
            "\n",
            "[('deep convolutional networks', 0.5825242718446602), ('structured data', 0.38834957456310665), ('graph', 0.02912625359223301)] \n",
            "\n",
            "[('cross - view image synthesis', 0.7142857142857144), ('conditional gans', 0.2857143457142857)] \n",
            "\n",
            "[('connected networks', 0.707893137615131), ('spectral networks', 0.707893097615131), ('graphs', 0.04761911761904763)] \n",
            "\n",
            "[('instance segmentation approach', 0.5000000999999998), ('end lane detection', 0.5000000499999999), ('end', 0.12840841446151133)] \n",
            "\n",
            "[('recurrent memory networks', 0.6000000000000001), ('language modeling', 0.40000004)] \n",
            "\n",
            "[('chemical compound space', 0.4285714985714287), ('quantum mechanics', 0.2857143157142857), ('machine learning', 0.2857142857142857)] \n",
            "\n",
            "[('artificial neural networks', 0.4026847037583893), ('inverse function', 0.2684564858389261), ('generative architecture', 0.26845644583892614), ('oracle', 0.02013427818791947), ('novel', 0.02013425818791947), ('oggn', 0.02013422818791947)] \n",
            "\n",
            "[('stochastic ensemble value expansion', 0.650406560331913), ('efficient reinforcement', 0.3252032701659565), ('sample', 0.024390249502130586)] \n",
            "\n",
            "[('general reinforcement', 0.726864450857648), ('algorithm', 0.05462725982847032), ('play', 0.054627199828470314), ('self', 0.05462717982847032), ('shogi', 0.054627159828470315), ('chess', 0.05462713982847032)] \n",
            "\n",
            "[('sentiment analysis', 0.3174312065020752), ('using sense', 0.31743113650207516), ('lexical resource', 0.3174311065020752), ('ontosensenet', 0.02385347024688721), ('annotations', 0.02385345024688721)] \n",
            "\n",
            "[('end memory networks', 1.00000004), ('end', 0.2567881043628736)] \n",
            "\n",
            "[('dense object detection', 0.6000000300000001), ('focal loss', 0.4)] \n",
            "\n",
            "[('ladder networks', 0.4819161282799999), ('supervised learning', 0.4819160982799999), ('semi', 0.03616784344000001)] \n",
            "\n",
            "[('radial velocity data', 0.9090909790909092), ('planet', 0.045454595454545464), ('evidence', 0.045454565454545466)] \n",
            "\n",
            "[('person re - identification', 0.6504065703319131), ('triplet loss', 0.3252032901659565), ('defense', 0.024390259502130588)] \n",
            "\n",
            "[('multilingual neural machine translation', 0.5000000299999999), ('dynamic vocabulary', 0.25000008), ('transfer learning', 0.25)] \n",
            "\n",
            "[('double higgs production', 0.5497301589076594), ('new physics', 0.3333333733333333), ('double take', 0.33333334333333325)] \n",
            "\n",
            "[('seismogram similarity', 0.46509705192617834), ('invariant measure', 0.46509702192617836), ('path', 0.03490307807382163), ('correlation', 0.034903048073821635)] \n",
            "\n",
            "[('reinforcement learning', 0.33333340333333333), ('scalable alternative', 0.3333333733333333), ('evolution strategies', 0.3333333333333333)] \n",
            "\n",
            "[('3d gaze estimation', 0.4195804495804195), ('shape cues', 0.27972036972027975), ('recurrent cnn', 0.27972027972027974), ('appearance', 0.020979090979020985)] \n",
            "\n",
            "[('general game playing', 0.4878049380487806), ('monte carlo q', 0.4878048780487806), ('learning', 0.02439028390243903)] \n",
            "\n",
            "[('cross fold training', 0.36809824950920256), ('early printed books', 0.36809819950920253), ('ocr accuracy', 0.245398783006135), ('voting', 0.01840503797546013)] \n",
            "\n",
            "[('convolutional neural networks', 0.5660378058490563), ('image classification', 0.37735853056603774), ('tricks', 0.028301906792452834), ('bag', 0.028301886792452834)] \n",
            "\n",
            "[('dimensional cox model', 0.39473694210526317), ('point detection', 0.263157944736842), ('automatic cut', 0.263157914736842), ('genetics', 0.019737002105263197), ('applications', 0.019736982105263198), ('high', 0.019736922105263198), ('binacox', 0.0197368421052632)] \n",
            "\n",
            "[('region proposal networks', 0.3614459031325301), ('time object detection', 0.36144586313253013), ('faster r', 0.24096385542168675), ('real', 0.01807234915662651), ('cnn', 0.01807231915662651)] \n",
            "\n",
            "[('stochastic video generation', 0.9523809523809526), ('learned', 0.04761909761904763)] \n",
            "\n",
            "[('sequential autoencoder', 1.00000001)] \n",
            "\n",
            "[('workload management advisor', 0.5660377958490562), ('cloud databases', 0.37735859056603777), ('learning', 0.028301916792452836), ('wisedb', 0.028301886792452834)] \n",
            "\n",
            "[('lip movements generation', 0.9523809523809526), ('glance', 0.04761909761904763)] \n",
            "\n",
            "[('cross - lingual adaptation', 0.5714285714285715), ('structural correspondence learning', 0.42857147857142863)] \n",
            "\n",
            "[('efficient convolutional neural networks', 0.5594405794405594), ('mobile vision applications', 0.41958048958041955), ('mobilenets', 0.020979020979020983)] \n",
            "\n",
            "[('deep reinforcement learning', 0.6000000300000001), ('continuous control', 0.4)] \n",
            "\n",
            "[('games', 0.25000008), ('reinforcement', 0.25000005), ('framework', 0.25000003), ('openspiel', 0.25)] \n",
            "\n",
            "[('deep neural networks', 0.7669923562421258), ('deep grasp', 0.46511080587908393), ('grasps', 0.163229415516042), ('localization', 0.03488924412091613), ('detection', 0.03488922412091613)] \n",
            "\n",
            "[('scale image recognition', 0.4878049480487806), ('deep convolutional networks', 0.4878048880487806), ('large', 0.02439029390243903)] \n",
            "\n",
            "[('normalizing neural networks', 0.9523809723809525), ('self', 0.04761904761904763)] \n",
            "\n",
            "[('computing optimal transport', 0.6000000300000001), ('multiscale strategies', 0.4)] \n",
            "\n",
            "[('multi - domain image', 0.48076755035179247), ('unified generative adversarial networks', 0.4444444644444443), ('image translation', 0.21132317590734806)] \n",
            "\n",
            "[('aware neural rendering', 0.9523809723809525), ('geometry', 0.04761904761904763)] \n",
            "\n",
            "[('voice activity detection', 0.42857153857142866), ('keyword spotting', 0.2857143657142857), ('end architecture', 0.28571433571428567), ('end', 0.14285715285714284)] \n",
            "\n",
            "[('centric multilingual machine translation', 0.9638554322554806), ('english', 0.03614460774451953)] \n",
            "\n",
            "[('efficient joint neural architecture', 0.5000000499999999), ('hyperparameter search', 0.2500001), ('deep learning', 0.25000002)] \n",
            "\n",
            "[('statistical parametric speech synthesis', 0.3940886699507389), ('task learning framework', 0.2955666224630542), ('generative adversarial networks', 0.2955665524630542), ('multi', 0.014778425123152712)] \n",
            "\n",
            "[('unitary tensor networks', 0.6000000300000001), ('perfect sampling', 0.4)] \n",
            "\n",
            "[('large scale calcium imaging data analysis', 0.60000006), ('structured matrix factorization framework', 0.4000000100000001)] \n",
            "\n",
            "[('natural language processing', 0.4285714885714287), ('deep learning', 0.2857143157142857), ('recent trends', 0.2857142857142857)] \n",
            "\n",
            "[('asymmetric training', 0.4493875273271151), ('image generation', 0.44938749732711514), ('fine', 0.03374175511525659), ('gan', 0.03374173511525659), ('cvae', 0.03374171511525659)] \n",
            "\n",
            "[('3d pose estimation', 0.6000000400000001), ('temporal information', 0.40000001)] \n",
            "\n",
            "[('flux characterization', 0.302976298063488), ('event sample', 0.30297624806348805), ('icecube high', 0.302976198063488), ('data', 0.022768028952384004), ('years', 0.022768008952384004), ('description', 0.022767948952384005), ('energy', 0.022767898952384003)] \n",
            "\n",
            "[('iterative generative modeling', 0.9090909290909092), ('graphs', 0.045454605454545466), ('graphite', 0.04545454545454546)] \n",
            "\n",
            "[('single shot scene text retrieval', 0.9999999999999997)] \n",
            "\n",
            "[('rectangular statistical cartograms', 0.5825242718446602), ('recmap package', 0.38834958456310664), ('r', 0.02912625359223301)] \n",
            "\n",
            "[('scale electron microscopy image segmentation', 0.9433962464150946), ('spark', 0.028301966792452834), ('large', 0.028301886792452834)] \n",
            "\n",
            "[('time semantic segmentation comparative study', 0.9708738264077669), ('real', 0.02912623359223301)] \n",
            "\n",
            "[('deep convolutional generative adversarial networks', 0.7142857542857144), ('unsupervised representation', 0.2857142857142857)] \n",
            "\n",
            "[('stochastic models', 0.9301939638523566), ('likelihood', 0.06980609614764327)] \n",
            "\n",
            "[('multi - scale affinity', 0.43715848994535544), ('3d instance segmentation', 0.3278689524590164), ('sparse convolution', 0.21857930497267755), ('masc', 0.016393442622950824)] \n",
            "\n",
            "[('progressive neural architecture search', 0.9999999999999996)] \n",
            "\n",
            "[('explain entity relationships', 0.6000000200000001), ('knowledge graphs', 0.40000006000000005)] \n",
            "\n",
            "[('structured prediction approach', 0.6000000100000001), ('label ranking', 0.40000005)] \n",
            "\n",
            "[('sample size', 0.24095817913999998), ('sgd training', 0.24095814913999997), ('deepnet hessians', 0.24095807913999998), ('full spectrum', 0.24095804913999996), ('dynamics', 0.018084011720000005), ('scale', 0.018083991720000005)] \n",
            "\n",
            "[('hamiltonian descent methods', 0.9999999999999999)] \n",
            "\n",
            "[('semi - supervised classification', 0.4444444944444443), ('deep generative model', 0.3333333433333333), ('noisy labels', 0.2222223222222222)] \n",
            "\n",
            "[('tensorflow deep neural networks', 0.43715856994535546), ('flexible fpga high', 0.3278688824590164), ('level synthesis', 0.21857930497267755), ('leflow', 0.016393442622950824)] \n",
            "\n",
            "[('negative rules', 0.4493874773271151), ('robust discovery', 0.4493874273271151), ('bases', 0.03374181511525659), ('knowledge', 0.03374179511525659), ('positive', 0.033741745115256586)] \n",
            "\n",
            "[('normalizing neural networks', 0.9523809723809525), ('self', 0.04761904761904763)] \n",
            "\n",
            "[('software design architecture', 0.4195804595804195), ('quantum computing', 0.27972040972027973), ('specific language', 0.27972037972027974), ('domain', 0.020979100979020983)] \n",
            "\n",
            "[('linear systems', 0.4819161182799999), ('quantum algorithm', 0.48191607827999994), ('equations', 0.036167913440000005)] \n",
            "\n",
            "[('fast kernel learning', 0.9523809823809526), ('structure', 0.04761905761904763)] \n",
            "\n",
            "[('sound correspondence patterns', 0.42857145857142864), ('multiple languages', 0.2857143557142857), ('automatic inference', 0.2857142857142857)] \n",
            "\n",
            "[('continuous space', 0.50000004), ('generating sentences', 0.5)] \n",
            "\n",
            "[('multiple causal models', 0.5000000399999999), ('counterfactually fair prediction', 0.4999999999999999)] \n",
            "\n",
            "[('conditional adversarial networks', 0.6000000700000001), ('image translation', 0.40000004), ('image', 0.2)] \n",
            "\n",
            "[('temporal event graph', 1.00000001)] \n",
            "\n",
            "[('word vectors', 0.9301939738523567), ('translation', 0.06980609614764327)] \n",
            "\n",
            "[('image retrieval', 0.50000003), ('neural codes', 0.5)] \n",
            "\n",
            "[('security issues', 0.8160738519637752), ('web', 0.06130885267874167), ('privacy', 0.061308802678741676), ('wallet', 0.06130878267874167)] \n",
            "\n",
            "[('research library', 0.8694942147232975), ('machine', 0.06525294763835122), ('pylearn2', 0.06525291763835123)] \n",
            "\n",
            "[('joint semantic parsers', 0.6000000100000001), ('disjoint data', 0.40000005)] \n",
            "\n",
            "[('power system big data analysis', 0.6134970025153375), ('convolutional neural networks', 0.36809815950920255), ('cnns', 0.018404947975460127)] \n",
            "\n",
            "[('sharing willingness', 0.46509702192617836), ('content preference', 0.4650969919261783), ('d2d', 0.034903128073821635), ('consideration', 0.034903048073821635)] \n",
            "\n",
            "[('shot learning', 0.4819161282799999), ('prototypical networks', 0.48191607827999994), ('few', 0.036167873440000005)] \n",
            "\n",
            "[('person tracking', 0.4650970119261783), ('articulated multi', 0.4650969819261783), ('wild', 0.034903128073821635), ('arttrack', 0.034903038073821634)] \n",
            "\n",
            "[('person pose estimation model', 0.6201293925682378), ('faster multi', 0.3100647212841188), ('stronger', 0.02326874204921443), ('deeper', 0.02326872204921443), ('deepercut', 0.023268692049214428)] \n",
            "\n",
            "[('film trope dataset', 0.9090909590909092), ('pictropes', 0.045454565454545466), ('overview', 0.04545454545454546)] \n",
            "\n",
            "[('neural language modeling', 0.5000000399999999), ('adaptive input representations', 0.4999999999999999)] \n",
            "\n",
            "[('end differentiable proving', 1.00000004), ('end', 0.2567881043628736)] \n",
            "\n",
            "[('deep learning', 0.50000003), ('extractive summarization', 0.5)] \n",
            "\n",
            "[('compact deep convolutional neural network architecture', 0.6349206749206349), ('traffic sign classification', 0.31746046746031753), ('time', 0.015873145873015877), ('real', 0.015873125873015877), ('micronnet', 0.015873015873015876)] \n",
            "\n",
            "[('general kinetic dispersion relation solver', 0.6993007293006994), ('magnetized plasma', 0.27972036972027975), ('pdrk', 0.020979020979020983)] \n",
            "\n",
            "[('binary neutron stars', 0.29126225592233007), ('dirichlet process gaussian', 0.2912621359223301), ('wave observations', 0.1941749372815533), ('mixture model', 0.19417479728155332), ('gravitational', 0.014563266796116506), ('application', 0.014563186796116505)] \n",
            "\n",
            "[('cma evolution strategy', 0.9523809623809526), ('tutorial', 0.04761910761904763)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('inverse problems', 0.9301939238523567), ('asteroseismology', 0.06980610614764327)] \n",
            "\n",
            "[('point convolutional neural network', 0.5479452254794518), ('time video processing', 0.41095899410958897), ('fpga', 0.020548075205479455), ('real', 0.020548015205479456)] \n",
            "\n",
            "[('efficient global optimization', 0.36809819950920253), ('predictive entropy search', 0.36809815950920255), ('box functions', 0.245398873006135), ('black', 0.018404987975460127)] \n",
            "\n",
            "[('visual representation learning', 0.2955666524630542), ('label image database', 0.2955666124630542), ('scale multi', 0.1970444149753695), ('tencent ml', 0.1970443349753695), ('images', 0.14367894801390604), ('large', 0.014778385123152712)] \n",
            "\n",
            "[('visual relationship detection', 0.9090909090909092), ('softmax', 0.04545461545454546), ('language', 0.04545458545454546)] \n",
            "\n",
            "[('batch deep reinforcement', 0.9523809623809526), ('algorithms', 0.04761909761904763)] \n",
            "\n",
            "[('policy deep reinforcement', 0.9523809723809525), ('exploration', 0.04761911761904763)] \n",
            "\n",
            "[('dialogue adaptation', 0.2453970390335145), ('language models', 0.24539700903351452), ('pre -', 0.2453969790335145), ('specific objectives', 0.2453969490335145), ('task', 0.018412283865941848)] \n",
            "\n",
            "[('uncertainty aware control', 0.41958049958041954), ('sepsis treatment', 0.27972039972027973), ('deep reinforcement', 0.2797203197202797), ('cardiovascular', 0.020979030979020985)] \n",
            "\n",
            "[('end memory networks', 1.00000004), ('end', 0.2567881043628736)] \n",
            "\n",
            "[('networks', 0.33333341333333333), ('eigenproblem', 0.3333333733333333), ('sponge', 0.3333333333333333)] \n",
            "\n",
            "[('fake news', 0.48191613827999996), ('stylometric inquiry', 0.48191608827999993), ('hyperpartisan', 0.03616788344000001)] \n",
            "\n",
            "[('adaptive deep learning solution', 0.841886064272775), ('upsampling', 0.03162294114544501), ('point', 0.03162292114544501), ('curvature', 0.03162285114544501), ('pu', 0.03162282114544501), ('cad', 0.03162280114544501)] \n",
            "\n",
            "[('generative adversarial tree search', 0.5714286114285715), ('surprising negative results', 0.42857142857142866)] \n",
            "\n",
            "[('local features', 0.8160738219637753), ('images', 0.06130882267874167), ('net', 0.06130876267874168), ('lf', 0.06130874267874167)] \n",
            "\n",
            "[('non - aligned musical score transcriptions', 0.9756097660975611), ('mv2h', 0.02439032390243906)] \n",
            "\n",
            "[('deep networks', 0.3174311965020752), ('structure learning', 0.3174311665020752), ('simple resource', 0.31743112650207517), ('fast', 0.02385339024688721), ('morphnet', 0.02385337024688721)] \n",
            "\n",
            "[('deep learning', 0.8160738319637753), ('tensorflow', 0.061308832678741675), ('fast', 0.06130876267874168), ('horovod', 0.06130874267874167)] \n",
            "\n",
            "[('imu dead', 0.8694941847232974), ('reckoning', 0.06525296763835123), ('ai', 0.06525291763835123)] \n",
            "\n",
            "[('supervised interest point detection', 0.89877489465423), ('description', 0.03374180511525659), ('self', 0.03374173511525659), ('superpoint', 0.03374171511525659)] \n",
            "\n",
            "[('graph neural networks', 0.9090909690909091), ('feature', 0.04545457545454546), ('superglue', 0.04545454545454546)] \n",
            "\n",
            "[('excitation networks', 0.9301939638523566), ('squeeze', 0.06980607614764327)] \n",
            "\n",
            "[('convolutional block attention module', 0.9638554222554806), ('cbam', 0.03614459774451953)] \n",
            "\n",
            "[('fully convolutional redundant counting', 0.9638554622554806), ('count', 0.16907816681168333), ('ception', 0.03614461774451953)] \n",
            "\n",
            "[('new backbone', 0.8160738019637753), ('cnn', 0.06130885267874167), ('capability', 0.061308832678741675), ('cspnet', 0.06130874267874167)] \n",
            "\n",
            "[('convolutional neural networks', 0.37500005000000003), ('markov random fields', 0.37500001000000005), ('image synthesis', 0.25000009)] \n",
            "\n",
            "[('scalable twin neural networks', 0.650406500331913), ('unbalanced data', 0.32520332016595654), ('classification', 0.024390299502130588)] \n",
            "\n",
            "[('time object detection', 0.9523810423809526), ('real', 0.04761911761904763)] \n",
            "\n",
            "[('type data generation', 0.42857147857142863), ('unsupervised learning', 0.2857143757142857), ('private synthetic', 0.2857142957142857)] \n",
            "\n",
            "[('sgd robust', 0.48191614827999996), ('lowest loss', 0.4819161182799999), ('sample', 0.03616786344000001)] \n",
            "\n",
            "[('intelligent agents', 0.48191613827999996), ('general platform', 0.4819161082799999), ('unity', 0.03616784344000001)] \n",
            "\n",
            "[('recurrent neural networks', 0.9090909490909092), ('trainability', 0.045454565454545466), ('capacity', 0.04545454545454546)] \n",
            "\n",
            "[('convolutional neural networks', 0.5504587855963303), ('longest chains', 0.3669725970642202), ('pruning', 0.027522985779816524), ('graph', 0.027522955779816522), ('lean', 0.027522935779816522)] \n",
            "\n",
            "[('dense visual object descriptors', 0.6342853102240515), ('dense object nets', 0.4951111070548132), ('robotic manipulation', 0.2857144057142857)] \n",
            "\n",
            "[('distributed machine learning', 0.5660378258490563), ('level module', 0.3773585505660378), ('high', 0.028301926792452834), ('tensorflow', 0.028301906792452834)] \n",
            "\n",
            "[('convolutional neural network', 0.5000000599999999), ('ecg arrhythmia classification', 0.4999999999999999)] \n",
            "\n",
            "[('volumetric segmentation', 0.4819161282799999), ('convolutional networks', 0.4819160982799999), ('3d', 0.03616784344000001)] \n",
            "\n",
            "[('multi - label evaluation measures', 0.7717483065399907), ('anti - monotonicity', 0.42235533797211694), ('label rules', 0.2777955727839333), ('multi', 0.07617283451835076)] \n",
            "\n",
            "[('multi - label classification', 0.6666667066666665), ('interpretable rules', 0.3333333433333333)] \n",
            "\n",
            "[('interaction networks', 0.8160737719637753), ('physics', 0.061308832678741675), ('relations', 0.06130881267874167), ('objects', 0.061308792678741675)] \n",
            "\n",
            "[('high performance video object detection', 0.9708737964077669), ('mobiles', 0.029126283592233012)] \n",
            "\n",
            "[('many neural translation', 0.41095903410958895), ('subword sampling', 0.273972632739726), ('transfer learning', 0.273972602739726), ('resource', 0.020548025205479454), ('asymmetric', 0.020548005205479454)] \n",
            "\n",
            "[('multimodal word distributions', 0.9999999999999999)] \n",
            "\n",
            "[('non - linear learning algorithms', 0.6250000400000001), ('feature importance measure', 0.37500000000000006)] \n",
            "\n",
            "[('autonomous vehicles', 0.3174311865020752), ('physical simulation', 0.3174311565020752), ('fidelity visual', 0.31743112650207517), ('high', 0.02385339024688721), ('airsim', 0.02385337024688721)] \n",
            "\n",
            "[('improved variational autoencoders', 0.5825242718446602), ('text modeling', 0.3883495545631066), ('convolutions', 0.02912629359223301)] \n",
            "\n",
            "[('tertiary eye movement classification', 0.6666666666666665), ('hybrid algorithm', 0.33333339333333334)] \n",
            "\n",
            "[('brightness variations', 0.4819161282799999), ('stellar parameters', 0.4819160982799999), ('inference', 0.03616784344000001)] \n",
            "\n",
            "[('unfolded temporal network models', 0.4733689530096304), ('order time', 0.23668450650481523), ('event graphs', 0.23668441650481523), ('second', 0.017754181326913112), ('applications', 0.017754161326913112), ('advances', 0.017754141326913112)] \n",
            "\n",
            "[('age death rates', 0.8000001296341913), ('old', 0.040000110073161746), ('deceleration', 0.04000009007316175), ('evidence', 0.040000070073161746), ('noise', 0.04000005007316175), ('signal', 0.04000002007316175)] \n",
            "\n",
            "[('image captioning', 0.4819161282799999), ('convolutional decoders', 0.4819160982799999), ('cnn+cnn', 0.03616784344000001)] \n",
            "\n",
            "[('deep neural networks', 0.4878049380487806), ('differentiable fluid dynamics', 0.4878048980487806), ('spnets', 0.02439024390243903)] \n",
            "\n",
            "[('bilinear dynamic mode decomposition', 0.6666666666666665), ('quantum control', 0.3333333833333333)] \n",
            "\n",
            "[('region proposal networks', 0.3614459031325301), ('time object detection', 0.36144586313253013), ('faster r', 0.24096385542168675), ('real', 0.01807234915662651), ('cnn', 0.01807231915662651)] \n",
            "\n",
            "[('points', 0.50000002), ('objects', 0.5)] \n",
            "\n",
            "[('deep networks', 0.4346990873215265), ('visual explanations', 0.43469905732152647), ('localization', 0.03265062133923674), ('gradient', 0.03265059133923674), ('cam', 0.03265051133923674), ('grad', 0.03265049133923674)] \n",
            "\n",
            "[('cross - view training', 0.650406560331913), ('supervised sequence', 0.3252032701659565), ('semi', 0.024390249502130586)] \n",
            "\n",
            "[('boosted decision trees', 0.4195804495804195), ('particle physics', 0.27972036972027975), ('fast inference', 0.27972027972027974), ('fpgas', 0.020979090979020985)] \n",
            "\n",
            "[('lingual emotion lexicons', 0.3278689824590164), ('quality multi', 0.21857933497267756), ('novel approach', 0.21857927497267757), ('representation mapping', 0.21857923497267756), ('high', 0.016393522622950824)] \n",
            "\n",
            "[('underwater multi - robot convoying', 0.6993006993006994), ('visual tracking', 0.27972033972027976), ('detection', 0.020979110979020985)] \n",
            "\n",
            "[('cloud images', 0.3100321972772823), ('color spaces', 0.3100320972772823), ('systematic study', 0.31003206727728233), ('sky', 0.02330137605605091), ('segmentation', 0.02330135605605091), ('components', 0.023301326056050908)] \n",
            "\n",
            "[('variational generative stochastic networks', 0.6666666666666665), ('collaborative shaping', 0.3333333833333333)] \n",
            "\n",
            "[('endless range', 0.48191613827999996), ('other people', 0.4819160982799999), ('possibilities', 0.03616793344000001)] \n",
            "\n",
            "[('language understanding', 0.9301939838523566), ('autoregressive', 0.06980610614764327)] \n",
            "\n",
            "[('rummikub problems', 0.9301939538523567), ('complexity', 0.06980608614764326)] \n",
            "\n",
            "[('box photo post - processing framework', 0.9523810023774537), ('white', 0.023809553811273188), ('exposure', 0.023809523811273187)] \n",
            "\n",
            "[('random walk approach', 0.4878048880487806), ('preference ranking', 0.36259443897723714), ('selectional preferences', 0.3625943989772371), ('propagation', 0.024390363902439028)] \n",
            "\n",
            "[('non - linear approximations', 0.6666666966666666), ('quantum splines', 0.3333333333333333)] \n",
            "\n",
            "[('statistical machine translation', 0.41958050958041954), ('rnn encoder', 0.2797203197202797), ('phrase representations', 0.27972028972027974), ('decoder', 0.020979090979020985)] \n",
            "\n",
            "[('focused multi - document summarization', 0.5555556155555554), ('abstractive query', 0.22222225222222222), ('data augmentation', 0.2222222222222222)] \n",
            "\n",
            "[('technical trading strategies', 0.9523809923809525), ('dynamics', 0.047619067619047634)] \n",
            "\n",
            "[('non - strongly convex composite objectives', 0.5825243718445519), ('fast incremental gradient method', 0.38834954456303455), ('support', 0.014563186796206693), ('saga', 0.014563106796206694)] \n",
            "\n",
            "[('deep neural networks', 1.00000001)] \n",
            "\n",
            "[('multi - task learning', 0.4444445344444443), ('nonparametric variable importance', 0.3333333333333333), ('neural network', 0.2222222822222222)] \n",
            "\n",
            "[('class classification', 0.9301939538523567), ('deep', 0.06980607614764327)] \n",
            "\n",
            "[('pre - trained language models', 0.9708738764077669), ('large', 0.029126283592233012)] \n",
            "\n",
            "[('time semantic segmentation network', 0.9302216517581678), ('real', 0.03488921412091613), ('shuffleseg', 0.03488919412091613)] \n",
            "\n",
            "[('monotonic chunkwise attention', 0.9999999999999999)] \n",
            "\n",
            "[('las vegas city', 0.24096399542168673), ('self organized mapping', 0.24096395542168672), ('lake mead', 0.16064275028112454), ('geographic regions', 0.16064262028112455), ('structural change', 0.16064259028112454), ('years', 0.01204841277108434), ('interest', 0.01204827277108434), ('detection', 0.01204819277108434)] \n",
            "\n",
            "[('recurrent deep learning models', 0.6666667066666665), ('neuroimaging data', 0.3333333433333333)] \n",
            "\n",
            "[('vector space', 0.33333339333333334), ('word representations', 0.3333333633333333), ('efficient estimation', 0.3333333333333333)] \n",
            "\n",
            "[('attentive sentence embedding', 0.6000000400000001), ('structured self', 0.40000001)] \n",
            "\n",
            "[('deep convolutional neural networks', 0.4907976257188495), ('offline handwritten signature verification', 0.49079757571884947), ('features', 0.01840491856230069)] \n",
            "\n",
            "[('sparse semidefinite programming relaxations', 0.43715851994535543), ('polynomial optimization problems', 0.3278689524590164), ('noncommuting variables', 0.21857937497267757), ('algorithm', 0.016393442622950824)] \n",
            "\n",
            "[('dense object detection', 0.6000000300000001), ('focal loss', 0.4)] \n",
            "\n",
            "[('stronger', 0.33333339333333334), ('better', 0.3333333533333333), ('yolo9000', 0.3333333333333333)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('stochastic gradient descent', 0.5825242918446601), ('warm restarts', 0.38834957456310665), ('sgdr', 0.02912621359223301)] \n",
            "\n",
            "[('video action recognition', 0.5000000399999999), ('spatiotemporal multiplier networks', 0.4999999999999999)] \n",
            "\n",
            "[('image captioning', 0.3333334533333333), ('visual sentinel', 0.3333334233333333), ('adaptive attention', 0.3333333833333333)] \n",
            "\n",
            "[('semi - analytic resampling', 0.9638554022554806), ('lasso', 0.03614464774451953)] \n",
            "\n",
            "[('mental health records', 0.4545456245454546), ('negation detection method', 0.4545455345454546), ('suicide', 0.022727422727272736), ('risk', 0.022727402727272736), ('misunderstood', 0.022727322727272736), ('notes', 0.022727302727272736)] \n",
            "\n",
            "[('mitigation measures', 0.726864370857648), ('states', 0.05462725982847032), ('hard', 0.05462722982847032), ('covid-19', 0.054627209828470316), ('spreading', 0.05462718982847032), ('effect', 0.05462712982847032)] \n",
            "\n",
            "[('mmd gans', 0.50000004), ('gradient regularizers', 0.50000001)] \n",
            "\n",
            "[('differentiable ensemble kalman filters', 0.9638554222554806), ('auto', 0.03614459774451953)] \n",
            "\n",
            "[('translation matrix', 0.8160737919637753), ('languages', 0.06130885267874167), ('names', 0.06130882267874167), ('bible', 0.061308802678741676)] \n",
            "\n",
            "[('multi - agent actor', 0.4907975457188495), ('competitive environments', 0.24539887285942485), ('mixed cooperative', 0.24539884285942484), ('critic', 0.01840495856230069)] \n",
            "\n",
            "[('recurrent neural network predictions', 0.6666666766666666), ('sentiment analysis', 0.33333339333333334)] \n",
            "\n",
            "[('natural language texts', 0.47619055619047623), ('temporal relation extraction', 0.4761905161904762), ('causal', 0.023809543809523815), ('catena', 0.023809523809523815)] \n",
            "\n",
            "[('wise regression', 0.3174311965020752), ('quality pixel', 0.3174311665020752), ('polarized self', 0.3174310865020752), ('high', 0.02385343024688721), ('attention', 0.023853400246887212)] \n",
            "\n",
            "[('contrastive predictive coding', 0.9523809823809526), ('representation', 0.04761904761904763)] \n",
            "\n",
            "[('hierarchical morphological segmentation', 0.5000000499999999), ('tree structured dirichlet', 0.4999999999999999)] \n",
            "\n",
            "[('coherent text spans', 0.5825243218446602), ('topic dependencies', 0.3883495245631066), ('copulas', 0.029126303592233012)] \n",
            "\n",
            "[('convolutional radio modulation recognition networks', 0.9999999999999997)] \n",
            "\n",
            "[('consistent word embeddings', 0.4878049380487806), ('encoding dictionary definitions', 0.4878048980487806), ('auto', 0.02439024390243903)] \n",
            "\n",
            "[('connected attention propagation', 0.6000000100000001), ('reading comprehension', 0.40000005)] \n",
            "\n",
            "[('deep neural network', 0.5000000399999999), ('tweet sarcasm detection', 0.4999999999999999)] \n",
            "\n",
            "[('competition software manual', 0.5825243218446602), ('simulated car', 0.3883495145631066), ('championship', 0.029126243592233012)] \n",
            "\n",
            "[('abstractive summarization', 0.9301939538523567), ('bottom', 0.06980607614764327)] \n",
            "\n",
            "[('adversarial examples', 0.48191615827999995), ('pronged defense', 0.4819161282799999), ('magnet', 0.03616784344000001)] \n",
            "\n",
            "[('universal sentence representations', 0.5825243318446601), ('evaluation toolkit', 0.3883495445631066), ('senteval', 0.02912621359223301)] \n",
            "\n",
            "[('simplified text pairs', 0.5825243718446601), ('explicit labeling', 0.3883495645631066), ('complex', 0.02912629359223301)] \n",
            "\n",
            "[('large scale multi', 0.33333340333333333), ('task learning', 0.2222223322222222), ('sentence representations', 0.22222226222222222), ('general purpose', 0.2222222322222222)] \n",
            "\n",
            "[('other gradient compression schemes', 0.5714286214285715), ('error feedback fixes', 0.42857142857142866)] \n",
            "\n",
            "[('resolution correspondence networks', 0.9523809723809525), ('dual', 0.04761904761904763)] \n",
            "\n",
            "[('person re - identification', 0.4444445144444443), ('deep neural networks', 0.3333333333333333), ('inexact matching', 0.22222226222222222)] \n",
            "\n",
            "[('salient object detection', 0.6000000000000001), ('fixation prediction', 0.40000005)] \n",
            "\n",
            "[('malware analysis', 0.8694941947232975), ('coverings', 0.06525298763835123), ('mimosa', 0.06525291763835123)] \n",
            "\n",
            "[('3d human pose estimation', 0.6666667066666665), ('temporal information', 0.3333333433333333)] \n",
            "\n",
            "[('time series forecasting', 0.3000001100000001), ('recurrent neural networks', 0.3000000700000001), ('exponential smoothing', 0.20000004000000002), ('hybrid method', 0.20000001)] \n",
            "\n",
            "[('deep forest', 1.0)] \n",
            "\n",
            "[('supervised learning', 0.32519475665675024), ('precision oncology', 0.3251947066567502), ('key sentences', 0.3251946766567502), ('semi', 0.024416070029749513)] \n",
            "\n",
            "[('natural language inference data', 0.4444445144444443), ('universal sentence representations', 0.3333333633333333), ('supervised learning', 0.2222222222222222)] \n",
            "\n",
            "[('automatic false friends detection', 0.5479452554794518), ('high coverage method', 0.41095891410958896), ('portuguese', 0.020548065205479454), ('spanish', 0.020548045205479454)] \n",
            "\n",
            "[('markov reward processes', 0.4285714885714287), ('discounted values', 0.2857143157142857), ('loop estimator', 0.2857142857142857)] \n",
            "\n",
            "[('hat simulation environment', 0.8333333633290623), ('agents', 0.04166678666773442), ('learning', 0.04166675666773442), ('reinforcement', 0.04166673666773442), ('chef', 0.04166667666773442)] \n",
            "\n",
            "[('deep video', 1.0)] \n",
            "\n",
            "[('reinforcement learning environments', 0.5825243418446602), ('python toolbox', 0.3883495445631066), ('apes', 0.02912621359223301)] \n",
            "\n",
            "[('learning optical flow', 0.6000000200000001), ('convolutional networks', 0.40000006000000005)] \n",
            "\n",
            "[('mb model size', 0.3614459131325301), ('50x fewer parameters', 0.36144585313253014), ('level accuracy', 0.24096389542168675), ('alexnet', 0.01807230915662651), ('squeezenet', 0.01807228915662651)] \n",
            "\n",
            "[('deep reinforcement learning', 0.4285714885714287), ('variance reduction', 0.2857143157142857), ('reward estimation', 0.2857142857142857)] \n",
            "\n",
            "[('variance data', 0.42094308713638745), ('error analysis', 0.42094299713638744), ('bayesian', 0.03162296114544501), ('cautions', 0.03162292114544501), ('high', 0.03162287114544501), ('sample', 0.03162285114544501), ('small', 0.03162283114544501)] \n",
            "\n",
            "[('graphical astrophysics code', 0.41095892410958895), ('lagrangian fluids', 0.273972712739726), ('body dynamics', 0.273972682739726), ('n', 0.020548005205479454), ('gandalf', 0.020547945205479454)] \n",
            "\n",
            "[('natural langevin dynamics', 0.6000000000000001), ('neural networks', 0.40000004)] \n",
            "\n",
            "[('non - aligned incomplete multi - view', 0.8560693255309273), ('multi - label learning', 0.489931479927412), ('effective model', 0.1970443649753695), ('concise', 0.014778335123152712)] \n",
            "\n",
            "[('future trajectory prediction', 0.4195804595804195), ('inverse reinforcement', 0.27972035972027975), ('neural networks', 0.27972028972027974), ('framework', 0.020979130979020984)] \n",
            "\n",
            "[('neural gpus', 0.9301939238523567), ('algorithms', 0.06980610614764327)] \n",
            "\n",
            "[('k_{33}$-free ising models', 0.9090909590909092), ('sampling', 0.045454565454545466), ('inference', 0.04545454545454546)] \n",
            "\n",
            "[('unsupervised monocular depth estimation', 0.650406500331913), ('right consistency', 0.32520332016595654), ('left', 0.024390299502130588)] \n",
            "\n",
            "[('box adversarial example generation', 0.650406520331913), ('normalizing flows', 0.32520332016595654), ('black', 0.024390249502130586)] \n",
            "\n",
            "[('baseline temporal tagger', 0.9523809623809526), ('languages', 0.04761910761904763)] \n",
            "\n",
            "[('dual generator generative adversarial networks', 0.5886939571150019), ('multi - domain image', 0.4807675403517925), ('image translation', 0.21132316590734804)] \n",
            "\n",
            "[('generating structured queries', 0.4195804395804195), ('reinforcement learning', 0.27972036972027975), ('natural language', 0.27972033972027976), ('seq2sql', 0.020979020979020983)] \n",
            "\n",
            "[('relative positional encodings', 0.41958048958041955), ('slot filling', 0.27972038972027974), ('aware self', 0.27972029972027973), ('position', 0.20429305320639185), ('attention', 0.020979070979020985)] \n",
            "\n",
            "[('anticausal learning', 0.9301939538523567), ('causal', 0.06980608614764326)] \n",
            "\n",
            "[('wide residual networks', 0.9999999999999999)] \n",
            "\n",
            "[('binarized neural network optimization', 0.6666667366666665), ('latent weights', 0.3333333333333333)] \n",
            "\n",
            "[('stable vector representation', 0.5000000399999999), ('persistent homology', 0.3715916755384886), ('persistence images', 0.3715915955384886)] \n",
            "\n",
            "[('multiple random projections', 0.6000000400000001), ('gan training', 0.40000001)] \n",
            "\n",
            "[('graph cut computer vision approach', 0.45454556454545464), ('transparent vessels', 0.18181825181818181), ('material boundaries', 0.18181822181818183), ('liquid level', 0.18181819181818182)] \n",
            "\n",
            "[('box functions', 0.3174311965020752), ('expensive black', 0.3174311665020752), ('parallel optimization', 0.31743113650207516), ('procedure', 0.023853400246887212), ('blackbox', 0.02385337024688721)] \n",
            "\n",
            "[('hierarchical dice loss', 0.3278689724590164), ('convolutional neural networks', 0.32786892245901644), ('brain tumor segmentation', 0.3278688524590164), ('refined', 0.016393492622950825)] \n",
            "\n",
            "[('hierarchical optimal transport', 0.6000000000000001), ('document representation', 0.40000004)] \n",
            "\n",
            "[('fast underwater image enhancement', 0.5714285714285715), ('improved visual perception', 0.42857147857142863)] \n",
            "\n",
            "[('random directions stochastic approximation', 0.5714286114285715), ('adaptive system optimization', 0.42857142857142866)] \n",
            "\n",
            "[('rare events', 1.00000003)] \n",
            "\n",
            "[('emergency blood donation request', 0.9302216317581679), ('twitter', 0.03488926412091613), ('identification', 0.03488919412091613)] \n",
            "\n",
            "[('actionable representations', 0.8694941747232975), ('policies', 0.06525298763835123), ('goal', 0.06525295763835123)] \n",
            "\n",
            "[('video object segmentation', 0.4285714985714287), ('evaluation methodology', 0.2857143257142857), ('benchmark dataset', 0.2857142957142857)] \n",
            "\n",
            "[('human pose estimation', 0.5825243418446602), ('stage networks', 0.3883495545631066), ('multi', 0.02912623359223301)] \n",
            "\n",
            "[('motivated goal exploration', 0.6000000100000001), ('automatic curriculum', 0.40000006000000005)] \n",
            "\n",
            "[('domain separation networks', 0.9999999999999999)] \n",
            "\n",
            "[('time anchor annotation', 0.6000000400000001), ('temporal relations', 0.40000001)] \n",
            "\n",
            "[('semantic image segmentation', 0.6000000400000001), ('atrous convolution', 0.40000001)] \n",
            "\n",
            "[('aware geodesic video object segmentation', 0.9708738064077669), ('saliency', 0.02912621359223301)] \n",
            "\n",
            "[('differentiable architecture search', 0.9523809723809525), ('darts', 0.04761904761904763)] \n",
            "\n",
            "[('hadamard estimator', 0.48191613827999996), ('robust inference', 0.48191607827999994), ('heteroskedasticity', 0.036167873440000005)] \n",
            "\n",
            "[('manifold attack', 1.0)] \n",
            "\n",
            "[('neural network architectures', 0.6000000300000001), ('bayesian learning', 0.4)] \n",
            "\n",
            "[('rank random matrices', 0.4878049480487806), ('complex quadratic systems', 0.4878048880487806), ('full', 0.02439029390243903)] \n",
            "\n",
            "[('semantic signatures', 0.48191613827999996), ('image provenance', 0.4819161082799999), ('proves', 0.03616784344000001)] \n",
            "\n",
            "[('conditional generative adversarial network', 0.4907976257188495), ('aware low dose ct', 0.49079756571884947), ('sharpness', 0.01840490856230069)] \n",
            "\n",
            "[('aligned pose attention transfer', 0.5594405794405594), ('person image generation', 0.41958048958041955), ('progressive', 0.020979020979020983)] \n",
            "\n",
            "[('memory augmented distant supervision', 0.5000000699999999), ('temporal reasoning', 0.25000003), ('relation extraction', 0.25)] \n",
            "\n",
            "[('scale cross - modal visual localization', 0.975609816097561), ('large', 0.02439028390243906)] \n",
            "\n",
            "[('sparse gaussian graphical models', 0.6666667066666665), ('bayesian structure', 0.3333333333333333)] \n",
            "\n",
            "[('efficient video recognition', 0.7164131038153893), ('mobile video networks', 0.7164130638153893), ('movinets', 0.02912621359223301)] \n",
            "\n",
            "[('video object segmentation', 0.8000000996341913), ('merging', 0.04000008007316175), ('refinement', 0.04000006007316175), ('generation', 0.04000004007316175), ('proposal', 0.04000002007316175), ('premvos', 0.04000000007316175)] \n",
            "\n",
            "[('multilayer annotated corpora', 0.8333334433290622), ('converter', 0.04166675666773442), ('tool', 0.04166673666773442), ('graph', 0.04166669666773442), ('discoursegraphs', 0.04166666666773442)] \n",
            "\n",
            "[('deep learning', 0.3333334333333333), ('model uncertainty', 0.33333340333333333), ('bayesian approximation', 0.3333333633333333)] \n",
            "\n",
            "[('mÃ¶bius domain wall fermion', 0.42328045328042335), ('omega baryon mass', 0.31746046746031753), ('hisq action', 0.21164032164021154), ('flow', 0.015873235873015874), ('gradient', 0.015873095873015875), ('scale', 0.015873015873015876)] \n",
            "\n",
            "[('multiple intermediate frames', 0.3000000700000001), ('high quality estimation', 0.3000000300000001), ('video interpolation', 0.20000011), ('super slomo', 0.2)] \n",
            "\n",
            "[('neural machine translation toolkit', 1.0000000199999997)] \n",
            "\n",
            "[('keypoint localization', 0.8694941847232974), ('transformer', 0.06525296763835123), ('transpose', 0.06525291763835123)] \n",
            "\n",
            "[('semantic visual navigation', 0.6000000000000001), ('youtube videos', 0.40000005)] \n",
            "\n",
            "[('sound event detection', 0.5504587155963303), ('frequency segmentation', 0.36697253706422023), ('data', 0.027523045779816523), ('weakly', 0.027523025779816523), ('time', 0.027522975779816522)] \n",
            "\n",
            "[('deep reinforcement learning', 0.5000000399999999), ('dueling network architectures', 0.4999999999999999)] \n",
            "\n",
            "[('scale image recognition', 0.4878049480487806), ('deep convolutional networks', 0.4878048880487806), ('large', 0.02439029390243903)] \n",
            "\n",
            "[('glass control', 0.4493875273271151), ('efficient training', 0.4493874273271151), ('spin', 0.03374179511525659), ('models', 0.03374177511525659), ('energy', 0.033741745115256586)] \n",
            "\n",
            "[('rotation feature decoupling', 0.9090909690909091), ('representation', 0.04545457545454546), ('self', 0.04545454545454546)] \n",
            "\n",
            "[('entity type tagging', 0.5825243318446601), ('dependent fine', 0.3883495345631066), ('context', 0.02912621359223301)] \n",
            "\n",
            "[('uniform manifold approximation', 0.5660377558490562), ('dimension reduction', 0.3773585705660378), ('projection', 0.028301946792452834), ('umap', 0.028301886792452834)] \n",
            "\n",
            "[('multivariate time series forecasting', 0.5714286114285715), ('temporal pattern attention', 0.42857142857142866)] \n",
            "\n",
            "[('generative networks', 1.00000001)] \n",
            "\n",
            "[('unsupervised deep embedding', 0.9523809523809526), ('analysis', 0.04761909761904763)] \n",
            "\n",
            "[('structured prediction models', 0.6000000500000001), ('smoother way', 0.40000001)] \n",
            "\n",
            "[('robust principal graphs', 0.6000000000000001), ('data approximation', 0.40000004)] \n",
            "\n",
            "[('ensemble feature selection', 0.9090909290909092), ('stability', 0.045454605454545466), ('efsis', 0.04545454545454546)] \n",
            "\n",
            "[('robust blind deconvolution', 0.6000000000000001), ('mirror descent', 0.40000004)] \n",
            "\n",
            "[('correct dnn object detector predictions', 0.9433963164150946), ('interpret', 0.028301956792452836), ('framework', 0.028301936792452836)] \n",
            "\n",
            "[('eigenvalue models', 0.4346991573215265), ('data transformation', 0.43469906732152647), ('dimension', 0.03265059133923674), ('high', 0.03265057133923674), ('classifier', 0.03265052133923674), ('distance', 0.03265049133923674)] \n",
            "\n",
            "[('domain questions', 0.8694942247232974), ('open', 0.06525295763835123), ('wikipedia', 0.06525292763835122)] \n",
            "\n",
            "[('scale neural patch synthesis', 0.6349201539374402), ('resolution image', 0.31746005696872015), ('multi', 0.023810004546919842), ('high', 0.023809944546919842)] \n",
            "\n",
            "[('graph neural networks', 0.9090909690909091), ('feature', 0.04545457545454546), ('superglue', 0.04545454545454546)] \n",
            "\n",
            "[('massive unlabeled data', 0.5660377858490563), ('face recognition', 0.37735858056603777), ('propagation', 0.028301916792452836), ('consensus', 0.028301886792452834)] \n",
            "\n",
            "[('lightweight convolutional neural network', 0.5714286014285715), ('optical flow estimation', 0.42857150857142867)] \n",
            "\n",
            "[('fly stellar parameter determination', 0.3940887294497779), ('optical spectra', 0.19704451472488899), ('flux ratios', 0.197044484724889), ('fgk stars', 0.197044444724889), ('athos', 0.014778326375555087)] \n",
            "\n",
            "[('minority language twitter', 0.41095890410958896), ('irish tweets', 0.273972732739726), ('speech tagging', 0.273972682739726), ('analysis', 0.020548055205479455), ('part', 0.020547985205479454)] \n",
            "\n",
            "[('deep multi - level network', 0.7142857242857145), ('saliency prediction', 0.2857143557142857)] \n",
            "\n",
            "[('quantum adiabatic machine', 0.9523809523809526), ('zooming', 0.04761909761904763)] \n",
            "\n",
            "[('large scale training', 0.5825242818446602), ('collaborative filtering', 0.38834958456310664), ('autoencoders', 0.029126263592233012)] \n",
            "\n",
            "[('end learning', 0.8694941847232974), ('end', 0.4347470823616487), ('cars', 0.06525299763835123), ('self', 0.06525296763835123)] \n",
            "\n",
            "[('relation classification', 0.32519476665675023), ('entity information', 0.32519473665675025), ('language model', 0.3251947066567502), ('pre', 0.024416010029749514)] \n",
            "\n",
            "[('chest x - rays', 0.36363644363636366), ('normal image synthesis', 0.2727274027272727), ('disease decomposition', 0.18181823181818182), ('generative model', 0.1818182018181818)] \n",
            "\n",
            "[('deep residual networks', 0.6000000300000001), ('identity mappings', 0.4)] \n",
            "\n",
            "[('neural network', 0.4346991273215265), ('deep learning', 0.4346990173215265), ('predictions', 0.03265065133923674), ('prototypes', 0.03265057133923674), ('reasoning', 0.03265055133923674), ('case', 0.03265052133923674)] \n",
            "\n",
            "[('compressing text classification models', 0.9638554222554806), ('fasttext.zip', 0.03614459774451953)] \n",
            "\n",
            "[('lexical relation learning', 0.5357144757142798), ('vector differences', 0.357143017142853), ('utility', 0.026785854285716785), ('book', 0.026785794285716785), ('goose', 0.026785774285716785), ('gaggle', 0.026785754285716785)] \n",
            "\n",
            "[('high performance video object detection', 0.9708737964077669), ('mobiles', 0.029126283592233012)] \n",
            "\n",
            "[('robust accelerated mri reconstruction', 0.4301076368817204), ('driven data augmentations', 0.3225806851612903), ('consistency training', 0.2150538434408603), ('physics', 0.01612905225806452), ('vortex', 0.01612903225806452)] \n",
            "\n",
            "[('temporal action localization', 0.9523810023809526), ('autoloc', 0.04761904761904763)] \n",
            "\n",
            "[('priority queue training', 0.5000000399999999), ('neural program synthesis', 0.4999999999999999)] \n",
            "\n",
            "[('accurate 3d localization', 0.41958041958041953), ('imu fusion', 0.27972036972027975), ('mav swarms', 0.2797203197202797), ('uwb', 0.020979090979020985)] \n",
            "\n",
            "[('efficient hierarchical reinforcement learning', 0.9638554222554806), ('data', 0.03614459774451953)] \n",
            "\n",
            "[('realistic single image super - resolution', 0.6557377249180331), ('generative adversarial network', 0.3278689524590164), ('photo', 0.016393442622950824)] \n",
            "\n",
            "[('probabilistic sentential decision', 0.3333333633333333), ('world assumption', 0.2222223322222222), ('partial closed', 0.22222230222222222), ('structural learning', 0.2222222222222222)] \n",
            "\n",
            "[('digital communication', 0.9301939538523567), ('conversation', 0.06980608614764326)] \n",
            "\n",
            "[('person pose estimation', 0.5357143457142798), ('lightweight openpose', 0.357142977142853), ('cpu', 0.026785814285716785), ('multi', 0.026785754285716785), ('time', 0.026785734285716786), ('real', 0.026785714285716786)] \n",
            "\n",
            "[('low dimensional landscape hypothesis', 0.6349200739374402), ('tiny subspaces', 0.31746015696872015), ('dnns', 0.023810014546919844), ('true', 0.023809994546919844)] \n",
            "\n",
            "[('curriculum search', 0.32519477665675023), ('efficient nas', 0.32519474665675024), ('space explosion', 0.3251947066567502), ('curse', 0.024416020029749512)] \n",
            "\n",
            "[('failure patients ehr clinical features', 0.38022816688212924), ('machine learning model predictions', 0.30418265950570345), ('shap interpretation', 0.15209134475285171), ('understanding heart', 0.1520912547528517), ('tree', 0.01140696410646388)] \n",
            "\n",
            "[('aware margin loss', 0.5660378158490562), ('imbalanced datasets', 0.37735850056603776), ('distribution', 0.028301946792452834), ('label', 0.028301926792452834)] \n",
            "\n",
            "[('temporal adjacent networks', 0.42857144857142865), ('natural language', 0.2857143757142857), ('moment localization', 0.2857143457142857)] \n",
            "\n",
            "[('inverse continuous wavelet transform', 0.49079758571884946), ('admissibility condition', 0.24539890285942484), ('computational implementation', 0.24539877285942485), ('requirement', 0.01840500856230069)] \n",
            "\n",
            "[('multigrid predictive filter flow', 0.9302216117581679), ('videos', 0.03488927412091613), ('learning', 0.03488925412091613)] \n",
            "\n",
            "[('predictive filter flow', 0.6000000300000001), ('image reconstruction', 0.4)] \n",
            "\n",
            "[('piecewise linear neural network verification', 0.7142857542857144), ('unified view', 0.2857142957142857)] \n",
            "\n",
            "[('new benchmark', 0.8694942247232974), ('rl', 0.06525302763835122), ('generalization', 0.06525300763835122)] \n",
            "\n",
            "[('human knowledge', 0.8694942247232974), ('cube', 0.06525295763835123), ('rubik', 0.06525293763835123)] \n",
            "\n",
            "[('aware style transfer', 0.5825243218446602), ('adaptive convolutions', 0.3883495145631066), ('structure', 0.029126243592233012)] \n",
            "\n",
            "[('density matrix renormalization group', 0.4444445244444443), ('linear rotor chains', 0.3333333633333333), ('ground states', 0.2222222222222222)] \n",
            "\n",
            "[('exploiting spatiotemporal dynamics', 0.5357143657142798), ('activity recognition', 0.357142977142853), ('inception', 0.026785774285716785), ('temporal', 0.026785754285716785), ('lstm', 0.026785734285716786), ('ts', 0.026785714285716786)] \n",
            "\n",
            "[('variable fidelity data', 0.5000000399999999), ('gaussian process classification', 0.4999999999999999)] \n",
            "\n",
            "[('augmented computer systems', 0.5825243518446601), ('open platform', 0.3883495445631066), ('park', 0.02912621359223301)] \n",
            "\n",
            "[('image transformations', 0.8694942047232974), ('invariance', 0.06525293763835123), ('confidence', 0.06525291763835123)] \n",
            "\n",
            "[('obliquely reflected diffusions', 0.6000000300000001), ('penalty method', 0.4)] \n",
            "\n",
            "[('cost volume', 0.4209431371363874), ('optical flow', 0.42094305713638747), ('warping', 0.03162291114544501), ('pyramid', 0.03162289114544501), ('cnns', 0.03162284114544501), ('net', 0.03162282114544501), ('pwc', 0.03162280114544501)] \n",
            "\n",
            "[('hierarchical representations', 0.50000004), ('poincarÃ© embeddings', 0.5)] \n",
            "\n",
            "[('end learning steers', 0.4878049980487806), ('deep neural network', 0.48780490804878057), ('end', 0.12521060907154347), ('car', 0.024390403902439028)] \n",
            "\n",
            "[('dense object detection', 0.6000000300000001), ('focal loss', 0.4)] \n",
            "\n",
            "[('proximal policy optimization algorithms', 0.9999999999999996)] \n",
            "\n",
            "[('pair image translation', 0.5660378558490563), ('decoder alignment', 0.3773585605660378), ('encoder', 0.028301936792452836), ('networks', 0.028301916792452836)] \n",
            "\n",
            "[('person re - identification', 0.4000000800000001), ('efficient feature ensemble', 0.3000000400000001), ('virtual cnn branching', 0.3000000000000001)] \n",
            "\n",
            "[('person re - identification', 0.6504065703319131), ('triplet loss', 0.3252032901659565), ('defense', 0.024390259502130588)] \n",
            "\n",
            "[('deep mixed effect model', 0.48192737466229213), ('reliable prediction', 0.24096379733114598), ('gaussian processes', 0.24096373733114598), ('healthcare', 0.018072765337708), ('personalized', 0.018072715337708004)] \n",
            "\n",
            "[('machine learning software', 0.5504587455963303), ('api design', 0.3669724770642202), ('project', 0.027523065779816523), ('scikit', 0.02752303577981652), ('experiences', 0.027523005779816524)] \n",
            "\n",
            "[('time series', 0.9301939538523567), ('attacks', 0.06980608614764326)] \n",
            "\n",
            "[('spatially localized atlas network tiles enables', 0.5454545454545456), ('whole brain segmentation', 0.2727273427272727), ('limited data', 0.18181829181818182)] \n",
            "\n",
            "[('deep neural networks', 0.5000000499999999), ('learning activation functions', 0.4999999999999999)] \n",
            "\n",
            "[('relation extraction', 0.50000004), ('prototypical representation', 0.5)] \n",
            "\n",
            "[('dynamic shifting network', 0.6000000600000001), ('panoptic segmentation', 0.40000003)] \n",
            "\n",
            "[('semantic image segmentation', 0.47619055619047623), ('atrous separable convolution', 0.4761905161904762), ('decoder', 0.023809543809523815), ('encoder', 0.023809523809523815)] \n",
            "\n",
            "[('sparse multiway decomposition', 0.5504587155963303), ('diffusion imaging', 0.3669725570642202), ('tractography', 0.027523045779816523), ('modeling', 0.027522995779816522), ('analysis', 0.027522975779816522)] \n",
            "\n",
            "[('efficient implementation', 0.8694941847232974), ('densenets', 0.06525296763835123), ('memory', 0.06525291763835123)] \n",
            "\n",
            "[('proximal policy optimization algorithms', 0.9999999999999996)] \n",
            "\n",
            "[('deep learning era', 0.5825243318446601), ('unreasonable effectiveness', 0.3883495245631066), ('data', 0.02912625359223301)] \n",
            "\n",
            "[('level convolutional networks', 0.5825242918446601), ('text classification', 0.38834957456310665), ('character', 0.02912621359223301)] \n",
            "\n",
            "[('general purpose atomic crosschain transactions', 0.9999999999999997)] \n",
            "\n",
            "[('spell', 0.50000004), ('attend', 0.50000002)] \n",
            "\n",
            "[('end 3d scene reconstruction', 0.9302216717581678), ('end', 0.163229365516042), ('images', 0.03488931412091613), ('atlas', 0.03488919412091613)] \n",
            "\n",
            "[('source 3d printed humanoid robot', 0.684931596849315), ('sized open', 0.27397266273972604), ('adult', 0.020547985205479454), ('op2x', 0.020547965205479454)] \n",
            "\n",
            "[('modeling regions', 0.46509704192617835), ('spatial descriptors', 0.4650970119261783), ('space', 0.03490314807382163), ('misty', 0.03490305807382164)] \n",
            "\n",
            "[('single shot multibox detector', 0.9638554222554806), ('ssd', 0.03614459774451953)] \n",
            "\n",
            "[('filter pruning', 0.46509704192617835), ('compact convnets', 0.4650969719261783), ('sparsity', 0.03490309807382164), ('structure', 0.03490307807382163)] \n",
            "\n",
            "[('pooled word vectors', 0.5660378958490563), ('fuzzy sets', 0.37735860056603776), ('max', 0.028301976792452836), ('average', 0.028301926792452834)] \n",
            "\n",
            "[('fewer pilots', 0.20000013), ('more antennas', 0.20000010000000001), ('1-bit adcs', 0.20000006), ('massive mimo', 0.20000003000000002), ('deep learning', 0.2)] \n",
            "\n",
            "[('inverse dynamics learning', 0.5825243318446601), ('dependent losses', 0.3883495445631066), ('state', 0.029126223592233012)] \n",
            "\n",
            "[('time series forecasting', 0.37500008000000007), ('deep learning architectures', 0.37500004000000003), ('experimental review', 0.25000001)] \n",
            "\n",
            "[('accurate scene', 0.4493875173271151), ('augmented network', 0.4493874673271151), ('fast', 0.033741785115256585), ('attention', 0.03374173511525659), ('attanet', 0.03374171511525659)] \n",
            "\n",
            "[('single shot multibox detector', 0.9638554222554806), ('ssd', 0.03614459774451953)] \n",
            "\n",
            "[('explicit discourse relations', 0.5825243318446601), ('sentence representation', 0.3883495345631066), ('dissent', 0.02912621359223301)] \n",
            "\n",
            "[('parsimonious pixel labeling', 0.5000000599999999), ('wise attentional gating', 0.5000000199999999), ('pixel', 0.24318319107697725)] \n",
            "\n",
            "[('level pneumonia detection', 0.40268460375838927), ('deep learning', 0.2684565058389261), ('chest x', 0.26845645583892613), ('rays', 0.02013433818791947), ('radiologist', 0.02013424818791947), ('chexnet', 0.02013422818791947)] \n",
            "\n",
            "[('mars express spacecraft', 0.4878049680487806), ('thermal power consumption', 0.48780491804878057), ('machine', 0.02439024390243903)] \n",
            "\n",
            "[('random erasing data augmentation', 0.9999999999999996)] \n",
            "\n",
            "[('end face alignment', 0.37500013000000004), ('mnemonic descent method', 0.37500000000000006), ('recurrent process', 0.25000005), ('end', 0.09632701419129755)] \n",
            "\n",
            "[('categorical features', 0.8694942147232975), ('unbiased', 0.06525293763835123), ('catboost', 0.06525291763835123)] \n",
            "\n",
            "[('information cascade analysis', 0.5504587455963303), ('recent advances', 0.3669725970642202), ('predictions', 0.027523025779816523), ('models', 0.027523005779816524), ('survey', 0.027522945779816524)] \n",
            "\n",
            "[('intelligent agents', 0.48191613827999996), ('general platform', 0.4819161082799999), ('unity', 0.03616784344000001)] \n",
            "\n",
            "[('large scale', 0.24539706903351452), ('urban environments', 0.2453970290335145), ('satellite imagery', 0.24539696903351452), ('convolutional networks', 0.2453969390335145), ('patterns', 0.018412363865941847)] \n",
            "\n",
            "[('free', 0.50000003), ('training', 0.50000001)] \n",
            "\n",
            "[('offline bilingual word vectors', 0.650406500331913), ('orthogonal transformations', 0.3252033001659565), ('softmax', 0.024390349502130586)] \n",
            "\n",
            "[('machine translation', 0.8694942147232975), ('languages', 0.06525294763835122), ('similarities', 0.06525292763835122)] \n",
            "\n",
            "[('parsing toolkit', 0.48191614827999996), ('universal graph', 0.4819161082799999), ('uniparse', 0.03616784344000001)] \n",
            "\n",
            "[('disease biomedical ontologies', 0.6000000300000001), ('multilingual enrichment', 0.4)] \n",
            "\n",
            "[('adversarial attacks', 0.46509704192617835), ('machine learning', 0.4650969719261783), ('defence', 0.03490309807382164), ('cryptography', 0.03490307807382163)] \n",
            "\n",
            "[('accurate neural machine translation', 0.48192745466229214), ('dull hardware', 0.240963717331146), ('sharp models', 0.24096368733114598), ('cpu', 0.018072775337708003), ('fast', 0.018072685337708002)] \n",
            "\n",
            "[('end learning', 0.8694941847232974), ('end', 0.4347470823616487), ('cars', 0.06525299763835123), ('self', 0.06525296763835123)] \n",
            "\n",
            "[('scene free', 0.25000009), ('human gaze', 0.25000006), ('saliency models', 0.25000003), ('invariance analysis', 0.25)] \n",
            "\n",
            "[('consistent adversarial networks', 0.4878049780487806), ('image translation', 0.3625943989772371), ('unpaired image', 0.36259434897723714), ('cycle', 0.024390323902439028)] \n",
            "\n",
            "[('ladder networks', 0.4819161282799999), ('supervised learning', 0.4819160982799999), ('semi', 0.03616784344000001)] \n",
            "\n",
            "[('hyper - parameters', 0.42857145857142864), ('reservoir computing', 0.2857143557142857), ('bayesian optimization', 0.2857142857142857)] \n",
            "\n",
            "[('convolutional neural network architecture', 0.6666666666666665), ('geometric matching', 0.3333333833333333)] \n",
            "\n",
            "[('electroweak scale', 0.3174312565020752), ('wilson coefficients', 0.3174311965020752), ('python package', 0.3174311165020752), ('wilson', 0.1587155432510376), ('matching', 0.023853460246887212), ('running', 0.023853440246887212)] \n",
            "\n",
            "[('neural audio synthesis', 0.42857142857142866), ('wavenet autoencoders', 0.2857143557142857), ('musical notes', 0.2857143257142857)] \n",
            "\n",
            "[('visual question answering', 0.8333333733290622), ('challenge', 0.04166678666773442), ('learnings', 0.04166674666773442), ('tricks', 0.041666686667734425), ('tips', 0.04166666666773442)] \n",
            "\n",
            "[('generating structured queries', 0.42857144857142865), ('reinforcement learning', 0.2857143757142857), ('natural language', 0.2857143457142857)] \n",
            "\n",
            "[('beyond empirical risk minimization', 0.9638554222554806), ('mixup', 0.03614459774451953)] \n",
            "\n",
            "[('fully convolutional neural networks', 0.5714286214285715), ('retinal vessel segmentation', 0.42857142857142866)] \n",
            "\n",
            "[('introspective adversarial networks', 0.5000000399999999), ('neural photo editing', 0.4999999999999999)] \n",
            "\n",
            "[('higher order singular value decomposition', 0.5376344486021504), ('outlier detection', 0.21505394344086026), ('features selection', 0.21505391344086028), ('tensors', 0.01612916225806452), ('use', 0.01612904225806452)] \n",
            "\n",
            "[('randomized matrix decompositions', 0.9523809523809526), ('r', 0.04761908761904763)] \n",
            "\n",
            "[('stellar convective penetration', 0.5825242718446602), ('dynamical simulations', 0.38834958456310664), ('theory', 0.029126263592233012)] \n",
            "\n",
            "[('adversarial training', 0.32519477665675023), ('human captions', 0.32519474665675024), ('same language', 0.3251946866567502), ('machine', 0.02441606002974951)] \n",
            "\n",
            "[('neural machine translation', 0.9523809523809526), ('translate', 0.24448794476582153), ('align', 0.04761911761904763)] \n",
            "\n",
            "[('co - expansion', 0.488251830812215), ('auxiliary sets generation', 0.48735810528534584), ('set expansion', 0.4664019641495116), ('corpus', 0.02439025390243903)] \n",
            "\n",
            "[('image denoising', 0.31861321076017113), ('gaussian denoiser', 0.3186131207601711), ('deep cnn', 0.2857143657142857), ('residual learning', 0.28571433571428567)] \n",
            "\n",
            "[('pass discourse segmentation', 0.5825242918446601), ('global features', 0.38834959456310664), ('pairing', 0.02912627359223301)] \n",
            "\n",
            "[('cross - type biomedical named entity recognition', 0.6863761061634297), ('deep multi - task learning', 0.4967869289775647)] \n",
            "\n",
            "[('relations classifiers', 0.46509703192617835), ('programming ensemble', 0.4650969819261783), ('temporal', 0.034903088073821635), ('integer', 0.034903038073821634)] \n",
            "\n",
            "[('greenai cost estimations', 0.5825243318446601), ('input dimensions', 0.3883495445631066), ('flops', 0.029126223592233012)] \n",
            "\n",
            "[('bilateral multi - perspective matching', 0.6250000000000001), ('natural language sentences', 0.3750000600000001)] \n",
            "\n",
            "[('unknown system', 0.3174312165020752), ('feedback control', 0.3174311765020752), ('data performance', 0.3174311065020752), ('output', 0.023853440246887212), ('finite', 0.02385337024688721)] \n",
            "\n",
            "[('deep dynamic neural networks', 0.5714286114285715), ('nonlinear systems identification', 0.42857142857142866)] \n",
            "\n",
            "[('stochastic optimization', 0.8694942147232975), ('method', 0.06525294763835122), ('adam', 0.06525291763835123)] \n",
            "\n",
            "[('visual question answering', 0.9523809723809525), ('vqa', 0.04761904761904763)] \n",
            "\n",
            "[('reliable interpretation', 0.4650970119261783), ('aware attention', 0.4650969819261783), ('prediction', 0.03490311807382163), ('uncertainty', 0.034903038073821634)] \n",
            "\n",
            "[('cross - modal representations', 0.5594405694405594), ('step relational reasoning', 0.41958049958041954), ('multi', 0.020979080979020983)] \n",
            "\n",
            "[('neural machine translation', 0.9523809523809526), ('translate', 0.24448794476582153), ('align', 0.04761911761904763)] \n",
            "\n",
            "[('natural language processing', 0.5000000799999998), ('dynamic memory networks', 0.5000000399999999)] \n",
            "\n",
            "[('recurrent neural networks', 0.9523810023809526), ('difficulty', 0.047619067619047634)] \n",
            "\n",
            "[('bayesian inference', 0.4819161282799999), ('visible universe', 0.4819160982799999), ('petascale', 0.036167923440000006)] \n",
            "\n",
            "[('calcium imaging data', 0.5000000399999999), ('bayesian spike inference', 0.4999999999999999)] \n",
            "\n",
            "[('deep generative models', 0.5825243218446602), ('supervised learning', 0.3883495345631066), ('semi', 0.02912621359223301)] \n",
            "\n",
            "[('unbalanced segmentations', 0.46509707192617833), ('loss function', 0.46509703192617835), ('deep', 0.034903088073821635), ('dice', 0.034903048073821635)] \n",
            "\n",
            "[('modular tracking framework', 0.5825242718446602), ('unified approach', 0.3883495645631066), ('tracking', 0.2832538065163084), ('registration', 0.02912629359223301)] \n",
            "\n",
            "[('hierarchical navigable small world graphs', 0.602409718554217), ('nearest neighbor search', 0.3614458231325301), ('robust', 0.01807230915662651), ('efficient', 0.01807228915662651)] \n",
            "\n",
            "[('linear bottlenecks', 0.8694942147232975), ('residuals', 0.06525294763835122), ('mobilenetv2', 0.06525291763835123)] \n",
            "\n",
            "[('gaze data collection', 1.00000001)] \n",
            "\n",
            "[('cross - modality cardiac segmentation', 0.4716982832075473), ('play adversarial domain adaptation network', 0.4716981932075473), ('benchmark', 0.014151093396226418), ('plug', 0.014150983396226417), ('adanet', 0.014150963396226417), ('pnp', 0.014150943396226417)] \n",
            "\n",
            "[('end trainable neural network', 0.4819277036925898), ('scene text recognition', 0.39735645104295825), ('sequence recognition', 0.2409639568462949), ('end', 0.08457140264963157), ('application', 0.018072516307410365), ('image', 0.018072446307410363)] \n",
            "\n",
            "[('textual question answering', 0.4878049380487806), ('dynamic memory networks', 0.4878048780487806), ('visual', 0.02439028390243903)] \n",
            "\n",
            "[('dialogue systems', 0.8694941947232975), ('entailment', 0.06525297763835122), ('coherence', 0.06525292763835122)] \n",
            "\n",
            "[('deformable convolutional networks', 0.9999999999999999)] \n",
            "\n",
            "[('stronger', 0.33333339333333334), ('better', 0.3333333533333333), ('yolo9000', 0.3333333333333333)] \n",
            "\n",
            "[('web images', 0.3100321972772823), ('domain transfer', 0.31003216727728233), ('temporal localization', 0.31003206727728233), ('videos', 0.023301346056050908), ('actions', 0.023301326056050908), ('fine', 0.02330129605605091)] \n",
            "\n",
            "[('shot refinement neural network', 0.650406520331913), ('object detection', 0.32520332016595654), ('single', 0.024390249502130586)] \n",
            "\n",
            "[('research library', 0.8694942147232975), ('machine', 0.06525294763835122), ('pylearn2', 0.06525291763835123)] \n",
            "\n",
            "[('text classification', 0.6891419988814423), ('korean', 0.05180982685309289), ('japanese', 0.051809806853092895), ('english', 0.05180978685309289), ('chinese', 0.051809766853092895), ('best', 0.051809716853092894), ('encoding', 0.051809686853092896)] \n",
            "\n",
            "[('modified shallow water equations', 0.9638554022554806), ('seabeds', 0.03614466774451953)] \n",
            "\n",
            "[('inverse imaging problems', 0.5825243618446602), ('proximal operators', 0.3883495245631066), ('networks', 0.02912627359223301)] \n",
            "\n",
            "[('unsupervised document revision detection', 0.6554184819985546), ('semantic document distance measures', 0.6554184319985545)] \n",
            "\n",
            "[('temporal relational reasoning', 0.9523809523809526), ('videos', 0.04761908761904763)] \n",
            "\n",
            "[('decomposable submodular function minimization', 0.6666666766666666), ('incidence relations', 0.33333339333333334)] \n",
            "\n",
            "[('visual descriptions', 0.48191614827999996), ('deep representations', 0.48191608827999993), ('fine', 0.03616788344000001)] \n",
            "\n",
            "[('vector boson fusion', 0.4525865447406338), ('higgs boson production', 0.45258650474063383), ('order qcd effects', 0.36809817950920254), ('second', 0.018404907975460127)] \n",
            "\n",
            "[('parse clothing', 0.9301939638523566), ('outfit', 0.06980609614764327)] \n",
            "\n",
            "[('end trainable neural network', 0.4819277036925898), ('scene text recognition', 0.39735645104295825), ('sequence recognition', 0.2409639568462949), ('end', 0.08457140264963157), ('application', 0.018072516307410365), ('image', 0.018072446307410363)] \n",
            "\n",
            "[('interference', 0.5000001), ('transfer', 0.50000007)] \n",
            "\n",
            "[('vqa challenge', 0.48191615827999995), ('winning entry', 0.4819161182799999), ('pythia', 0.03616784344000001)] \n",
            "\n",
            "[('generative adversarial text', 0.6000000000000001), ('image synthesis', 0.40000004)] \n",
            "\n",
            "[('attentional generative adversarial networks', 0.9638555022554806), ('generation', 0.312849614316057), ('text', 0.03614464774451953)] \n",
            "\n",
            "[('multi - centre eicu critical care dataset', 0.958904159589041), ('models', 0.020547975205479456), ('machine', 0.020547955205479456)] \n",
            "\n",
            "[('aspect extraction', 0.24539707903351451), ('rule selection', 0.2453970490335145), ('hybrid model', 0.24539701903351452), ('review highlights', 0.2453969290335145), ('reviews', 0.12269852451675725), ('opinion', 0.01841231386594185)] \n",
            "\n",
            "[('video generation', 0.4493875373271151), ('temporal coherence', 0.4493874373271151), ('gan', 0.03374179511525659), ('supervision', 0.03374177511525659), ('self', 0.03374175511525659)] \n",
            "\n",
            "[('specific image region', 0.4195804395804195), ('roi mask', 0.27972037972027974), ('convolutional nets', 0.27972033972027976), ('input', 0.020979150979020984)] \n",
            "\n",
            "[('excitation networks', 0.9301939638523566), ('squeeze', 0.06980607614764327)] \n",
            "\n",
            "[('Î±$-divergence minimization', 0.8694942047232974), ('box', 0.06525293763835123), ('black', 0.06525291763835123)] \n",
            "\n",
            "[('crowd judge truthfulness', 0.42857144857142865), ('recent misinformation', 0.2857143857142857), ('longitudinal study', 0.2857143557142857)] \n",
            "\n",
            "[('semantic segmentation', 0.50000004), ('convolutional networks', 0.50000001)] \n",
            "\n",
            "[('medical imaging', 0.8694942447232974), ('platform', 0.06525297763835122), ('niftynet', 0.06525291763835123)] \n",
            "\n",
            "[('continuous deep q', 0.8695652173912978), ('acceleration', 0.04347835086956741), ('model', 0.04347832086956741), ('learning', 0.04347830086956741)] \n",
            "\n",
            "[('non - parametric instance', 0.5000000399999999), ('level discrimination', 0.25000009), ('unsupervised feature', 0.25)] \n",
            "\n",
            "[('bleu score', 0.9301939738523567), ('differentiable', 0.06980607614764327)] \n",
            "\n",
            "[('knowledge distillation', 0.4346990973215265), ('pytorch library', 0.43469906732152647), ('quantization', 0.03265062133923674), ('pruning', 0.03265060133923674), ('lib', 0.03265051133923674), ('kd', 0.03265049133923674)] \n",
            "\n",
            "[('uri', 1.00000003)] \n",
            "\n",
            "[('proximal policy optimization algorithms', 0.9999999999999996)] \n",
            "\n",
            "[('non - smooth', 0.5660377958490562), ('landweber iteration', 0.37735851056603775), ('problem', 0.028302006792452834), ('bouligand', 0.028301886792452834)] \n",
            "\n",
            "[('augmented neural networks', 0.5825243418446602), ('shot learning', 0.3883495345631066), ('memory', 0.029126263592233012)] \n",
            "\n",
            "[('experience replay', 1.00000001)] \n",
            "\n",
            "[('resolution image blending', 0.5660378158490562), ('realistic high', 0.37735854056603774), ('gan', 0.028301906792452834), ('gp', 0.028301886792452834)] \n",
            "\n",
            "[('agnostic estimation', 0.8694941647232974), ('covariance', 0.06525296763835123), ('mean', 0.06525294763835122)] \n",
            "\n",
            "[('augmented self', 0.8160738219637753), ('tracker', 0.061308832678741675), ('memory', 0.06130877267874167), ('mast', 0.06130874267874167)] \n",
            "\n",
            "[('adversarial trajectory perturbation', 0.6000000400000001), ('lidar perception', 0.40000001)] \n",
            "\n",
            "[('supervised sequence learning', 1.0000000199999999)] \n",
            "\n",
            "[('real nvp', 0.50000003), ('density estimation', 0.5)] \n",
            "\n",
            "[('gated graph neural networks', 0.6666667366666665), ('sequence learning', 0.3333333733333333), ('graph', 0.21639675557432614)] \n",
            "\n",
            "[('bayesian data reweighting', 0.5000000399999999), ('robust probabilistic modeling', 0.4999999999999999)] \n",
            "\n",
            "[('auxiliary prediction tasks', 0.5825243518446601), ('sentence embeddings', 0.3883495645631066), ('analysis', 0.029126243592233012)] \n",
            "\n",
            "[('image segmentation tasks', 0.44247924390562876), ('image annotation tool', 0.4424792039056287), ('mask editor', 0.2857142857142857)] \n",
            "\n",
            "[('attentive deep document dater', 0.9638554222554806), ('ad3', 0.03614459774451953)] \n",
            "\n",
            "[('public advanced ligo data', 0.35874454461883404), ('first open gravitational', 0.26905832596412554), ('binary mergers', 0.17937229730941695), ('wave catalog', 0.17937226730941694), ('analysis', 0.013453044798206278)] \n",
            "\n",
            "[('attention', 1.0)] \n",
            "\n",
            "[('traditional deep learning', 0.41095899410958897), ('mathematical approach', 0.273972652739726), ('swag algorithm', 0.273972612739726), ('implementation', 0.020548095205479455), ('theory', 0.020548075205479455)] \n",
            "\n",
            "[('online learning activity', 0.9523809923809525), ('dynamics', 0.047619067619047634)] \n",
            "\n",
            "[('faster graph embeddings', 0.9523809523809526), ('coarsening', 0.04761908761904763)] \n",
            "\n",
            "[('hierarchical density order embeddings', 0.9999999999999996)] \n",
            "\n",
            "[('descriptive captions', 0.50000004), ('discriminability objective', 0.5)] \n",
            "\n",
            "[('visual tracking', 0.33333339333333334), ('resolution convnet', 0.3333333633333333), ('kernalised multi', 0.3333333333333333)] \n",
            "\n",
            "[('object detection', 0.32519475665675024), ('local parts', 0.32519472665675025), ('global structure', 0.3251946966567502), ('couplenet', 0.024416000029749512)] \n",
            "\n",
            "[('manual summary evaluation', 0.6000000400000001), ('lightweight pyramids', 0.40000001)] \n",
            "\n",
            "[('hand segmentation', 0.8694941847232974), ('wild', 0.06525297763835122), ('analysis', 0.06525291763835123)] \n",
            "\n",
            "[('video action recognition', 0.5000000399999999), ('spatiotemporal residual networks', 0.4999999999999999)] \n",
            "\n",
            "[('end information extraction', 0.5825243118446601), ('level supervision', 0.3883496145631066), ('end', 0.14963528266417592), ('token', 0.02912629359223301)] \n",
            "\n",
            "[('multi - person pose estimation', 0.6250000400000001), ('generative partition networks', 0.37500000000000006)] \n",
            "\n",
            "[('compact factorization', 0.8160737719637753), ('rank', 0.06130882267874167), ('round', 0.061308802678741676), ('matrices', 0.06130877267874167)] \n",
            "\n",
            "[('traffic graph convolutional recurrent neural network', 0.6454021386240384), ('scale traffic learning', 0.37254316697433126), ('deep learning framework', 0.2815560021302736), ('network', 0.06336349151893723), ('forecasting', 0.014778505123152713)] \n",
            "\n",
            "[('end 3d neural network', 0.6504065703319131), ('multiview stereopsis', 0.3252033701659565), ('end', 0.11414170706313431), ('surfacenet', 0.024390249502130586)] \n",
            "\n",
            "[('geometric adaptive monte carlo', 0.6666666666666665), ('random environment', 0.3333333833333333)] \n",
            "\n",
            "[('language models', 0.8160737919637753), ('numbers', 0.06130886267874167), ('ability', 0.061308832678741675), ('numeracy', 0.06130874267874167)] \n",
            "\n",
            "[('factual consistency', 0.46509703192617835), ('answering questions', 0.4650969819261783), ('summaries', 0.034903138073821637), ('asking', 0.034903038073821634)] \n",
            "\n",
            "[('molecular dynamics trajectory analysis', 0.6666666966666666), ('parallel performance', 0.3333333333333333)] \n",
            "\n",
            "[('random directions stochastic approximation', 0.6666666666666665), ('deterministic perturbations', 0.3333333833333333)] \n",
            "\n",
            "[('fundamental frequency estimation', 0.9090909390909092), ('music', 0.04545461545454546), ('multitask', 0.04545454545454546)] \n",
            "\n",
            "[('deep network framework', 0.8695652473912978), ('data', 0.04347836086956741), ('manifold', 0.04347833086956741), ('manifoldnet', 0.04347826086956741)] \n",
            "\n",
            "[('complex word identification', 0.42857145857142864), ('multiple languages', 0.2857143557142857), ('strong baselines', 0.2857142857142857)] \n",
            "\n",
            "[('neural image caption generator', 1.0000000499999995)] \n",
            "\n",
            "[('consistent variational auto', 0.5660378058490563), ('disentangling factors', 0.37735849056603776), ('variation', 0.27522281760871375), ('encoders', 0.028301996792452835), ('cycle', 0.028301936792452836)] \n",
            "\n",
            "[('adversarial training', 0.32519474665675024), ('deep representations', 0.3251947166567502), ('disentangling factors', 0.32519466665675023), ('variation', 0.024416030029749514)] \n",
            "\n",
            "[('nonparametric variational inference', 0.6000000300000001), ('early stopping', 0.4)] \n",
            "\n",
            "[('universal adversarial perturbations', 0.4285714985714287), ('free objective', 0.2857143157142857), ('generalizable data', 0.2857142857142857)] \n",
            "\n",
            "[('conditional image generation', 0.9523809523809526), ('decoders', 0.04761909761904763)] \n",
            "\n",
            "[('recurrent network training', 0.8333333733290622), ('sequences', 0.04166679666773442), ('event', 0.041666766667734424), ('long', 0.04166674666773442), ('lstm', 0.04166667666773442)] \n",
            "\n",
            "[('smooth homogeneous neural nets', 0.40000009000000003), ('exponentially weight', 0.20000006), ('gradient descent', 0.20000003000000002), ('inductive bias', 0.2)] \n",
            "\n",
            "[('arcade learning environment', 0.3333333533333333), ('general agents', 0.2222223422222222), ('open problems', 0.22222231222222222), ('evaluation protocols', 0.2222222822222222)] \n",
            "\n",
            "[('mammogram classification', 0.32519475665675024), ('visual primitives', 0.3251946966567502), ('expert identification', 0.32519466665675023), ('cnns', 0.024416070029749513)] \n",
            "\n",
            "[('normal multiplicative noise', 0.5825243318446601), ('structured bayesian', 0.3883495145631066), ('log', 0.02912625359223301)] \n",
            "\n",
            "[('differentiable point clouds', 0.5660378058490563), ('unsupervised learning', 0.37735849056603776), ('pose', 0.028301936792452836), ('shape', 0.028301916792452836)] \n",
            "\n",
            "[('face detection benchmark', 0.8245471083213667), ('wider face', 0.4999999999999998)] \n",
            "\n",
            "[('computer interface design', 0.4195804495804195), ('regret theory', 0.27972037972027974), ('quantitative measure', 0.27972034972027976), ('human', 0.020979030979020985)] \n",
            "\n",
            "[('deep reinforcement learning', 0.6000000300000001), ('asynchronous methods', 0.4)] \n",
            "\n",
            "[('face recognition', 0.46509702192617836), ('unified embedding', 0.4650969919261783), ('clustering', 0.034903128073821635), ('facenet', 0.034903038073821634)] \n",
            "\n",
            "[('visual summaries', 0.50000004), ('deep architectures', 0.50000001)] \n",
            "\n",
            "[('things', 0.50000005), ('starspace', 0.5)] \n",
            "\n",
            "[('cross - domain disentanglement', 0.6666667366666665), ('image translation', 0.3333333733333333), ('image', 0.16666666666666666)] \n",
            "\n",
            "[('high fidelity natural image synthesis', 0.5555556055555555), ('large scale gan training', 0.4444444444444443)] \n",
            "\n",
            "[('play', 0.3333333833333333), ('programs', 0.3333333633333333), ('playgol', 0.3333333333333333)] \n",
            "\n",
            "[('richer convolutional features', 0.6000000000000001), ('edge detection', 0.40000004)] \n",
            "\n",
            "[('acoustic scene classification', 0.5660378258490563), ('simple fusion', 0.37735850056603776), ('shallow', 0.028301946792452834), ('deep', 0.028301926792452834)] \n",
            "\n",
            "[('box functions', 0.3100322072772823), ('expensive black', 0.31003217727728233), ('modular framework', 0.3100320972772823), ('optimization', 0.02330135605605091), ('model', 0.023301326056050908), ('mlrmbo', 0.023301266056050908)] \n",
            "\n",
            "[('conscious machines', 0.8694941647232974), ('programming', 0.06525296763835123), ('consciousness', 0.06525294763835122)] \n",
            "\n",
            "[('disembodied developmental robotic agent', 0.6666666766666666), ('samu bÃ¡tfai', 0.33333339333333334)] \n",
            "\n",
            "[('deformable convnets v2', 0.6000000000000001), ('better results', 0.40000007000000004), ('deformable', 0.15410397436514003)] \n",
            "\n",
            "[('knowledge graph convolutional networks', 0.6666666666666665), ('recommender systems', 0.3333333833333333)] \n",
            "\n",
            "[('object detection', 0.50000004), ('multipath network', 0.50000001)] \n",
            "\n",
            "[('super - convergence', 0.41095890410958896), ('neural networks', 0.273972682739726), ('fast training', 0.273972652739726), ('rates', 0.020548075205479455), ('large', 0.020548055205479455)] \n",
            "\n",
            "[('agnostic boundary refinement', 0.8695652573912978), ('segmentation', 0.04347834086956741), ('model', 0.04347828086956741), ('segfix', 0.04347826086956741)] \n",
            "\n",
            "[('joint embeddings', 0.32519475665675024), ('natural language', 0.3251947166567502), ('generating shapes', 0.3251946866567502), ('text2shape', 0.024416000029749512)] \n",
            "\n",
            "[('scaled simplex representation', 0.9523809523809526), ('subspace', 0.04761908761904763)] \n",
            "\n",
            "[('raw audio', 0.48191613827999996), ('generative model', 0.4819161082799999), ('wavenet', 0.03616784344000001)] \n",
            "\n",
            "[('attention generative adversarial networks', 0.9638554222554806), ('self', 0.03614459774451953)] \n",
            "\n",
            "[('deep convolutional generative adversarial networks', 0.7142857542857144), ('unsupervised representation', 0.2857142857142857)] \n",
            "\n",
            "[('interlaced sparse self', 0.5825242718446602), ('semantic segmentation', 0.38834957456310665), ('attention', 0.02912625359223301)] \n",
            "\n",
            "[('object context network', 0.5825242918446601), ('scene parsing', 0.38834957456310665), ('ocnet', 0.02912621359223301)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('embodied question answering', 0.9999999999999999)] \n",
            "\n",
            "[('dimensional continuous control', 0.5825242918446601), ('advantage estimation', 0.38834958456310664), ('high', 0.02912621359223301)] \n",
            "\n",
            "[('universal adversarial perturbations', 0.33333342333333327), ('data independent approach', 0.33333338333333323), ('fast feature fool', 0.33333333333333326)] \n",
            "\n",
            "[('recycling krylov subspace methods', 0.6504065103319131), ('adjoint problems', 0.32520333016595654), ('self', 0.024390309502130586)] \n",
            "\n",
            "[('robust adversarial reinforcement learning', 0.9999999999999996)] \n",
            "\n",
            "[('artificial neural network training', 0.9638554222554806), ('multigrid', 0.03614467774451953)] \n",
            "\n",
            "[('computer vision', 0.46509704192617835), ('deep learning', 0.4650970119261783), ('library', 0.03490306807382163), ('chainercv', 0.034903038073821634)] \n",
            "\n",
            "[('diffusion probabilistic models', 1.00000001)] \n",
            "\n",
            "[('switchable normalization', 0.50000007), ('differentiable learning', 0.5), ('normalize', 0.25000005)] \n",
            "\n",
            "[('temporal action proposal generation', 0.5594406194405593), ('boundary sensitive network', 0.4195804395804195), ('bsn', 0.020979020979020983)] \n",
            "\n",
            "[('deep neural networks', 0.6000000300000001), ('variational dropout', 0.4)] \n",
            "\n",
            "[('hundred layers tiramisu', 0.42857144857142865), ('semantic segmentation', 0.2857143857142857), ('convolutional densenets', 0.2857143557142857)] \n",
            "\n",
            "[('deep residual learning', 0.6000000000000001), ('image recognition', 0.40000004)] \n",
            "\n",
            "[('connected convolutional networks', 1.00000001)] \n",
            "\n",
            "[('adversarial training', 0.8694942347232975), ('images', 0.06525296763835123), ('simulated', 0.06525293763835123)] \n",
            "\n",
            "[('source slam system', 0.5084746462220284), ('d cameras', 0.3389832208146857), ('rgb', 0.025423878827214322), ('stereo', 0.025423858827214322), ('monocular', 0.025423838827214322), ('open', 0.025423778827214322), ('slam2', 0.02542374882721432), ('orb', 0.02542372882721432)] \n",
            "\n",
            "[('convolutional neural networks', 0.3750000700000001), ('camera model identification', 0.37500003000000004), ('first steps', 0.25)] \n",
            "\n",
            "[('attentive sentence embedding', 0.6000000400000001), ('structured self', 0.40000001)] \n",
            "\n",
            "[('human rationales', 0.50000004), ('machine attention', 0.50000001)] \n",
            "\n",
            "[('augmented minimax linear estimation', 0.9999999999999996)] \n",
            "\n",
            "[('i$-block transitive tilings', 0.6000000500000001), ('convex pentagons', 0.4)] \n",
            "\n",
            "[('steered neural style transfer', 0.9638554222554806), ('laplacian', 0.03614459774451953)] \n",
            "\n",
            "[('differentiable architecture search', 0.9523809723809525), ('darts', 0.04761904761904763)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('dark', 1.00000005)] \n",
            "\n",
            "[('joint 3d proposal generation', 0.4999999999999999), ('view aggregation', 0.25000008), ('object detection', 0.25000005)] \n",
            "\n",
            "[('share research designs', 0.5825243218446602), ('experimental shift', 0.3883495345631066), ('quasi', 0.02912621359223301)] \n",
            "\n",
            "[('internal representation collaging', 0.5000000499999999), ('controllable image synthesis', 0.5000000099999999)] \n",
            "\n",
            "[('deep reinforcement learning', 0.6000000300000001), ('continuous control', 0.4)] \n",
            "\n",
            "[('deep convolutional generative adversarial networks', 0.7142857542857144), ('unsupervised representation', 0.2857142857142857)] \n",
            "\n",
            "[('3d object generation', 0.48780491804878057), ('improved adversarial systems', 0.4878048780487806), ('reconstruction', 0.024390323902439028)] \n",
            "\n",
            "[('convolutional networks', 0.7689600821483594), ('region', 0.05776007696291016), ('detection', 0.057760056962910165), ('fcn', 0.05776002696291017), ('r', 0.057760006962910164)] \n",
            "\n",
            "[('aware joint salient object', 0.7682641382353996), ('camouflaged object detection', 0.48735810528534584), ('uncertainty', 0.02439024390243903)] \n",
            "\n",
            "[('deep neural networks', 0.5000000399999999), ('brain tumor segmentation', 0.4999999999999999)] \n",
            "\n",
            "[('shot classification benchmark', 0.36809826950920255), ('meta learning approaches', 0.36809818950920253), ('unified few', 0.245398853006135), ('transfer', 0.01840491797546013)] \n",
            "\n",
            "[('interactive translation system', 0.36809825950920255), ('heterogeneous bilingual resources', 0.3680982095092025), ('box integration', 0.24539879300613499), ('black', 0.018404907975460127)] \n",
            "\n",
            "[('depthwise separable convolutions', 0.5825243218446602), ('deep learning', 0.3883495345631066), ('xception', 0.02912621359223301)] \n",
            "\n",
            "[('computer vision', 0.50000005), ('inception architecture', 0.50000002)] \n",
            "\n",
            "[('drosophila imaginal discs', 0.33333342333333327), ('gene expression representation', 0.33333338333333323), ('binary pattern dictionary', 0.33333333333333326)] \n",
            "\n",
            "[('resolution multi - view stereo depth inference', 0.7650273724043719), ('recurrent mvsnet', 0.21857923497267756), ('high', 0.016393472622950826)] \n",
            "\n",
            "[('machine learning', 0.32519475665675024), ('riemannian geometry', 0.32519472665675025), ('python package', 0.3251946966567502), ('geomstats', 0.024416000029749512)] \n",
            "\n",
            "[('feature generation', 0.8694941947232975), ('selection', 0.06525297763835122), ('autolearn', 0.06525291763835123)] \n",
            "\n",
            "[('scale video classification benchmark', 0.650406560331913), ('youtube-8 m', 0.3252032501659565), ('large', 0.024390289502130586)] \n",
            "\n",
            "[('rectified linear units', 0.42857150857142867), ('recurrent networks', 0.28571433571428567), ('simple way', 0.2857142957142857)] \n",
            "\n",
            "[('vmf mixture model', 0.9523809723809525), ('dirichlet', 0.04761904761904763)] \n",
            "\n",
            "[('sound event classifiers', 0.42857143857142865), ('noisy labels', 0.2857143657142857), ('web audio', 0.28571433571428567)] \n",
            "\n",
            "[('realistic single image super - resolution', 0.6557377249180331), ('generative adversarial network', 0.3278689524590164), ('photo', 0.016393442622950824)] \n",
            "\n",
            "[('multi - category visual complexity dataset', 0.9523810023774537), ('diverse', 0.023809553811273188), ('savoias', 0.023809523811273187)] \n",
            "\n",
            "[('fake news', 1.00000003)] \n",
            "\n",
            "[('inter - system discrepancies', 0.5263158494736841), ('journal subject classification', 0.39473684210526316), ('scopus', 0.0197369921052632), ('science', 0.0197369721052632), ('web', 0.0197369521052632), ('intra-', 0.019736882105263198)] \n",
            "\n",
            "[('multi - view dynamic images', 0.5555556155555554), ('depth video', 0.22222225222222222), ('action recognition', 0.2222222222222222)] \n",
            "\n",
            "[('text generation models', 0.9090909690909091), ('platform', 0.04545458545454546), ('texygen', 0.04545454545454546)] \n",
            "\n",
            "[('new backbone', 0.8160738019637753), ('cnn', 0.06130885267874167), ('capability', 0.061308832678741675), ('cspnet', 0.06130874267874167)] \n",
            "\n",
            "[('object detection', 0.46509703192617835), ('optimal speed', 0.4650969819261783), ('accuracy', 0.034903088073821635), ('yolov4', 0.034903038073821634)] \n",
            "\n",
            "[('automatic openmp s2s parallelization', 0.89877492465423), ('compiler', 0.03374176511525659), ('multi', 0.033741745115256586), ('compar', 0.03374171511525659)] \n",
            "\n",
            "[('incremental improvement', 0.9301939538523567), ('yolov3', 0.06980607614764327)] \n",
            "\n",
            "[('adversarial malware binaries', 0.9090909690909091), ('deep', 0.04545457545454546), ('vulnerabilities', 0.045454555454545464)] \n",
            "\n",
            "[('sensitive learning', 0.8160737919637753), ('ensembles', 0.06130881267874167), ('planning', 0.061308792678741675), ('uncertainty', 0.06130874267874167)] \n",
            "\n",
            "[('unsupervised representation learning', 0.5825243318446601), ('update rules', 0.3883495445631066), ('meta', 0.02912621359223301)] \n",
            "\n",
            "[('neural machine translation', 0.5000000399999999), ('asynchronous bidirectional decoding', 0.4999999999999999)] \n",
            "\n",
            "[('deep autoencoding gaussian mixture model', 0.6849315568493151), ('intrusion detection', 0.273972712739726), ('map', 0.020547975205479456), ('self', 0.020547945205479454)] \n",
            "\n",
            "[('knowledge graph embeddings', 0.5000000399999999), ('fast linear model', 0.4999999999999999)] \n",
            "\n",
            "[('textual attributes alignment', 0.4195804595804195), ('natural language', 0.27972038972027974), ('person search', 0.27972035972027975), ('visual', 0.020979040979020983)] \n",
            "\n",
            "[('approximate stochastic transition models', 1.0000000099999995)] \n",
            "\n",
            "[('variational multi - phase segmentation', 0.6134969325153374), ('dimensional local features', 0.36809823950920256), ('high', 0.018404967975460127)] \n",
            "\n",
            "[('simple practical accelerated method', 0.6666666766666666), ('finite sums', 0.33333339333333334)] \n",
            "\n",
            "[('third multilingual surface realisation', 0.6201292925682379), ('evaluation results', 0.3100647712841188), ('overview', 0.02326880204921443), ('srâ€™20', 0.023268772049214428), ('task', 0.023268752049214428)] \n",
            "\n",
            "[('unsupervised visual sense disambiguation', 0.650406500331913), ('multimodal embeddings', 0.32520332016595654), ('verbs', 0.024390299502130588)] \n",
            "\n",
            "[('neural networks', 0.32519476665675023), ('catastrophic forgetting', 0.3251947066567502), ('empirical investigation', 0.3251946766567502), ('gradient', 0.024416070029749513)] \n",
            "\n",
            "[('pre - trained language models', 0.9708738764077669), ('large', 0.029126283592233012)] \n",
            "\n",
            "[('sql queries', 0.3174312065020752), ('semantic parsing', 0.3174311765020752), ('logical alignments', 0.3174311465020752), ('lexico', 0.02385341024688721), ('potential', 0.02385339024688721)] \n",
            "\n",
            "[('hierarchical video generation', 0.41958041958041953), ('optical flow', 0.27972034972027976), ('orthogonal information', 0.2797203197202797), ('texture', 0.020979120979020983)] \n",
            "\n",
            "[('multi - scale deep network', 0.5000000899999999), ('depth map prediction', 0.3000000000000001), ('single image', 0.20000005)] \n",
            "\n",
            "[('commonsense causal reasoning', 0.41095902410958896), ('plausible alternatives', 0.273972632739726), ('visual choice', 0.273972602739726), ('image', 0.020548035205479456), ('evaluation', 0.020548015205479456)] \n",
            "\n",
            "[('domain adaptive semantic segmentation', 0.6504065503319131), ('consistent matching', 0.3252032701659565), ('content', 0.024390249502130586)] \n",
            "\n",
            "[('science texts', 0.50000004), ('linguistic complexity', 0.50000001)] \n",
            "\n",
            "[('hyperspherical prototype networks', 0.9999999999999999)] \n",
            "\n",
            "[('parliamentary corpora', 0.32519475665675024), ('ideological placement', 0.32519472665675025), ('word embeddings', 0.32519466665675023), ('analysis', 0.024416040029749512)] \n",
            "\n",
            "[('emotion intensity prediction', 0.5825243518446601), ('neural architectures', 0.3883495645631066), ('emotional', 0.1496352926641759), ('microblogs', 0.02912633359223301)] \n",
            "\n",
            "[('sentiment analysis systems', 0.4761905661904762), ('metamorphic test generation', 0.4761904961904762), ('bias', 0.023809593809523816), ('biasfinder', 0.023809523809523815)] \n",
            "\n",
            "[('state variational autoencoders', 0.5504587355963303), ('joint discovery', 0.36697253706422023), ('relations', 0.027523045779816523), ('factorization', 0.027523025779816523), ('discrete', 0.027522935779816522)] \n",
            "\n",
            "[('social activity', 0.4493875273271151), ('individual mobility', 0.44938749732711514), ('us', 0.03374185511525659), ('adaptation', 0.03374176511525659), ('pandemic', 0.033741745115256586)] \n",
            "\n",
            "[('label embedding', 0.3174312065020752), ('hierarchical partial', 0.3174311765020752), ('automatic fine', 0.3174311065020752), ('entity', 0.02385343024688721), ('afet', 0.02385337024688721)] \n",
            "\n",
            "[('gesture recognition', 0.4819161282799999), ('convolutional lstm', 0.4819160982799999), ('attention', 0.03616784344000001)] \n",
            "\n",
            "[('neural network explanation methods', 0.5000000099999999), ('morphological agreement', 0.25000009), ('hybrid documents', 0.25000006)] \n",
            "\n",
            "[('muon edm measurement', 0.32495046680148554), ('internal tracker alignment', 0.2955666024630542), ('straw tracking detectors', 0.29556656246305424), ('fermilab muon', 0.19704434497536943), ('g-2', 0.014778365123152712)] \n",
            "\n",
            "[('neural constituent parsing', 0.6000000400000001), ('local models', 0.40000001)] \n",
            "\n",
            "[('simple stochastic variance reduction method', 0.6849315568493151), ('machine learning', 0.273972712739726), ('sgd', 0.020547965205479454), ('vr', 0.020547945205479454)] \n",
            "\n",
            "[('informative object annotations', 0.9999999999999999)] \n",
            "\n",
            "[('neural machine translation', 0.9999999999999999)] \n",
            "\n",
            "[('professional cyclists', 0.8694942247232974), ('data', 0.06525295763835123), ('machine', 0.06525292763835122)] \n",
            "\n",
            "[('image description evaluation', 0.9090909590909092), ('consensus', 0.045454565454545466), ('cider', 0.04545454545454546)] \n",
            "\n",
            "[('hierarchical reinforcement learning', 0.6015167304583015), ('optimal representation learning', 0.6015166904583015), ('near', 0.02912621359223301)] \n",
            "\n",
            "[('image question answering', 0.5000000399999999), ('stacked attention networks', 0.4999999999999999)] \n",
            "\n",
            "[('unsupervised cross - modality domain adaptation', 0.5381165919282511), ('biomedical image segmentations', 0.26905838596412557), ('adversarial loss', 0.17937232730941693), ('convnets', 0.013452984798206278)] \n",
            "\n",
            "[('system log messages', 0.5825243118446601), ('length matters', 0.3883495145631066), ('length', 0.19417483728155333), ('words', 0.02912631359223301)] \n",
            "\n",
            "[('deep ordinal classification', 0.5000000399999999), ('cumulative link models', 0.4999999999999999)] \n",
            "\n",
            "[('continual learning', 0.9301939238523567), ('hypernetworks', 0.06980610614764327)] \n",
            "\n",
            "[('cross - lingual language model', 0.9999999999999997)] \n",
            "\n",
            "[('neural system identification', 0.6000000000000001), ('large populations', 0.40000004)] \n",
            "\n",
            "[('convolutional neural networks', 0.6000000000000001), ('sentence classification', 0.40000004)] \n",
            "\n",
            "[('ai solutions', 0.8694941647232974), ('gathering', 0.06525299763835123), ('magic', 0.06525296763835123)] \n",
            "\n",
            "[('inertial odometry', 0.46509704192617835), ('authentic dataset', 0.4650969919261783), ('visual', 0.03490309807382164), ('advio', 0.034903038073821634)] \n",
            "\n",
            "[('multi - scale super - resolution generation', 0.966917486037512), ('pathological images', 0.245398893006135), ('resolution', 0.12994384246881485), ('low', 0.018404987975460127)] \n",
            "\n",
            "[('multi - party dialogues', 0.6349201539374402), ('structured network', 0.31746008696872013), ('graph', 0.023809974546919844), ('gsn', 0.023809944546919842)] \n",
            "\n",
            "[('route constrained optimization', 0.6000000300000001), ('knowledge distillation', 0.4)] \n",
            "\n",
            "[('humor analysis', 0.48191614827999996), ('spanish corpus', 0.4819161182799999), ('crowd', 0.03616785344000001)] \n",
            "\n",
            "[('natural language processing', 0.4878049480487806), ('neural network models', 0.48780490804878057), ('primer', 0.02439025390243903)] \n",
            "\n",
            "[('task cascaded convolutional networks', 0.5479452854794518), ('joint face detection', 0.41095890410958896), ('multi', 0.020548005205479454), ('alignment', 0.020547985205479454)] \n",
            "\n",
            "[('discriminative localization', 0.50000004), ('deep features', 0.50000001)] \n",
            "\n",
            "[('sparse gaussian graphical models', 0.43010766688172036), ('scalable joint estimator', 0.3225806751612903), ('additional knowledge', 0.2150538434408603), ('multiple', 0.01612915225806452), ('fast', 0.01612904225806452)] \n",
            "\n",
            "[('consistent adversarial networks', 0.4878049780487806), ('image translation', 0.3625943989772371), ('unpaired image', 0.36259434897723714), ('cycle', 0.024390323902439028)] \n",
            "\n",
            "[('supervised speaker diarization', 1.00000001)] \n",
            "\n",
            "[('deep convolutional neural networks', 0.4444444444444443), ('environmental sound classification', 0.33333341333333333), ('data augmentation', 0.2222222722222222)] \n",
            "\n",
            "[('neural machine translation', 0.42857142857142866), ('subword units', 0.2857143557142857), ('rare words', 0.2857143257142857)] \n",
            "\n",
            "[('scalable parallel method', 0.4285714885714287), ('composite optimization', 0.2857143857142857), ('nonsmooth barrier', 0.2857143057142857)] \n",
            "\n",
            "[('recurrent neural networks', 0.6000000300000001), ('generating sequences', 0.4)] \n",
            "\n",
            "[('efficient text classification', 0.9090909490909092), ('tricks', 0.045454565454545466), ('bag', 0.04545454545454546)] \n",
            "\n",
            "[('artificial neural networks', 0.9523809923809525), ('topology', 0.04761904761904763)] \n",
            "\n",
            "[('korean abstract meaning representation corpus', 1.0000000099999997)] \n",
            "\n",
            "[('deep seeded region', 0.5000000699999999), ('semantic segmentation network', 0.5000000299999999)] \n",
            "\n",
            "[('neuron selectivity transfer', 0.60000008), ('knowledge distill', 0.40000005)] \n",
            "\n",
            "[('grammatical error correction', 0.9090909490909092), ('grammaticality', 0.23346464673810635), ('fluency', 0.04545462545454546), ('goals', 0.045454565454545466)] \n",
            "\n",
            "[('deep graph convolutional encoders', 0.4999999999999999), ('text generation', 0.25000008), ('structured data', 0.25000005)] \n",
            "\n",
            "[('compact binary descriptors', 0.6000000300000001), ('regularized gan', 0.40000008000000004)] \n",
            "\n",
            "[('connectionist temporal classification', 0.5825243318446601), ('keras model', 0.3883495445631066), ('ctcmodel', 0.02912621359223301)] \n",
            "\n",
            "[('direction graph matching', 1.00000001)] \n",
            "\n",
            "[('convolutional neural networks', 0.6000000300000001), ('fast algorithms', 0.4)] \n",
            "\n",
            "[('hard exploration games', 0.9523809623809526), ('youtube', 0.04761910761904763)] \n",
            "\n",
            "[('online variational bayes', 0.5000000499999999), ('task agnostic continual', 0.4999999999999999)] \n",
            "\n",
            "[('3d object detection', 0.4195805395804195), ('point cloud', 0.27972036972027975), ('end learning', 0.27972033972027976), ('end', 0.13986015986013986), ('voxelnet', 0.020979020979020983)] \n",
            "\n",
            "[('gaussian mixture alignment', 0.5825243318446601), ('dynamic uncertainty', 0.3883495345631066), ('dugma', 0.02912621359223301)] \n",
            "\n",
            "[('deep convolutional generative adversarial networks', 0.7142857542857144), ('unsupervised representation', 0.2857142857142857)] \n",
            "\n",
            "[('generative models', 0.48191615827999995), ('quantitative analysis', 0.4819160982799999), ('decoder', 0.03616789344000001)] \n",
            "\n",
            "[('end learning', 0.8694941847232974), ('end', 0.4347470823616487), ('cars', 0.06525299763835123), ('self', 0.06525296763835123)] \n",
            "\n",
            "[('unstructured multi - view stereo', 0.6993007493006994), ('depth inference', 0.27972029972027973), ('mvsnet', 0.020979020979020983)] \n",
            "\n",
            "[('unsupervised audio restoration', 0.5825243418446602), ('deep priors', 0.3883495545631066), ('design', 0.02912623359223301)] \n",
            "\n",
            "[('social media hate speech', 0.9302216517581678), ('aspects', 0.03488921412091613), ('targets', 0.03488919412091613)] \n",
            "\n",
            "[('song lyrics', 0.8694942247232974), ('bias', 0.06525295763835123), ('style', 0.06525293763835123)] \n",
            "\n",
            "[('dynamical systems', 0.3174311965020752), ('molecular biology', 0.3174311665020752), ('principal manifolds', 0.3174310865020752), ('practice', 0.023853420246887212), ('graphs', 0.023853400246887212)] \n",
            "\n",
            "[('stanford doggo', 0.726864350857648), ('drive', 0.054627249828470316), ('direct', 0.05462722982847032), ('quasi', 0.054627209828470316), ('source', 0.05462718982847032), ('open', 0.054627169828470316)] \n",
            "\n",
            "[('surface networks', 1.0)] \n",
            "\n",
            "[('cnn feature maps', 0.9090909490909092), ('multisampling', 0.045454635454545464), ('resolution', 0.045454565454545466)] \n",
            "\n",
            "[('infrared person re - identification', 0.49261093743842377), ('variational subspace disentanglement', 0.2955665424630542), ('dual gaussian', 0.1970443349753695), ('visible', 0.014778405123152712)] \n",
            "\n",
            "[('speech representation learning', 0.36809825950920255), ('pretext task selection', 0.36809818950920253), ('conditional independence', 0.245398773006135), ('self', 0.01840497797546013)] \n",
            "\n",
            "[('pretrained multilingual language models', 0.6201293825682378), ('small data', 0.3100646412841188), ('languages', 0.20131806454137388), ('low', 0.02326884204921443), ('viability', 0.023268772049214428), ('problem', 0.023268732049214428)] \n",
            "\n",
            "[('fully convolutional encoder', 0.37500005000000003), ('object contour detection', 0.37500000000000006), ('decoder network', 0.25000009)] \n",
            "\n",
            "[('person tracking', 0.4650970119261783), ('articulated multi', 0.4650969819261783), ('wild', 0.034903128073821635), ('arttrack', 0.034903038073821634)] \n",
            "\n",
            "[('person pose estimation model', 0.6201293925682378), ('faster multi', 0.3100647212841188), ('stronger', 0.02326874204921443), ('deeper', 0.02326872204921443), ('deepercut', 0.023268692049214428)] \n",
            "\n",
            "[('dynamic graph cnn', 0.6000000000000001), ('point clouds', 0.40000006000000005)] \n",
            "\n",
            "[('squared planar markers', 0.5504588255963303), ('simultaneous localization', 0.3669724970642202), ('keypoints', 0.027523025779816523), ('fusion', 0.027523005779816524), ('mapping', 0.027522985779816524)] \n",
            "\n",
            "[('complex 3d environments', 0.9090909690909091), ('navigation', 0.04545458545454546), ('classic', 0.045454555454545464)] \n",
            "\n",
            "[('large graph convolutional networks', 0.6201293925682378), ('efficient algorithm', 0.3100646912841188), ('deep', 0.02326878204921443), ('gcn', 0.023268712049214428), ('cluster', 0.023268692049214428)] \n",
            "\n",
            "[('truncated singular value decomposition', 0.5714286114285715), ('efficient thresholded correlation', 0.42857142857142866)] \n",
            "\n",
            "[('time eye gaze estimation', 0.6201293425682378), ('natural environments', 0.3100647512841188), ('real', 0.023268732049214428), ('gene', 0.023268712049214428), ('rt', 0.023268692049214428)] \n",
            "\n",
            "[('inertial bregman proximal gradient algorithms', 0.5555556255555555), ('matrix factorization', 0.22222226222222222), ('alternating updates', 0.2222222322222222)] \n",
            "\n",
            "[('optical character recognition', 0.3614459131325301), ('deep learning package', 0.3614458731325301), ('performance tensorflow', 0.24096390542168675), ('high', 0.01807231915662651), ('calamari', 0.01807228915662651)] \n",
            "\n",
            "[('scene graph parsing', 0.42857145857142864), ('global context', 0.2857143557142857), ('neural motifs', 0.2857142857142857)] \n",
            "\n",
            "[('efficient neural architecture search', 0.6666666666666665), ('parameter sharing', 0.3333333833333333)] \n",
            "\n",
            "[('logic programs', 1.00000003)] \n",
            "\n",
            "[('scale audio classification', 0.5825243218446602), ('cnn architectures', 0.3883495145631066), ('large', 0.029126243592233012)] \n",
            "\n",
            "[('team ability parameters', 0.3225808151612904), ('random forest approach', 0.3225807351612904), ('fifa world cup', 0.32258067516129035), ('emphasis', 0.01612917225806452), ('prediction', 0.01612903225806452)] \n",
            "\n",
            "[('stronger', 0.33333339333333334), ('better', 0.3333333533333333), ('yolo9000', 0.3333333333333333)] \n",
            "\n",
            "[('dense object detection', 0.6000000300000001), ('focal loss', 0.4)] \n",
            "\n",
            "[('multi - level visual similarity based personalized tourist attraction recommendation', 0.9708737864077667), ('photos', 0.014563246796116504), ('geo', 0.014563216796116505)] \n",
            "\n",
            "[('scale machine', 0.7689600121483594), ('systems', 0.05776010696291017), ('heterogeneous', 0.057760086962910163), ('large', 0.05776002696291017), ('tensorflow', 0.057760006962910164)] \n",
            "\n",
            "[('deep quantum neural networks', 0.6666666966666666), ('efficient learning', 0.3333333333333333)] \n",
            "\n",
            "[('feature space augmentation', 0.5660378058490563), ('kutta networks', 0.37735853056603774), ('runge', 0.028301906792452834), ('classification', 0.028301886792452834)] \n",
            "\n",
            "[('deep neural networks', 0.4878049380487806), ('aware channel pruning', 0.4878048980487806), ('discrimination', 0.02439024390243903)] \n",
            "\n",
            "[('entity equalization', 0.50000003), ('coreference resolution', 0.5)] \n",
            "\n",
            "[('neural image caption generator', 1.0000000499999995)] \n",
            "\n",
            "[('ubuntu dialogue corpus', 0.4056180748079739), ('dialogue systems', 0.3204387336036778), ('unstructured multi', 0.245398883006135), ('large dataset', 0.245398833006135), ('research', 0.01840499797546013)] \n",
            "\n",
            "[('latent topic clustering', 0.3000001200000001), ('hierarchical recurrent encoder', 0.3000000800000001), ('answer pairs', 0.20000005), ('rank question', 0.20000002)] \n",
            "\n",
            "[('level cosmological emulator', 0.5825243418446602), ('universal field', 0.3883495545631066), ('necola', 0.02912621359223301)] \n",
            "\n",
            "[('associated pooling mechanisms', 0.4026847037583893), ('simple word', 0.26845643583892614), ('more love', 0.2684563958389261), ('models', 0.020134348187919468), ('embedding', 0.02013431818791947), ('baseline', 0.02013422818791947)] \n",
            "\n",
            "[('facial expression recognition', 0.6000000000000001), ('deep learning', 0.40000004)] \n",
            "\n",
            "[('convolutional neural networks', 0.5825243218446602), ('photo geolocation', 0.3883495345631066), ('planet', 0.02912621359223301)] \n",
            "\n",
            "[('covid-19 diagnosis models', 0.4285714985714287), ('thoracic disease', 0.2857143257142857), ('semantic interpretation', 0.2857142957142857)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('semantic segmentation', 0.50000004), ('convolutional networks', 0.50000001)] \n",
            "\n",
            "[('attention augmented convolutional networks', 0.9999999999999996)] \n",
            "\n",
            "[('polyphonic music', 0.25000009), ('pitch detection', 0.25000006), ('harmonic priors', 0.25000003), ('efficient learning', 0.25)] \n",
            "\n",
            "[('implicit bitrate optimization', 0.5504587555963303), ('compressive autoencoders', 0.3669726070642202), ('pruning', 0.027523045779816523), ('admm', 0.027522955779816522), ('cae', 0.027522935779816522)] \n",
            "\n",
            "[('kepler planet occurrence rates', 0.650406580331913), ('transit multiplicity', 0.3252033001659565), ('incompleteness', 0.024390269502130586)] \n",
            "\n",
            "[('3d shape analysis', 0.4761905861904762), ('graph convolutional network', 0.47619054619047624), ('gcn', 0.023809543809523815), ('view', 0.023809523809523815)] \n",
            "\n",
            "[('semantic segmentation', 0.50000004), ('convolutional networks', 0.50000001)] \n",
            "\n",
            "[('hyperspectral imagery super - resolution', 0.7142857542857144), ('unsupervised adaptation', 0.2857142857142857)] \n",
            "\n",
            "[('elaborate pre - processing', 0.650406610331913), ('face recognition', 0.32520342016595655), ('frontalize', 0.024390259502130588)] \n",
            "\n",
            "[('fast mser', 1.0)] \n",
            "\n",
            "[('deep neural networks', 0.5000000399999999), ('aggregated residual transformations', 0.4999999999999999)] \n",
            "\n",
            "[('chinese named entity recognition', 0.49079758571884946), ('attention mechanism', 0.24539888285942485), ('transfer learning', 0.24539878285942485), ('self', 0.01840499856230069)] \n",
            "\n",
            "[('discovery prediction', 0.2453970590335145), ('genealogical features', 0.2453969990335145), ('knowledge graph', 0.24539696903351452), ('temporal evolution', 0.2453969390335145), ('literature', 0.018412383865941847)] \n",
            "\n",
            "[('backprop', 1.00000005)] \n",
            "\n",
            "[('convolutive transfer function invariant sdr training criteria', 0.5384615384615384), ('multi - channel reverberant speech separation', 0.46153854153846147)] \n",
            "\n",
            "[('shot semantic segmentation', 0.4878049380487806), ('harmonic feature activation', 0.4878048780487806), ('few', 0.02439028390243903)] \n",
            "\n",
            "[('scale learnable graph convolutional networks', 0.9708738064077669), ('large', 0.02912621359223301)] \n",
            "\n",
            "[('linear colour segmentation', 0.9999999999999999)] \n",
            "\n",
            "[('conditional invertible neural networks', 0.6666667066666665), ('image generation', 0.3333333433333333)] \n",
            "\n",
            "[('generative flow', 0.8694941847232974), ('convolutions', 0.06525298763835123), ('invertible', 0.06525296763835123)] \n",
            "\n",
            "[('pairwise comparison experiments', 0.5825243418446602), ('practical guide', 0.3883495245631066), ('software', 0.02912625359223301)] \n",
            "\n",
            "[('automatic differentiation', 0.50000004), ('simple essence', 0.50000001)] \n",
            "\n",
            "[('visual tracking', 0.32519476665675023), ('squares estimator', 0.3251946966567502), ('recursive least', 0.32519466665675023), ('online', 0.024416070029749513)] \n",
            "\n",
            "[('chinese input method', 0.5504588655963303), ('ensemble model', 0.3669724870642202), ('models', 0.1834863485321101), ('japanese', 0.027523065779816523), ('character', 0.027523015779816522), ('word', 0.027522975779816522)] \n",
            "\n",
            "[('cascaded anisotropic convolutional neural networks', 0.5555556055555555), ('automatic brain tumor segmentation', 0.4444444444444443)] \n",
            "\n",
            "[('perceptual quality assessment', 0.6000000000000001), ('smartphone photography', 0.40000004)] \n",
            "\n",
            "[('scale invariant descriptor', 0.5825243018446602), ('texture classification', 0.38834958456310664), ('fwlbp', 0.02912621359223301)] \n",
            "\n",
            "[('body orientation', 0.4650970119261783), ('monocular estimation', 0.4650969819261783), ('wild', 0.034903128073821635), ('mebow', 0.034903038073821634)] \n",
            "\n",
            "[('multi - view generation', 0.6349201539374402), ('complete representations', 0.31746008696872013), ('gan', 0.023809964546919842), ('cr', 0.023809944546919842)] \n",
            "\n",
            "[('service disruption attacks', 0.3225806951612903), ('unsupervised learning', 0.21505391344086028), ('data analytics', 0.21505388344086027), ('optical networks', 0.21505385344086028), ('detection', 0.01612906225806452), ('experiment', 0.01612903225806452)] \n",
            "\n",
            "[('ai problems', 0.33333340333333333), ('validate models', 0.3333333733333333), ('smt solvers', 0.3333333433333333)] \n",
            "\n",
            "[('sound sample generation', 0.4195804295804195), ('adversarial auto', 0.27972035972027975), ('musical conditioning', 0.2797203297202797), ('encoders', 0.020979130979020984)] \n",
            "\n",
            "[('internal covariate shift', 0.37500009000000006), ('deep network training', 0.37500004000000003), ('batch normalization', 0.25)] \n",
            "\n",
            "[('discrete optimization perspective', 0.4285714985714287), ('phase transitions', 0.2857143157142857), ('sparse classification', 0.2857142857142857)] \n",
            "\n",
            "[('sports videos', 0.46509703192617835), ('zoom slam', 0.4650970019261783), ('tilt', 0.03490305807382164), ('pan', 0.034903038073821634)] \n",
            "\n",
            "[('generalized linear models', 0.4026845737583893), ('technical report', 0.2684565958389261), ('precision learning', 0.26845656583892613), ('system', 0.02013437818791947), ('fits', 0.020134348187919468), ('size', 0.020134328187919468)] \n",
            "\n",
            "[('natural images', 0.48191615827999995), ('3d representations', 0.4819161282799999), ('learning', 0.036167873440000005)] \n",
            "\n",
            "[('contour integration', 0.33333340333333333), ('astrometric microlensing', 0.3333333733333333), ('public code', 0.3333333433333333)] \n",
            "\n",
            "[('video caption generation', 0.3614458931325301), ('candidate pool evaluation', 0.36144585313253014), ('level features', 0.24096389542168675), ('segment', 0.01807230915662651), ('frame-', 0.01807228915662651)] \n",
            "\n",
            "[('dependent types', 0.8160737719637753), ('practice', 0.06130881267874167), ('theory', 0.061308792678741675), ('haskell', 0.06130877267874167)] \n",
            "\n",
            "[('deep reinforcement learning', 0.6000000300000001), ('continuous control', 0.4)] \n",
            "\n",
            "[('end learning', 0.8694941847232974), ('end', 0.4347470823616487), ('cars', 0.06525299763835123), ('self', 0.06525296763835123)] \n",
            "\n",
            "[('visual object tracking', 0.4285714885714287), ('similarity transformation', 0.2857143157142857), ('robust estimation', 0.2857142857142857)] \n",
            "\n",
            "[('high level visual representations', 0.6504065703319131), ('textual descriptions', 0.3252033701659565), ('mind', 0.024390289502130586)] \n",
            "\n",
            "[('text classification need', 0.4195804695804195), ('vocabulary selection', 0.27972040972027973), ('variational approach', 0.27972037972027974), ('vocabulary', 0.13986016986013988), ('large', 0.020979030979020985)] \n",
            "\n",
            "[('hardware robot information model', 0.8433562154841203), ('modular robots', 0.4745588042062714), ('information model', 0.36879738127784895), ('hrim', 0.02912634359223301)] \n",
            "\n",
            "[('prototype generating network', 0.5660377658490563), ('shot learning', 0.37735858056603777), ('zero', 0.028301956792452836), ('episode', 0.028301886792452834)] \n",
            "\n",
            "[('stellar binaries', 0.32519477665675023), ('black holes', 0.3251947066567502), ('spin orientations', 0.32519466665675023), ('evolution', 0.024416090029749513)] \n",
            "\n",
            "[('temporal interaction data', 0.5504588155963303), ('community structure', 0.3669725470642202), ('reciprocity', 0.027522985779816524), ('heterogeneity', 0.027522965779816524), ('sparsity', 0.027522945779816524)] \n",
            "\n",
            "[('hierarchical attention based position', 0.43715846994535545), ('level sentiment analysis', 0.3278689524590164), ('aware network', 0.21857928497267756), ('aspect', 0.016393522622950824)] \n",
            "\n",
            "[('youtube-8 m video understanding challenge', 0.6250000600000001), ('size tensorflow models', 0.37500002000000004)] \n",
            "\n",
            "[('artistic style', 0.50000004), ('neural algorithm', 0.50000001)] \n",
            "\n",
            "[('intra - domain structures', 0.6666667166666665), ('easy transfer', 0.3333333333333333)] \n",
            "\n",
            "[('deep reinforcement', 0.8694941947232975), ('chess', 0.06525299763835123), ('giraffe', 0.06525291763835123)] \n",
            "\n",
            "[('improved quality', 0.7689600221483595), ('variation', 0.05776011696291016), ('stability', 0.057760086962910163), ('gans', 0.05776003696291016), ('progressive', 0.057760006962910164)] \n",
            "\n",
            "[('messi biological systems', 0.9523809823809526), ('structure', 0.04761905761904763)] \n",
            "\n",
            "[('deep neural networks', 0.6000000300000001), ('variational dropout', 0.4)] \n",
            "\n",
            "[('deconvolutional paragraph representation learning', 0.9999999999999996)] \n",
            "\n",
            "[('artistic style', 0.50000004), ('neural algorithm', 0.50000001)] \n",
            "\n",
            "[('deep image homography estimation', 0.9999999999999996)] \n",
            "\n",
            "[('generative adversarial networks', 0.9999999999999999)] \n",
            "\n",
            "[('future directions', 0.4493875373271151), ('monocular slam', 0.4493874573271151), ('survey', 0.03374179511525659), ('design', 0.03374177511525659), ('keyframe', 0.03374171511525659)] \n",
            "\n",
            "[('incremental euclidean distance fields', 0.43715849994535544), ('online motion planning', 0.32786893245901644), ('aerial robots', 0.21857935497267755), ('fiesta', 0.016393442622950824)] \n",
            "\n",
            "[('rare event modeling', 0.6000000300000001), ('variational disentanglement', 0.4)] \n",
            "\n",
            "[('k steps', 0.50000003), ('lookahead optimizer', 0.5), ('step', 0.25000008)] \n",
            "\n",
            "[('convolutional neural networks', 0.5660378158490562), ('deep recurrent', 0.37735854056603774), ('eeg', 0.028301916792452836), ('representations', 0.028301896792452836)] \n",
            "\n",
            "[('neural network representations', 0.9523809723809525), ('similarity', 0.04761904761904763)] \n",
            "\n",
            "[('discrete space curves', 0.4285714985714287), ('integrable deformations', 0.2857143257142857), ('linkage mechanisms', 0.2857142857142857)] \n",
            "\n",
            "[('hybrid deep learning model', 0.5714285814285716), ('arabic text recognition', 0.4285714885714287)] \n",
            "\n",
            "[('discrete mÃ¶bius strip', 0.4878049680487806), ('closed linkage mechanism', 0.4878048880487806), ('shape', 0.02439030390243903)] \n",
            "\n",
            "[('iterated supervised learning', 0.9523810023809526), ('goals', 0.04761907761904763)] \n",
            "\n",
            "[('deep temporal clustering', 0.41958041958041953), ('domain features', 0.27972037972027974), ('unsupervised learning', 0.2797203297202797), ('time', 0.020979100979020983)] \n",
            "\n",
            "[('centered gaussian distributions', 0.3333334433333333), ('wasserstein distance', 0.2222222922222222), ('improved estimation', 0.22222225222222222), ('random matrix', 0.2222222222222222)] \n",
            "\n",
            "[('head pose estimation', 0.9523809923809525), ('alignment', 0.04761905761904763)] \n",
            "\n",
            "[('stage temporal convolutional network', 0.6201293425682378), ('action segmentation', 0.3100647512841188), ('multi', 0.023268732049214428), ('tcn++', 0.023268712049214428), ('ms', 0.023268692049214428)] \n",
            "\n",
            "[('scalable outlier detection', 0.5825243318446601), ('python toolbox', 0.3883495445631066), ('pyod', 0.02912621359223301)] \n",
            "\n",
            "[('convolutional recurrent neural networks', 0.5714286114285715), ('music artist classification', 0.42857142857142866)] \n",
            "\n",
            "[('level convolutional networks', 0.5825242918446601), ('text classification', 0.38834957456310665), ('character', 0.02912621359223301)] \n",
            "\n",
            "[('additive angular margin loss', 0.5594405794405594), ('deep face recognition', 0.41958048958041955), ('arcface', 0.020979020979020983)] \n",
            "\n",
            "[('sequence modeling', 0.2500001), ('recurrent networks', 0.25000007), ('generic convolutional', 0.25000004), ('empirical evaluation', 0.25000001)] \n",
            "\n",
            "[('cross - modal transfer', 0.9638554522554806), ('shot', 0.03614461774451953)] \n",
            "\n",
            "[('semantic image segmentation', 0.6000000400000001), ('atrous convolution', 0.40000001)] \n",
            "\n",
            "[('semantic image synthesis', 0.6000000000000001), ('adversarial learning', 0.40000004)] \n",
            "\n",
            "[('crystal materials symmetry prediction', 0.6349200939374402), ('enhanced descriptors', 0.31746013696872016), ('machine', 0.023810014546919844), ('composition', 0.023809944546919842)] \n",
            "\n",
            "[('generative adversarial text', 0.6000000000000001), ('image synthesis', 0.40000004)] \n",
            "\n",
            "[('deep neural networks', 0.5000000599999999), ('optimal power control', 0.5000000099999999)] \n",
            "\n",
            "[('fast depth completion', 0.47619054619047624), ('classical image processing', 0.4761905061904762), ('cpu', 0.023809643809523814), ('defense', 0.023809533809523817)] \n",
            "\n",
            "[('understanding atari agents', 1.0000000199999999)] \n",
            "\n",
            "[('measurement error perspective', 0.49500330111277024), ('predictor measurement heterogeneity', 0.41628139117520463), ('prediction models', 0.2684564758389261), ('performance', 0.020134308187919468), ('settings', 0.02013428818791947), ('impact', 0.02013422818791947)] \n",
            "\n",
            "[('stochastic depth', 0.50000003), ('deep networks', 0.5)] \n",
            "\n",
            "[('beyond empirical risk minimization', 0.9638554222554806), ('mixup', 0.03614459774451953)] \n",
            "\n",
            "[('video compressive', 0.48191614827999996), ('adaptive reconstruction', 0.4819161182799999), ('scalable', 0.03616786344000001)] \n",
            "\n",
            "[('excitation networks', 0.9301939638523566), ('squeeze', 0.06980607614764327)] \n",
            "\n",
            "[('3d data processing', 0.5825243318446601), ('modern library', 0.3883495445631066), ('open3d', 0.02912621359223301)] \n",
            "\n",
            "[('field keyword spotting', 0.5660378058490563), ('trainable frontend', 0.37735849056603776), ('far', 0.028301936792452836), ('robust', 0.028301916792452836)] \n",
            "\n",
            "[('spatially localized atlas network tiles', 0.5555556055555555), ('3d whole brain segmentation', 0.4444444444444443)] \n",
            "\n",
            "[('implicit feature interactions', 0.5660377858490563), ('recommender systems', 0.37735858056603777), ('explicit', 0.028301916792452836), ('xdeepfm', 0.028301886792452834)] \n",
            "\n",
            "[('autonomous uav', 0.3100322072772823), ('multiple radio', 0.31003213727728235), ('time localization', 0.3100320872772823), ('animals', 0.02330137605605091), ('tracking', 0.02330131605605091), ('real', 0.023301266056050908)] \n",
            "\n",
            "[('scalable gaussian processes', 0.9523809823809526), ('thoughts', 0.04761904761904763)] \n",
            "\n",
            "[('stellar binaries', 0.32519477665675023), ('black holes', 0.3251947066567502), ('spin orientations', 0.32519466665675023), ('evolution', 0.024416090029749513)] \n",
            "\n",
            "[('black bodies', 0.50000003), ('new insights', 0.5)] \n",
            "\n",
            "[('realistic single image super - resolution', 0.6557377249180331), ('generative adversarial network', 0.3278689524590164), ('photo', 0.016393442622950824)] \n",
            "\n",
            "[('deep learning', 0.9301939338523567), ('generalization', 0.06980612614764327)] \n",
            "\n",
            "[('data generation', 0.9301939538523567), ('geometry', 0.06980607614764327)] \n",
            "\n",
            "[('deep scene cnns', 0.6000000400000001), ('object detectors', 0.4)] \n",
            "\n",
            "[('compound pcfgs', 1.00000002)] \n",
            "\n",
            "[('yau threefolds', 0.8160738419637753), ('calabi', 0.061308792678741675), ('schoen', 0.06130877267874167), ('note', 0.061308752678741675)] \n",
            "\n",
            "[('double q', 0.4819161182799999), ('deep reinforcement', 0.48191607827999994), ('learning', 0.036167913440000005)] \n",
            "\n",
            "[('coreference resolution', 0.46509705192617834), ('order inference', 0.46509702192617836), ('higher', 0.03490307807382163), ('myth', 0.03490305807382164)] \n",
            "\n",
            "[('manifold embedded distribution alignment', 0.5714286114285715), ('visual domain adaptation', 0.42857142857142866)] \n",
            "\n",
            "[('embodied question answering', 0.5000000399999999), ('neural modular control', 0.4999999999999999)] \n",
            "\n",
            "[('flavour anomalies', 0.33333340333333333), ('precision constraints', 0.3333333733333333), ('global likelihood', 0.3333333433333333)] \n",
            "\n",
            "[('initial speech recognition baseline', 0.5096605697774615), ('source kazakh speech corpus', 0.5096605197774615), ('crowdsourced open', 0.2222222322222222)] \n",
            "\n",
            "[('covid-19 research', 0.48191618827999994), ('question generation', 0.48191614827999996), ('questions', 0.24095804913999996), ('corpus', 0.03616788344000001)] \n",
            "\n",
            "[('new hierarchical bayesian framework', 0.36363648363636364), ('binary stellar evolution', 0.2727273427272727), ('wave catalogs', 0.18181821181818184), ('mining gravitational', 0.18181818181818182)] \n",
            "\n",
            "[('shot visual recognition', 0.4878049480487806), ('unsupervised domain adaptation', 0.4878048880487806), ('zero', 0.02439029390243903)] \n",
            "\n",
            "[('low frequency adversarial perturbation', 0.9999999999999996)] \n",
            "\n",
            "[('massive lossless data compression', 0.4444444444444443), ('multiple parameter estimation', 0.3333333833333333), ('galaxy spectra', 0.22222231222222222)] \n",
            "\n",
            "[('machine comprehension', 0.8160738819637753), ('application', 0.061308832678741675), ('attention', 0.06130881267874167), ('fusionnet', 0.06130874267874167)] \n",
            "\n",
            "[('linear bottlenecks', 0.8694942147232975), ('residuals', 0.06525294763835122), ('mobilenetv2', 0.06525291763835123)] \n",
            "\n",
            "[('autoencoders', 1.00000001)] \n",
            "\n",
            "[('deep unsupervised domain adaptation', 0.5714285814285716), ('transfer channel pruning', 0.4285714885714287)] \n",
            "\n",
            "[('natural language inference', 0.6000000000000001), ('mixed effects', 0.40000004)] \n",
            "\n",
            "[('shot medical image segmentation', 0.5000000799999998), ('learned transformations', 0.25000003), ('data augmentation', 0.25)] \n",
            "\n",
            "[('region proposal networks', 0.3614459031325301), ('time object detection', 0.36144586313253013), ('faster r', 0.24096385542168675), ('real', 0.01807234915662651), ('cnn', 0.01807231915662651)] \n",
            "\n",
            "[('inverse classification framework', 0.5825243118446601), ('smooth classifiers', 0.38834959456310664), ('budget', 0.029126223592233012)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('search engine', 0.48191615827999995), ('total volume', 0.4819160982799999), ('queries', 0.03616789344000001)] \n",
            "\n",
            "[('general purpose bayesian inference algorithm', 0.5555556155555554), ('stein variational gradient descent', 0.4444444444444443)] \n",
            "\n",
            "[('promotion time cure model', 0.9638554522554806), ('extension', 0.03614461774451953)] \n",
            "\n",
            "[('calf mri', 0.19703747960999932), ('human thigh', 0.19703744960999933), ('annotated images', 0.19703738960999934), ('semantic segmentation', 0.19703734960999933), ('cnn classifiers', 0.19703731960999932), ('application', 0.014813571950003417)] \n",
            "\n",
            "[('3d point cloud upsampling', 0.6201293825682378), ('centric network', 0.31006471128411883), ('geometry', 0.02326874204921443), ('net', 0.023268712049214428), ('pugeo', 0.023268692049214428)] \n",
            "\n",
            "[('dense object detection', 0.6000000300000001), ('focal loss', 0.4)] \n",
            "\n",
            "[('single shot multibox detector', 0.9638554222554806), ('ssd', 0.03614459774451953)] \n",
            "\n",
            "[('topological data analysis', 0.5825243218446602), ('gaussian kernel', 0.3883495345631066), ('persistence', 0.02912621359223301)] \n",
            "\n",
            "[('aware attentive knowledge tracing', 0.9638554222554806), ('context', 0.03614459774451953)] \n",
            "\n",
            "[('iterative visual attention', 0.9090909690909091), ('wild', 0.04545458545454546), ('recognition', 0.045454555454545464)] \n",
            "\n",
            "[('non - stationary spectral kernels', 0.9999999999999997)] \n",
            "\n",
            "[('linear machine learning models', 0.9638554322554806), ('persistence', 0.03614459774451953)] \n",
            "\n",
            "[('convex feasible set algorithm', 0.4444444544444443), ('real time optimization', 0.33333339333333334), ('motion planning', 0.2222223222222222)] \n",
            "\n",
            "[('deep ensembles', 0.32519476665675023), ('normal mixture', 0.3251947066567502), ('prediction intervals', 0.32519466665675023), ('quality', 0.024416070029749513)] \n",
            "\n",
            "[('subgraph count estimation', 0.32258077516129036), ('large state spaces', 0.3225807051612904), ('sequential stratified regeneration', 0.32258064516129037), ('application', 0.01612914225806452), ('mcmc', 0.01612907225806452)] \n",
            "\n",
            "[('region sampling', 0.46509704192617835), ('faster rcnn', 0.4650969919261783), ('study', 0.03490309807382164), ('implementation', 0.034903048073821635)] \n",
            "\n",
            "[('recursive tree search', 0.48780492804878056), ('compositional neural programs', 0.4878048880487806), ('planning', 0.02439033390243903)] \n",
            "\n",
            "[('object detection', 0.46509703192617835), ('optimal speed', 0.4650969819261783), ('accuracy', 0.034903088073821635), ('yolov4', 0.034903038073821634)] \n",
            "\n",
            "[('boltzmann machine training', 0.41958050958041954), ('las vegas', 0.2797203197202797), ('monte carlo', 0.27972028972027974), ('sets', 0.020979160979020983)] \n",
            "\n",
            "[('conversational recommender systems', 0.42857143857142865), ('semantic fusion', 0.2857143657142857), ('knowledge graph', 0.28571433571428567)] \n",
            "\n",
            "[('non - volumetric depth fusion', 0.7142857242857145), ('successive reprojections', 0.2857143557142857)] \n",
            "\n",
            "[('aware dialog', 0.32519475665675024), ('visual scene', 0.32519472665675025), ('simple baseline', 0.3251946766567502), ('audio', 0.024416040029749512)] \n",
            "\n",
            "[('neural proximal gradient iterations', 0.5000000499999998), ('compressive mr fingerprinting reconstruction', 0.4999999999999998)] \n",
            "\n",
            "[('visual recognition', 0.50000004), ('deep isometric', 0.5)] \n",
            "\n",
            "[('free planning', 0.8694942147232975), ('model', 0.06525294763835122), ('investigation', 0.06525292763835122)] \n",
            "\n",
            "[('multilingual pre -', 0.4195804295804195), ('level subwords', 0.27972037972027974), ('language model', 0.2797203297202797), ('byte', 0.020979100979020983)] \n",
            "\n",
            "[('semantic segmentation', 0.50000004), ('convolutional networks', 0.50000001)] \n",
            "\n",
            "[('non - factoid answer selection', 0.6134970025153375), ('deep learning models', 0.36809818950920253), ('lstm', 0.018404907975460127)] \n",
            "\n",
            "[('multidimensional scaling', 0.50000004), ('coordinate search', 0.50000001)] \n",
            "\n",
            "[('flexible cross - layer information inflows', 0.7228916362647086), ('deep networks', 0.24096387542156944), ('efficient', 0.018072339156861024), ('delugenets', 0.018072289156861022)] \n",
            "\n",
            "[('world models', 1.0)] \n",
            "\n",
            "[('regression recurrent neural networks', 0.40000008000000004), ('online human action detection', 0.4), ('joint classification', 0.20000005)] \n",
            "\n",
            "[('multilayer lstm networks', 0.36809825950920255), ('based action recognition', 0.3680982195092026), ('geometric features', 0.245398783006135), ('skeleton', 0.018404947975460127)] \n",
            "\n",
            "[('unrooted subtree prune', 0.6000000200000001), ('regraft distance', 0.40000008000000004)] \n",
            "\n",
            "[('vector space', 0.33333339333333334), ('word representations', 0.3333333633333333), ('efficient estimation', 0.3333333333333333)] \n",
            "\n",
            "[('multilingual knowledge graph completion', 0.7895744139845788), ('ensemble knowledge transfer', 0.5876884802726318)] \n",
            "\n",
            "[('deep residual learning', 0.6000000000000001), ('image recognition', 0.40000004)] \n",
            "\n",
            "[('deep residual learning', 0.6000000000000001), ('image recognition', 0.40000004)] \n",
            "\n",
            "[('automated differential diagnosis', 0.3333334333333333), ('disease inference', 0.2222222922222222), ('medical inquiry', 0.22222226222222222), ('bayesian approach', 0.2222222322222222)] \n",
            "\n",
            "[('sentiment prediction', 0.46509702192617836), ('aspect extraction', 0.4650969919261783), ('weakly', 0.03490315807382163), ('opinions', 0.034903048073821635)] \n",
            "\n",
            "[('differential geometry view', 0.9090909590909092), ('p$-laplacian', 0.045454565454545466), ('hypergraph', 0.04545454545454546)] \n",
            "\n",
            "[('systematic wavelength calibration', 0.5660378358490562), ('exoplanet xo-2b', 0.3773585505660378), ('spectroscopy', 0.028301916792452836), ('ground', 0.028301886792452834)] \n",
            "\n",
            "[('compressing physical properties', 0.42857142857142866), ('predictive chemistry', 0.2857143657142857), ('atomic species', 0.2857143257142857)] \n",
            "\n",
            "[('neural conversational model', 1.00000001)] \n",
            "\n",
            "[('based model predictive control', 0.42328047328042334), ('output lc filter', 0.3174604774603176), ('phase inverter', 0.21164033164021154), ('three', 0.015873115873015875), ('network', 0.015873045873015877), ('neural', 0.015873025873015877)] \n",
            "\n",
            "[('neural network explanation methods', 0.5000000099999999), ('morphosyntactic agreement', 0.25000009), ('hybrid documents', 0.25000006)] \n",
            "\n",
            "[('convolutional generative adversarial networks', 0.520128405571304), ('polyphonic music generation', 0.4414763821439917), ('binary neurons', 0.25000005)] \n",
            "\n",
            "[('approximate gaussian processes', 0.5825243218446602), ('quantile propagation', 0.3883495145631066), ('wasserstein', 0.029126243592233012)] \n",
            "\n",
            "[('readability assessment', 0.50000003), ('linguistic features', 0.5)] \n",
            "\n",
            "[('monocular video', 0.3174311965020752), ('consistent depth', 0.3174311165020752), ('unsupervised scale', 0.3174310865020752), ('motion', 0.02385345024688721), ('ego', 0.02385343024688721)] \n",
            "\n",
            "[('generative adversarial nets', 0.4878049380487806), ('missing data imputation', 0.4878048980487806), ('gain', 0.02439024390243903)] \n",
            "\n",
            "[('generative adversarial networks', 0.9999999999999999)] \n",
            "\n",
            "[('deep convolutional generative adversarial networks', 0.7142857542857144), ('unsupervised representation', 0.2857142857142857)] \n",
            "\n",
            "[('optimal transport', 0.33333341333333333), ('sparse alignments', 0.3333333833333333), ('text matching', 0.3333333433333333)] \n",
            "\n",
            "[('subspace optimization techniques', 0.9090909690909091), ('stochastic', 0.04545457545454546), ('seboost', 0.04545454545454546)] \n",
            "\n",
            "[('efficient neural architecture search system', 0.9433962764150946), ('keras', 0.028301906792452834), ('auto', 0.028301886792452834)] \n",
            "\n",
            "[('deep neural networks', 0.7688669347344762), ('binarized neural networks', 0.7688668847344762), ('activations', 0.03488383093023256), ('weights', 0.034883810930232566)] \n",
            "\n",
            "[('expression recognition', 0.8694942047232974), ('images', 0.06525300763835122), ('refer360$^\\\\circ$', 0.06525291763835123)] \n",
            "\n",
            "[('extreme conditions', 0.33333341333333333), ('audiovisual crowd', 0.3333333733333333), ('ambient sound', 0.3333333333333333)] \n",
            "\n",
            "[('social media', 0.33333339333333334), ('lexical variation', 0.3333333633333333), ('gender identity', 0.3333333333333333)] \n",
            "\n",
            "[('salient object detection', 0.5000000399999999), ('contour knowledge transfer', 0.4999999999999999)] \n",
            "\n",
            "[('simple data augmentation method', 0.5594405894405594), ('automatic speech recognition', 0.41958049958041954), ('specaugment', 0.020979020979020983)] \n",
            "\n",
            "[('universal language model fine', 0.650406500331913), ('text classification', 0.32520332016595654), ('tuning', 0.024390299502130588)] \n",
            "\n",
            "[('shot object detection', 0.5825243318446601), ('negative information', 0.3883495245631066), ('few', 0.02912625359223301)] \n",
            "\n",
            "[('recurrent neural network', 0.5660377458490563), ('deeper rnn', 0.37735861056603776), ('longer', 0.028301986792452834), ('indrnn', 0.028301936792452836)] \n",
            "\n",
            "[('predictive entropy search', 0.42857142857142866), ('unknown constraints', 0.2857143557142857), ('bayesian optimization', 0.2857143257142857)] \n",
            "\n",
            "[('resource language translation', 0.4878049480487806), ('optimal transformer depth', 0.4878048880487806), ('low', 0.02439029390243903)] \n",
            "\n",
            "[('non - negative matrix factorization', 0.39215688274509786), ('time streaming context', 0.2352943476470588), ('distribution function', 0.15686287509803912), ('atomic pair', 0.15686284509803913), ('real', 0.011764915882352943), ('data', 0.011764885882352943), ('pdf', 0.011764865882352944), ('assessment', 0.011764785882352942), ('validation', 0.011764705882352943)] \n",
            "\n",
            "[('multi - scale dense networks', 0.5555555555555555), ('resource efficient image classification', 0.44444450444444433)] \n",
            "\n",
            "[('time air pollution prediction model', 0.6134969525153374), ('spatiotemporal big data', 0.36809824950920256), ('real', 0.018404907975460127)] \n",
            "\n",
            "[('dense object detection', 0.6000000300000001), ('focal loss', 0.4)] \n",
            "\n",
            "[('3d convolutional networks', 0.6000000400000001), ('spatiotemporal features', 0.40000001)] \n",
            "\n",
            "[('co - part segmentation', 0.9638554322554806), ('motion', 0.03614459774451953)] \n",
            "\n",
            "[('supervised representation learning', 0.5660377758490562), ('event coreference', 0.37735850056603776), ('regularization', 0.028301996792452835), ('clustering', 0.028301966792452834)] \n",
            "\n",
            "[('deep multimodal feature', 0.6000000000000001), ('video ordering', 0.40000005)] \n",
            "\n",
            "[('disease digital biomarker discovery', 0.43715848994535544), ('inferred markov emissions', 0.3278689524590164), ('optimized transitions', 0.21857930497267755), ('parkinson', 0.016393442622950824)] \n",
            "\n",
            "[('sequence models', 0.8694942847232975), ('sequence', 0.43474716236164873), ('generate', 0.06525295763835123), ('plan', 0.06525291763835123)] \n",
            "\n",
            "[('question answering systems', 0.7947772754437973), ('question paraphrasing', 0.4819277811277403), ('robustness', 0.03614461774451953)] \n",
            "\n",
            "[('wasserstein procrustes', 0.4819161282799999), ('unsupervised alignment', 0.48191607827999994), ('embeddings', 0.036167873440000005)] \n",
            "\n",
            "[('unsupervised machine translation', 0.6000000000000001), ('monolingual corpora', 0.40000004)] \n",
            "\n",
            "[('mixed precision training', 0.9999999999999999)] \n",
            "\n",
            "[('efficient convolutional neural networks', 0.4444444444444443), ('heterogeneous hardware systems', 0.33333341333333333), ('pixelwise classification', 0.2222222722222222)] \n",
            "\n",
            "[('deep reinforcement learning', 0.6000000300000001), ('continuous control', 0.4)] \n",
            "\n",
            "[('neural network dynamics', 0.41095890410958896), ('free fine', 0.273972732739726), ('deep reinforcement', 0.27397267273972603), ('tuning', 0.020548105205479453), ('model', 0.020547985205479454)] \n",
            "\n",
            "[('explainable video action reasoning', 0.4999999999999999), ('state transitions', 0.25000008), ('prior knowledge', 0.25000005)] \n",
            "\n",
            "[('voice', 0.33333340333333333), ('face', 0.3333333733333333), ('speech2face', 0.3333333333333333)] \n",
            "\n",
            "[('realtime tracking', 0.50000003), ('simple online', 0.5)] \n",
            "\n",
            "[('image gan', 0.4819161282799999), ('cycle text', 0.48191607827999994), ('bert', 0.036167923440000006)] \n",
            "\n",
            "[('dialect segmentation', 0.4346990473215265), ('arabic multi', 0.4346990173215265), ('svm', 0.03265061133923674), ('crf', 0.03265059133923674), ('lstm', 0.03265057133923674), ('bi', 0.03265055133923674)] \n",
            "\n",
            "[('learning challenges', 0.24095815913999996), ('unbounded invention', 0.24095812913999998), ('ended reinforcement', 0.24095808913999997), ('enhanced poet', 0.24095803913999997), ('solutions', 0.018084081720000002), ('open', 0.018083951720000005)] \n",
            "\n",
            "[('incremental loop closure detection', 0.49079756571884947), ('proximity graphs', 0.24539887285942485), ('deep features', 0.24539884285942484), ('fast', 0.01840490856230069)] \n",
            "\n",
            "[('generative models', 0.50000005), ('distinguishability criteria', 0.50000001)] \n",
            "\n",
            "[('ultrasound scan plane detection', 0.9302216717581678), ('networks', 0.03488922412091613), ('attention', 0.03488919412091613)] \n",
            "\n",
            "[('order taylor methods', 0.5660377658490563), ('celestial mechanics', 0.37735858056603777), ('astrodynamics', 0.028301956792452836), ('high', 0.028301896792452836)] \n",
            "\n",
            "[('scene text script identification', 0.89877489465423), ('networks', 0.03374183511525659), ('ensembles', 0.03374180511525659), ('patch', 0.03374172511525659)] \n",
            "\n",
            "[('first instagram dataset', 0.9523809623809526), ('covid-19', 0.04761909761904763)] \n",
            "\n",
            "[('softmax', 0.25000007), ('margin', 0.25000004), ('distillation', 0.25000002), ('margindistillation', 0.25)] \n",
            "\n",
            "[('local adversarial disentangling network', 0.49079756571884947), ('de - makeup', 0.4046639533314243), ('facial makeup', 0.24539884285942473), ('ladn', 0.01840490856230069)] \n",
            "\n",
            "[('structured machine learning', 0.3000001100000001), ('dual optimization framework', 0.3000000700000001), ('efficient primal', 0.20000004000000002), ('accelerated communication', 0.20000001)] \n",
            "\n",
            "[('sequence pre - training', 0.5369128216778526), ('natural language generation', 0.4026846837583893), ('sequence', 0.09419524959254941), ('comprehension', 0.02013441818791947), ('translation', 0.020134388187919468), ('bart', 0.02013422818791947)] \n",
            "\n",
            "[('non - negative risk estimator', 0.6993007493006994), ('unlabeled learning', 0.27972029972027973), ('positive', 0.020979020979020983)] \n",
            "\n",
            "[('lifelong learning', 0.24539707903351451), ('unified deep', 0.24539701903351452), ('informatics modules', 0.2453969790335145), ('multiple facial', 0.2453969490335145), ('model', 0.01841241386594185)] \n",
            "\n",
            "[('robust human action recognition', 0.6201293625682378), ('feature integration', 0.3100646912841188), ('videos', 0.02326882204921443), ('pose', 0.023268712049214428), ('integralaction', 0.023268692049214428)] \n",
            "\n",
            "[('convolutional pixel adaptive image denoiser', 1.0000000099999997)] \n",
            "\n",
            "[('contrastive equilibrium learning', 0.4285714985714287), ('speaker recognition', 0.2857143257142857), ('unsupervised representation', 0.2857142857142857)] \n",
            "\n",
            "[('position map regression network', 0.40000008000000004), ('joint 3d face reconstruction', 0.4), ('dense alignment', 0.20000005)] \n",
            "\n",
            "[('fake news', 0.46509704192617835), ('trustworthiness indicators', 0.4650970019261783), ('web', 0.03490315807382163), ('source', 0.03490305807382164)] \n",
            "\n",
            "[('lasso l1 reweighting', 0.9523809923809525), ('analysis', 0.047619067619047634)] \n",
            "\n",
            "[('function approximation error', 0.5825242818446602), ('critic methods', 0.38834958456310664), ('actor', 0.029126263592233012)] \n",
            "\n",
            "[('functional theory', 0.24095816913999996), ('wave density', 0.24095813913999997), ('consistent description', 0.24095806913999998), ('implicit self', 0.24095803913999997), ('plane', 0.018084001720000003), ('electrolyte', 0.018083981720000003)] \n",
            "\n",
            "[('challenging nlu task', 0.6000000500000001), ('news headline', 0.4)] \n",
            "\n",
            "[('natural language interface', 0.6000000200000001), ('neural programmer', 0.40000006000000005)] \n",
            "\n",
            "[('neural network architectures', 0.8695652373912978), ('keras', 0.04347834086956741), ('codeepneat', 0.04347832086956741), ('neuroevolution', 0.04347826086956741)] \n",
            "\n",
            "[('descent back propagation', 0.41095901410958896), ('deep architecture', 0.27397276273972604), ('end learning', 0.273972642739726), ('end', 0.136986301369863), ('mirror', 0.020548035205479456), ('lda', 0.020548015205479456)] \n",
            "\n",
            "[('efficient structured inference', 0.41095890410958896), ('error states', 0.273972722739726), ('neural networks', 0.273972692739726), ('parsing', 0.020548015205479456), ('transition', 0.020547985205479454)] \n",
            "\n",
            "[('collaborative filtering', 0.50000003), ('variational autoencoders', 0.5)] \n",
            "\n",
            "[('multi - label submodular mrfs', 0.5555556055555555), ('memory efficient max flow', 0.4444444444444443)] \n",
            "\n",
            "[('incremental loop closure detection', 0.650406520331913), ('proximity graphs', 0.32520332016595654), ('fast', 0.024390249502130586)] \n",
            "\n",
            "[('richly annotated corpus', 0.4195804295804195), ('automated fact', 0.27972035972027975), ('different tasks', 0.2797203297202797), ('checking', 0.020979130979020984)] \n",
            "\n",
            "[('linguistic properties', 0.3333334633333333), ('sentence embeddings', 0.3333334333333333), ('single vector', 0.33333339333333334)] \n",
            "\n",
            "[('wasserstein exponential kernels', 0.9999999999999999)] \n",
            "\n",
            "[('open government data', 0.37500004000000003), ('spatial search strategies', 0.37500000000000006), ('systematic comparison', 0.25000009)] \n",
            "\n",
            "[('object detection', 0.33333339333333334), ('context reasoning', 0.3333333633333333), ('spatial memory', 0.3333333333333333)] \n",
            "\n",
            "[('deep reinforcement learning', 0.5660378458490563), ('# exploration', 0.37735849056603776), ('exploration', 0.1886793352830189), ('count', 0.028301946792452834), ('study', 0.028301926792452834)] \n",
            "\n",
            "[('neural conditional random field', 0.5714286114285715), ('cancer metastasis detection', 0.42857142857142866)] \n",
            "\n",
            "[('deep kernel learning', 0.3750000700000001), ('stepwise model selection', 0.37500000000000006), ('sequence prediction', 0.25000004)] \n",
            "\n",
            "[('stacked generative adversarial networks', 0.5479453054794517), ('realistic image synthesis', 0.410958964109589), ('photo', 0.020547985205479454), ('text', 0.020547965205479454)] \n",
            "\n",
            "[('graph residual network', 0.5660377558490562), ('deep gnns', 0.3773585605660378), ('animation', 0.028301996792452835), ('gresnet', 0.028301886792452834)] \n",
            "\n",
            "[('generative adversarial network', 0.8000000696341913), ('text', 0.040000150073161746), ('informative', 0.04000012007316175), ('diversity', 0.04000004007316175), ('gan', 0.04000002007316175), ('dp', 0.04000000007316175)] \n",
            "\n",
            "[('rank video representation', 0.5660377858490563), ('generative models', 0.37735849056603776), ('reconstruction', 0.028301976792452836), ('low', 0.028301916792452836)] \n",
            "\n",
            "[('end neural architecture search', 1.0000000899999995), ('end', 0.17545298167863288)] \n",
            "\n",
            "[('probabilistic semantic inpainting', 0.9090909090909092), ('cnns', 0.045454605454545466), ('pixel', 0.04545458545454546)] \n",
            "\n",
            "[('person pose estimation model', 0.6201293925682378), ('faster multi', 0.3100647212841188), ('stronger', 0.02326874204921443), ('deeper', 0.02326872204921443), ('deepercut', 0.023268692049214428)] \n",
            "\n",
            "[('double path networks', 0.9090909090909092), ('learning', 0.04545461545454546), ('sequence', 0.04545458545454546)] \n",
            "\n",
            "[('3d face shape', 0.6000000400000001), ('disentangled representation', 0.4)] \n",
            "\n",
            "[('generative adversarial networks', 0.7947773054437973), ('generator architecture', 0.4819277411277403), ('style', 0.03614460774451953)] \n",
            "\n",
            "[('discretizing quantitative attributes', 0.5825243118446601), ('linear classifiers', 0.38834959456310664), ('effectiveness', 0.02912623359223301)] \n",
            "\n",
            "[('bit supervision', 0.48191614827999996), ('sat solver', 0.4819160982799999), ('single', 0.03616789344000001)] \n",
            "\n",
            "[('creative adversarial networks', 0.5660377558490562), ('style norms', 0.3773586605660378), ('styles', 0.18867937528301887), ('art', 0.028301966792452834), ('generating', 0.028301946792452834)] \n",
            "\n",
            "[('rule lists', 0.48191613827999996), ('optimization approach', 0.48191608827999993), ('learning', 0.03616788344000001)] \n",
            "\n",
            "[('multilayer graph clustering', 0.5000000499999999), ('power mean laplacian', 0.5000000099999999)] \n",
            "\n",
            "[('deep reinforcement learning', 0.5000000399999999), ('dueling network architectures', 0.4999999999999999)] \n",
            "\n",
            "[('deep reinforcement learning', 0.6000000300000001), ('asynchronous methods', 0.4)] \n",
            "\n",
            "[('deep neural networks', 0.3000001200000001), ('relative attributing propagation', 0.3000000000000001), ('individual units', 0.20000009000000002), ('comparative contributions', 0.20000006)] \n",
            "\n",
            "[('end embedding learning', 0.46129316287204314), ('video object segmentation', 0.4195805295804195), ('fast end', 0.2797202997202797), ('feelvos', 0.020979020979020983)] \n",
            "\n",
            "[('wifi estimate person pose', 1.0000000099999995)] \n",
            "\n",
            "[('movie editing', 0.50000012), ('visual attention', 0.50000008), ('movies', 0.25000005)] \n",
            "\n",
            "[('superaccurate camera calibration', 0.6000000000000001), ('inverse rendering', 0.40000004)] \n",
            "\n",
            "[('generative adversarial networks', 0.7669923462421258), ('face generation', 0.4651108558790839), ('speech', 0.03488921412091613), ('wav2pix', 0.03488919412091613)] \n",
            "\n",
            "[('dimensional emotion analysis', 0.41095902410958896), ('representation format', 0.273972692739726), ('annotation perspective', 0.27397266273972604), ('impact', 0.020547985205479454), ('emobank', 0.020547945205479454)] \n",
            "\n",
            "[('sampling', 0.25000008), ('energy', 0.25000005), ('superpixels', 0.25000002), ('seeds', 0.25)] \n",
            "\n",
            "[('named entity recognition', 0.6000000300000001), ('neural architectures', 0.4)] \n",
            "\n",
            "[('visual question answering', 0.8695652773912977), ('vqa', 0.04347836086956741), ('architectures', 0.04347830086956741), ('attention', 0.04347826086956741)] \n",
            "\n",
            "[('abstractive summarization', 0.9301939538523567), ('bottom', 0.06980607614764327)] \n",
            "\n",
            "[('proximal policy optimization algorithms', 0.9999999999999996)] \n",
            "\n",
            "[('latent variable model approach', 0.5594405794405594), ('knowledge graph embedding', 0.41958048958041955), ('relwalk', 0.020979020979020983)] \n",
            "\n",
            "[('partial domain adaptation', 0.9523810023809526), ('examples', 0.04761907761904763)] \n",
            "\n",
            "[('end memory networks', 1.00000004), ('end', 0.2567881043628736)] \n",
            "\n",
            "[('hierarchical structure parsing', 0.5825242918446601), ('document renderings', 0.38834957456310665), ('docparser', 0.02912621359223301)] \n",
            "\n",
            "[('deep video inpainting', 0.9999999999999999)] \n",
            "\n",
            "[('feature pyramid networks', 0.6000000000000001), ('object detection', 0.40000004)] \n",
            "\n",
            "[('land cover classification', 0.3664718648927797), ('deep learning benchmark', 0.33333339333333334), ('land use', 0.22222232222222216), ('novel dataset', 0.22222225222222222)] \n",
            "\n",
            "[('novel sampling theorem', 0.6000000100000001), ('rotation group', 0.40000006000000005)] \n",
            "\n",
            "[('source code modeling', 0.6000000400000001), ('deep transfer', 0.4)] \n",
            "\n",
            "[('dense object detection', 0.6000000300000001), ('focal loss', 0.4)] \n",
            "\n",
            "[('soccer videos', 0.46509705192617834), ('scalable dataset', 0.4650969919261783), ('action', 0.03490309807382164), ('soccernet', 0.034903038073821634)] \n",
            "\n",
            "[('deep convolutional neural networks', 0.4907976157188495), ('robust multiple colorchecker detection', 0.49079756571884947), ('fast', 0.01840490856230069)] \n",
            "\n",
            "[('modular proximal optimization', 0.42857142857142866), ('variation regularization', 0.2857143557142857), ('multidimensional total', 0.2857143257142857)] \n",
            "\n",
            "[('organic reaction outcomes', 0.5825242818446602), ('lehman network', 0.38834958456310664), ('weisfeiler', 0.029126263592233012)] \n",
            "\n",
            "[('relaxed supervision', 1.00000002)] \n",
            "\n",
            "[('convolutional neural networks', 0.5660378058490563), ('oriented approximation', 0.37735853056603774), ('hardware', 0.028301906792452834), ('ristretto', 0.028301886792452834)] \n",
            "\n",
            "[('simple data augmentation method', 0.5594405894405594), ('automatic speech recognition', 0.41958049958041954), ('specaugment', 0.020979020979020983)] \n",
            "\n",
            "[('wmt19 news translation task submission', 0.7142857442857145), ('facebook fair', 0.2857142857142857)] \n",
            "\n",
            "[('vectors', 0.50000003), ('skip', 0.5)] \n",
            "\n",
            "[('graph convolutional networks', 0.5660378058490563), ('question answering', 0.37735849056603776), ('documents', 0.028301936792452836), ('reasoning', 0.028301916792452836)] \n",
            "\n",
            "[('hard negatives', 0.46509704192617835), ('semantic embeddings', 0.4650970119261783), ('visual', 0.03490306807382163), ('vse++', 0.034903038073821634)] \n",
            "\n",
            "[('convolutional layer', 0.7078931776151309), ('deconvolution layer', 0.707893117615131), ('same', 0.04761909761904763)] \n",
            "\n",
            "[('3d neural networks', 0.4285714885714287), ('lung nodules', 0.2857143157142857), ('diagnostic classification', 0.2857142857142857)] \n",
            "\n",
            "[('generative adversarial networks', 0.5825243218446602), ('realistic ecg', 0.3883495345631066), ('synthesis', 0.02912621359223301)] \n",
            "\n",
            "[('microsoft coco captions', 0.42857142857142866), ('evaluation server', 0.2857143557142857), ('data collection', 0.2857143257142857)] \n",
            "\n",
            "[('deep decision trees', 0.8266032620417758), ('decision stream', 0.6532064440835518)] \n",
            "\n",
            "[('object detection', 0.33333339333333334), ('context reasoning', 0.3333333633333333), ('spatial memory', 0.3333333333333333)] \n",
            "\n",
            "[('noisy time series data', 0.9638554322554806), ('causality', 0.03614460774451953)] \n",
            "\n",
            "[('double q', 0.4819161182799999), ('deep reinforcement', 0.48191607827999994), ('learning', 0.036167913440000005)] \n",
            "\n",
            "[('extensible neural machine translation toolkit', 0.9708738164077669), ('xnmt', 0.02912621359223301)] \n",
            "\n",
            "[('unbalanced segmentations', 0.46509707192617833), ('loss function', 0.46509703192617835), ('deep', 0.034903088073821635), ('dice', 0.034903048073821635)] \n",
            "\n",
            "[('stochastic gradient descent', 0.5660377758490562), ('uncertainty sampling', 0.37735849056603776), ('loss', 0.028301996792452835), ('zero', 0.028301966792452834)] \n",
            "\n",
            "[('influence functions', 0.33333339333333334), ('box predictions', 0.3333333633333333), ('understanding black', 0.3333333333333333)] \n",
            "\n",
            "[('partial convolutions', 0.48191613827999996), ('irregular holes', 0.4819161082799999), ('image', 0.03616784344000001)] \n",
            "\n",
            "[('mutual information', 0.48191613827999996), ('formal limitations', 0.48191607827999994), ('measurement', 0.03616788344000001)] \n",
            "\n",
            "[('rich 3d environment', 0.5825243418446602), ('generalizable agents', 0.3883495245631066), ('realistic', 0.029126263592233012)] \n",
            "\n",
            "[('temporal action localization', 0.6015167404583015), ('temporal adjacent networks', 0.6015167004583015), ('sparse', 0.029126223592233012)] \n",
            "\n",
            "[('time joint semantic reasoning', 0.6349201139374402), ('autonomous driving', 0.31746012696872017), ('real', 0.023809964546919842), ('multinet', 0.023809944546919842)] \n",
            "\n",
            "[('networks', 0.2500001), ('representation', 0.25000005), ('path', 0.25000002), ('hyper', 0.25)] \n",
            "\n",
            "[('deep recurrent spiking neural networks', 0.6250000700000001), ('train level backpropagation', 0.37500002000000004), ('spike', 0.1494519241912976)] \n",
            "\n",
            "[('meeg source connectivity analysis', 0.6666667166666665), ('leakage effect', 0.3333333533333333)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('medical imaging diagnosis', 0.4761905761904762), ('multi - modality', 0.47619053619047624), ('use', 0.023809563809523815), ('breastscreening', 0.023809523809523815)] \n",
            "\n",
            "[('deep reinforcement learning', 0.5000000399999999), ('dueling network architectures', 0.4999999999999999)] \n",
            "\n",
            "[('single shot face detector', 0.9302216717581678), ('context', 0.03488922412091613), ('pyramidbox', 0.03488919412091613)] \n",
            "\n",
            "[('experience replay', 1.00000001)] \n",
            "\n",
            "[('sequence modeling', 0.2500001), ('recurrent networks', 0.25000007), ('generic convolutional', 0.25000004), ('empirical evaluation', 0.25000001)] \n",
            "\n",
            "[('video synthesis', 1.00000004), ('video', 0.5)] \n",
            "\n",
            "[('cross - view geo', 0.4907976057188495), ('neural networks', 0.24539880285942486), ('lending orientation', 0.24539877285942485), ('localization', 0.01840501856230069)] \n",
            "\n",
            "[('image colorization', 0.4346990473215265), ('deep koalarization', 0.4346990173215265), ('v2', 0.03265061133923674), ('resnet', 0.03265059133923674), ('inception', 0.03265057133923674), ('cnns', 0.03265055133923674)] \n",
            "\n",
            "[('automatic speaker verification', 0.5825243318446601), ('contrastive predictive', 0.3883495145631066), ('feature', 0.02912625359223301)] \n",
            "\n",
            "[('archival holographic memory', 0.2090595834325916), ('wdm long', 0.1393731222883944), ('wireless channel', 0.1393730922883944), ('density parity', 0.1393730322883944), ('cyclic low', 0.1393730022883944), ('edge type', 0.1393729022883944), ('haul', 0.010453291680604077), ('codes', 0.010453211680604077), ('quasi', 0.010453121680604078), ('nodes', 0.010453091680604076)] \n",
            "\n",
            "[('multiple imputation', 0.8694941847232974), ('autoencoders', 0.06525297763835122), ('mida', 0.06525291763835123)] \n",
            "\n",
            "[('fake news articles', 0.9523809923809525), ('coherence', 0.047619067619047634)] \n",
            "\n",
            "[('semantic image segmentation', 0.47619055619047623), ('atrous separable convolution', 0.4761905161904762), ('decoder', 0.023809543809523815), ('encoder', 0.023809523809523815)] \n",
            "\n",
            "[('linear bottlenecks', 0.8694942147232975), ('residuals', 0.06525294763835122), ('mobilenetv2', 0.06525291763835123)] \n",
            "\n",
            "[('behavioral heterogeneity', 0.50000003), ('structural estimation', 0.5)] \n",
            "\n",
            "[('scale image recognition', 0.4878049480487806), ('deep convolutional networks', 0.4878048880487806), ('large', 0.02439029390243903)] \n",
            "\n",
            "[('direct lookup', 0.50000004), ('rotated bitboards', 0.50000001)] \n",
            "\n",
            "[('multilingual chatbot conversations', 0.37500004000000003), ('churn intent detection', 0.37500000000000006), ('social media', 0.25000008)] \n",
            "\n",
            "[('text normalization', 0.4819161282799999), ('decoder methods', 0.4819160982799999), ('encoder', 0.03616784344000001)] \n",
            "\n",
            "[('auxiliary classifier generative adversarial network', 0.9174312526605505), ('text', 0.027522975779816522), ('gan', 0.027522955779816522), ('tac', 0.027522935779816522)] \n",
            "\n",
            "[('auxiliary classifier gans', 0.5000000399999999), ('conditional image synthesis', 0.4999999999999999)] \n",
            "\n",
            "[('vectors', 0.50000003), ('skip', 0.5)] \n",
            "\n",
            "[('shot compositional learning', 0.5660378158490562), ('modular networks', 0.37735852056603775), ('zero', 0.028301946792452834), ('task', 0.028301886792452834)] \n",
            "\n",
            "[('e - commerce', 0.3614459231325301), ('question answering systems', 0.3614458831325301), ('domain relationships', 0.24096386542168674), ('retrieval', 0.01807235915662651), ('transfer', 0.01807232915662651)] \n",
            "\n",
            "[('powered empirical bayes estimation', 0.9638554222554806), ('covariate', 0.03614459774451953)] \n",
            "\n",
            "[('deep neural networks', 0.5825243118446601), ('natural image', 0.38834959456310664), ('patterns', 0.02912623359223301)] \n",
            "\n",
            "[('double q', 0.4819161182799999), ('deep reinforcement', 0.48191607827999994), ('learning', 0.036167913440000005)] \n",
            "\n",
            "[('level affect analysis', 0.41095893410958895), ('text features', 0.273972712739726), ('multimodal utterance', 0.273972602739726), ('audio', 0.020548035205479456), ('visual', 0.020548015205479456)] \n",
            "\n",
            "[('image transformations', 0.8160738219637753), ('model', 0.061308842678741676), ('dataset', 0.06130882267874167), ('gaze', 0.06130876267874168)] \n",
            "\n",
            "[('deep reinforcement learning', 0.4878049380487806), ('agnostic dynamics priors', 0.4878048980487806), ('task', 0.02439024390243903)] \n",
            "\n",
            "[('recurrent neural network explanations', 1.0000000099999995)] \n",
            "\n",
            "[('multi - party conversations', 0.43715851994535543), ('speaker interaction rnns', 0.3278689524590164), ('response selection', 0.21857925497267755), ('addressee', 0.016393442622950824)] \n",
            "\n",
            "[('edge computing networks', 0.3000001100000001), ('wireless powered mobile', 0.3000000700000001), ('online offloading', 0.20000004000000002), ('deep reinforcement', 0.2)] \n",
            "\n",
            "[('interpret federated learning', 0.6000000000000001), ('shapley values', 0.40000004)] \n",
            "\n",
            "[('silver standard corpus', 0.42857143857142865), ('gene relations', 0.2857143657142857), ('human phenotype', 0.28571433571428567)] \n",
            "\n",
            "[('slice residual networks', 0.5825242918446601), ('food recognition', 0.38834957456310665), ('wide', 0.02912621359223301)] \n",
            "\n",
            "[('tensor network contractor', 0.9090909390909092), ('matlab', 0.04545461545454546), ('ncon', 0.04545454545454546)] \n",
            "\n",
            "[('scale antenna arrays', 0.5660377858490563), ('deep learning', 0.37735858056603777), ('large', 0.028301916792452836), ('design', 0.028301896792452836)] \n",
            "\n",
            "[('reinforcement learning', 0.9301939638523566), ('representation', 0.06980608614764326)] \n",
            "\n",
            "[('monolithic neural event inference architecture', 0.9174312726605505), ('control', 0.027522985779816524), ('planning', 0.027522955779816522), ('learning', 0.027522935779816522)] \n",
            "\n",
            "[('neural networks', 0.50000003), ('diagonal rescaling', 0.5)] \n",
            "\n",
            "[('sparse projection oblique randomer forests', 0.9999999999999997)] \n",
            "\n",
            "[('neural image caption generator', 1.0000000499999995)] \n",
            "\n",
            "[('region proposal networks', 0.3614459031325301), ('time object detection', 0.36144586313253013), ('faster r', 0.24096385542168675), ('real', 0.01807234915662651), ('cnn', 0.01807231915662651)] \n",
            "\n",
            "[('inaugural pommerman team competition', 0.4907976357188495), ('top deep rl agent', 0.49079757571884947), ('skynet', 0.01840490856230069)] \n",
            "\n",
            "[('propagating activation differences', 0.6000000400000001), ('important features', 0.40000001)] \n",
            "\n",
            "[('deep networks', 0.50000003), ('axiomatic attribution', 0.5)] \n",
            "\n",
            "[('pixel recurrent neural networks', 0.9999999999999996)] \n",
            "\n",
            "[('single image super - resolution', 0.5555556055555555), ('enhanced deep residual networks', 0.4444444444444443)] \n",
            "\n",
            "[('human bandit feedback', 0.4878049680487806), ('neural semantic parser', 0.4878048980487806), ('counterfactual', 0.02439030390243903)] \n",
            "\n",
            "[('image manipulation detection', 0.6000000400000001), ('rich features', 0.40000001)] \n",
            "\n",
            "[('regret online learning', 0.5362649332687788), ('structured prediction', 0.32520331016595655), ('imitation learning', 0.3252032801659565), ('reduction', 0.024390259502130588)] \n",
            "\n",
            "[('general reinforcement', 0.726864450857648), ('algorithm', 0.05462725982847032), ('play', 0.054627199828470314), ('self', 0.05462717982847032), ('shogi', 0.054627159828470315), ('chess', 0.05462713982847032)] \n",
            "\n",
            "[('efficient interactive annotation', 0.8695652173912978), ('rnn++', 0.04347835086956741), ('polygon', 0.04347833086956741), ('segmentation', 0.04347830086956741)] \n",
            "\n",
            "[('better bitmap performance', 0.8266032220417758), ('roaring bitmaps', 0.6532064840835518)] \n",
            "\n",
            "[('financial portfolio management problem', 0.6504065703319131), ('deep reinforcement', 0.3252032601659565), ('framework', 0.024390289502130586)] \n",
            "\n",
            "[('slot attention', 0.4819161282799999), ('centric learning', 0.4819160982799999), ('object', 0.03616784344000001)] \n",
            "\n",
            "[('metric learning algorithms', 0.9523809923809525), ('metric', 0.24448785476582152), ('python', 0.04761912761904763)] \n",
            "\n",
            "[('connected convolutional networks', 1.00000001)] \n",
            "\n",
            "[('resource domain adaptation', 0.3225807851612903), ('gram representations', 0.21505385344086028), ('language models', 0.21505380344086028), ('pre -', 0.21505377344086027), ('low', 0.01612915225806452), ('n', 0.01612910225806452)] \n",
            "\n",
            "[('converse lyapunov function', 0.2586207896551725), ('line study', 0.17241401310344828), ('theoretical framework', 0.17241396310344828), ('gaussian process', 0.17241386310344828), ('power systems', 0.1724138331034483), ('off', 0.012931234482758623), ('part', 0.012931174482758621), ('attraction', 0.012931054482758622), ('region', 0.012931034482758622)] \n",
            "\n",
            "[('cosmic linear anisotropy', 0.5504587255963304), ('approximation schemes', 0.3669725870642202), ('ii', 0.027523025779816523), ('class', 0.027523005779816524), ('system', 0.027522985779816524)] \n",
            "\n",
            "[('linear bottlenecks', 0.8694942147232975), ('residuals', 0.06525294763835122), ('mobilenetv2', 0.06525291763835123)] \n",
            "\n",
            "[] \n",
            "\n",
            "[('archival holographic memory', 0.2090595834325916), ('wdm long', 0.1393731222883944), ('wireless channel', 0.1393730922883944), ('density parity', 0.1393730322883944), ('cyclic low', 0.1393730022883944), ('edge type', 0.1393729022883944), ('haul', 0.010453291680604077), ('codes', 0.010453211680604077), ('quasi', 0.010453121680604078), ('nodes', 0.010453091680604076)] \n",
            "\n",
            "[('face super - resolution', 0.6666666666666665), ('wasserstein gans', 0.3333333833333333)] \n",
            "\n",
            "[('noise', 0.50000003), ('smoothgrad', 0.5)] \n",
            "\n",
            "[('supervised learning algorithms', 0.4285714885714287), ('deep semi', 0.2857143157142857), ('realistic evaluation', 0.2857142857142857)] \n",
            "\n",
            "[('consistent individualized feature attribution', 0.6666666666666665), ('tree ensembles', 0.3333333833333333)] \n",
            "\n",
            "[('lenient multi - agent deep reinforcement learning', 1.0)] \n",
            "\n",
            "[('zip trees', 1.0)] \n",
            "\n",
            "[('ctr prediction', 0.4493875273271151), ('neural network', 0.44938749732711514), ('machine', 0.03374176511525659), ('factorization', 0.033741745115256586), ('deepfm', 0.03374171511525659)] \n",
            "\n",
            "[('attentive sentence embedding', 0.6000000400000001), ('structured self', 0.40000001)] \n",
            "\n",
            "[('interpretable partial substitute', 0.5660378158490562), ('cost transparency', 0.37735854056603774), ('low', 0.028301916792452836), ('free', 0.028301896792452836)] \n",
            "\n",
            "[('hierarchical cross - modal talking face generationwith dynamic pixel', 0.8181818181818183), ('wise loss', 0.18181828181818183)] \n",
            "\n",
            "[('deep residual learning', 0.6000000000000001), ('image recognition', 0.40000004)] \n",
            "\n",
            "[('box adversarial examples', 0.8695652873912978), ('black', 0.04347831086956741), ('hyperparameters', 0.04347829086956741), ('note', 0.04347827086956741)] \n",
            "\n",
            "[('hierarchical cross - modal', 0.4), ('wise loss', 0.20000011), ('dynamic pixel', 0.20000008000000002), ('face generation', 0.20000005)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('semantic image segmentation', 0.47619055619047623), ('atrous separable convolution', 0.4761905161904762), ('decoder', 0.023809543809523815), ('encoder', 0.023809523809523815)] \n",
            "\n",
            "[('efficient regional aggregation', 0.5825243318446601), ('image search', 0.3883496145631066), ('retrieve', 0.02912625359223301)] \n",
            "\n",
            "[('neural caption generators', 0.5825243518446601), ('visual information', 0.3883495545631066), ('amount', 0.02912623359223301)] \n",
            "\n",
            "[('graph generation', 0.50000004), ('robust representation', 0.50000001)] \n",
            "\n",
            "[('generative adversarial network', 0.5825242718446602), ('medical imaging', 0.3883495545631066), ('review', 0.02912629359223301)] \n",
            "\n",
            "[('optimal transport', 0.50000003), ('model fusion', 0.5)] \n",
            "\n",
            "[('attention', 1.0)] \n",
            "\n",
            "[('reading comprehension questions', 0.5825243018446602), ('real examinations', 0.38834958456310664), ('distractors', 0.029126223592233012)] \n",
            "\n",
            "[('dynamic community detection', 0.37500004000000003), ('dyncomm r package', 0.37500000000000006), ('evolving networks', 0.25000008)] \n",
            "\n",
            "[('robust facial landmark localisation', 0.4444444744444443), ('convolutional neural networks', 0.33333341333333333), ('wing loss', 0.2222222222222222)] \n",
            "\n",
            "[('region sampling', 0.46509704192617835), ('faster rcnn', 0.4650969919261783), ('study', 0.03490309807382164), ('implementation', 0.034903048073821635)] \n",
            "\n",
            "[('means space partitioning', 0.3333334133333333), ('equal intensity k', 0.33333337333333324), ('concept drift detection', 0.33333333333333326)] \n",
            "\n",
            "[('unsupervised monocular depth estimation', 0.650406500331913), ('right consistency', 0.32520332016595654), ('left', 0.024390299502130588)] \n",
            "\n",
            "[('variational inference', 0.48191613827999996), ('accelerating particle', 0.4819160982799999), ('understanding', 0.03616784344000001)] \n",
            "\n",
            "[('collective matrix completion', 0.9999999999999999)] \n",
            "\n",
            "[('feature pyramid networks', 0.6000000000000001), ('object detection', 0.40000004)] \n",
            "\n",
            "[('detecting ventricular fibrillation', 0.32258076516129036), ('machine learning techniques', 0.3225807251612904), ('ecg signals', 0.23971930035927158), ('signal processing', 0.23971919035927158), ('fusion', 0.01612906225806452), ('vfpred', 0.01612903225806452)] \n",
            "\n",
            "[('european transmission system', 0.4761905761904762), ('open optimisation model', 0.4761905261904762), ('eur', 0.023809543809523815), ('pypsa', 0.023809523809523815)] \n",
            "\n",
            "[('non - consecutive convolutions', 0.6075201995402918), ('non - linear', 0.4337312700537324), ('molding cnns', 0.27972027972027974), ('text', 0.020979050979020985)] \n",
            "\n",
            "[('generalized low rank models', 0.9999999999999996)] \n",
            "\n",
            "[('neural similarity encoders', 0.6000000400000001), ('pairwise relations', 0.40000001)] \n",
            "\n",
            "[('invariant face detector', 0.4878049380487806), ('single shot scale', 0.4878048980487806), ('s$^3$fd', 0.02439024390243903)] \n",
            "\n",
            "[('tatoeba translation challenge', 0.3333333433333333), ('multilingual mt', 0.2222223422222222), ('low resource', 0.22222231222222222), ('realistic data', 0.2222222722222222)] \n",
            "\n",
            "[('named entity recognition', 0.4878049480487806), ('pointwise mutual information', 0.48780490804878057), ('models', 0.02439025390243903)] \n",
            "\n",
            "[('clinical endpoint prediction', 0.37500009000000006), ('heterogeneous temporal events', 0.37500005000000003), ('joint representation', 0.25000002)] \n",
            "\n",
            "[('automated game design learning', 0.9999999999999996)] \n",
            "\n",
            "[('semantic segmentation', 0.7689600021483595), ('data', 0.05776012696291016), ('event', 0.057760096962910165), ('accidents', 0.05776006696291017), ('issafe', 0.057760006962910164)] \n",
            "\n",
            "[('electronic health records', 0.2870813897129187), ('drug interactions', 0.2134387515467762), ('known drug', 0.21343872154677618), ('age biases', 0.19138766980861244), ('wide analysis', 0.19138757980861243), ('administration', 0.014354216985645938), ('gender', 0.014354156985645936), ('city', 0.014354066985645937)] \n",
            "\n",
            "[('second', 0.33333339333333334), ('gigabyte', 0.3333333733333333), ('number', 0.3333333333333333)] \n",
            "\n",
            "[('time monocular depth estimation', 0.35874441461883405), ('image style transfer', 0.26905842596412555), ('domain adaptation', 0.17937229730941695), ('synthetic data', 0.17937226730941694), ('real', 0.013452914798206279)] \n",
            "\n",
            "[('semi - supervised detection', 0.4166667566666668), ('extreme weather events', 0.3125001900000001), ('scale climate', 0.20833338333333334), ('understanding', 0.015625170000000008), ('localization', 0.015625140000000006), ('large', 0.01562503000000001), ('extremeweather', 0.015625000000000007)] \n",
            "\n",
            "[('convolutional neural networks', 0.42857146857142864), ('coordconv solution', 0.2857143757142857), ('intriguing failing', 0.2857142957142857)] \n",
            "\n",
            "[('cherenkov telescope array', 0.9523809823809526), ('science', 0.04761904761904763)] \n",
            "\n",
            "[('runaway feedback loops', 0.6000000000000001), ('predictive policing', 0.40000004)] \n",
            "\n",
            "[('critic ensemble', 0.9301939838523566), ('actor', 0.06980611614764327)] \n",
            "\n",
            "[('scale image recognition', 0.4878049480487806), ('deep convolutional networks', 0.4878048880487806), ('large', 0.02439029390243903)] \n",
            "\n",
            "[('unifying mutual information view', 0.36363637363636364), ('cross - entropy', 0.2727273627272727), ('pairwise losses', 0.1818183118181818), ('metric learning', 0.18181824181818182)] \n",
            "\n",
            "[('attention generative adversarial networks', 0.9638554222554806), ('self', 0.03614459774451953)] \n",
            "\n",
            "[('predicting fluid intelligence', 0.5660377358490563), ('mr images', 0.3773585605660378), ('stacknet', 0.028301996792452835), ('children', 0.028301926792452834)] \n",
            "\n",
            "[('technical report', 0.3174312065020752), ('similarity search', 0.3174311465020752), ('index framework', 0.3174311165020752), ('gpu', 0.02385347024688721), ('generic', 0.023853380246887212)] \n",
            "\n",
            "[('machine learning algorithms', 0.8695652773912977), ('hyperparameters', 0.04347830086956741), ('importance', 0.04347828086956741), ('tunability', 0.04347826086956741)] \n",
            "\n",
            "[('differentiable sparsity allocation', 0.5825243418446602), ('more efficient', 0.3883495345631066), ('dsa', 0.02912621359223301)] \n",
            "\n",
            "[('recurrent slice networks', 0.42857142857142866), ('point clouds', 0.2857143557142857), ('3d segmentation', 0.2857143257142857)] \n",
            "\n",
            "[('deep bidirectional transformers', 0.3680982195092026), ('pre - training', 0.36809817950920254), ('language understanding', 0.245398873006135), ('bert', 0.018404907975460127)] \n",
            "\n",
            "[('reinforcement learning generalization', 0.6000000000000001), ('surprise minimization', 0.40000004)] \n",
            "\n",
            "[('aware tractable learning', 0.5825243018446602), ('probabilistic models', 0.38834958456310664), ('hardware', 0.029126223592233012)] \n",
            "\n",
            "[('efficient multi - scale training', 0.9708738064077669), ('sniper', 0.02912621359223301)] \n",
            "\n",
            "[('unaligned multimodal language sequences', 0.8686670579381718), ('multimodal transformer', 0.48881594391147476)] \n",
            "\n",
            "[('modal memes', 0.48191613827999996), ('hate speech', 0.48191608827999993), ('multi', 0.03616788344000001)] \n",
            "\n",
            "[('syntax tree networks', 0.5357143057142798), ('sql task', 0.35714299714285297), ('domaintext', 0.026785814285716785), ('cross', 0.026785794285716785), ('complex', 0.026785774285716785), ('syntaxsqlnet', 0.026785714285716786)] \n",
            "\n",
            "[('box adversarial attacks', 0.5660377858490563), ('prior convictions', 0.37735849056603776), ('priors', 0.18867935528301888), ('bandits', 0.028301976792452836), ('black', 0.028301916792452836)] \n",
            "\n",
            "[('edge flows', 0.4493875273271151), ('active learning', 0.44938749732711514), ('supervised', 0.03374176511525659), ('semi', 0.033741745115256586), ('graph', 0.03374171511525659)] \n",
            "\n",
            "[('differential evolution algorithm', 0.5660378058490563), ('gaussian process', 0.37735853056603774), ('optimization', 0.028301906792452834), ('hyper', 0.028301886792452834)] \n",
            "\n",
            "[('stochastic optimization', 0.8694942147232975), ('method', 0.06525294763835122), ('adam', 0.06525291763835123)] \n",
            "\n",
            "[('video domain adaptation', 0.5000000399999999), ('temporal attentive alignment', 0.4999999999999999)] \n",
            "\n",
            "[('hybrid reward architecture', 0.6000000000000001), ('reinforcement learning', 0.40000004)] \n",
            "\n",
            "[('accuracy', 0.33333339333333334), ('odds', 0.3333333733333333), ('robustness', 0.3333333333333333)] \n",
            "\n",
            "[('empirical bayes transductive meta', 0.650406500331913), ('synthetic gradients', 0.32520332016595654), ('learning', 0.024390299502130588)] \n",
            "\n",
            "[('level convolutional networks', 0.5825242918446601), ('text classification', 0.38834957456310665), ('character', 0.02912621359223301)] \n",
            "\n",
            "[('deep session interest network', 0.650406500331913), ('rate prediction', 0.32520333016595654), ('click', 0.024390299502130588)] \n",
            "\n",
            "[('detailed gpu simulator', 0.9090909690909091), ('workloads', 0.04545457545454546), ('machine', 0.045454555454545464)] \n",
            "\n",
            "[('variational objectives', 1.00000001)] \n",
            "\n",
            "[('monocular 3d object slam', 0.9638554222554806), ('cubeslam', 0.03614459774451953)] \n",
            "\n",
            "[('time series clustering', 0.5825242718446602), ('community detection', 0.3883495545631066), ('networks', 0.029126283592233012)] \n",
            "\n",
            "[('deep reinforcement learning', 0.5000000399999999), ('dueling network architectures', 0.4999999999999999)] \n",
            "\n",
            "[('deep reinforcement learning', 0.9090909590909092), ('improvements', 0.04545457545454546), ('rainbow', 0.04545454545454546)] \n",
            "\n",
            "[('convolutional neural networks', 0.5000000999999998), ('end compression framework', 0.5000000499999999), ('end', 0.12840841446151133)] \n",
            "\n",
            "[('time series', 0.4493875473271151), ('continuous modeling', 0.44938748732711514), ('bayes', 0.03374175511525659), ('ode', 0.03374173511525659), ('gru', 0.03374171511525659)] \n",
            "\n",
            "[('autoencoders', 1.00000001)] \n",
            "\n",
            "[('training data', 0.3174312065020752), ('confidence predictions', 0.3174311465020752), ('relu networks', 0.3174310965020752), ('problem', 0.02385356024688721), ('high', 0.02385341024688721)] \n",
            "\n",
            "[('end training', 0.48191619827999993), ('supervised learning', 0.4819160982799999), ('end', 0.24095811913999998), ('alternative', 0.03616790344000001)] \n",
            "\n",
            "[('recurrent neural network', 0.5825243018446602), ('image generation', 0.38834958456310664), ('draw', 0.02912621359223301)] \n",
            "\n",
            "[('scale image recognition', 0.4878049480487806), ('deep convolutional networks', 0.4878048880487806), ('large', 0.02439029390243903)] \n",
            "\n",
            "[('deep residual learning', 0.6000000000000001), ('image recognition', 0.40000004)] \n",
            "\n",
            "[('deep residual networks', 0.6000000300000001), ('identity mappings', 0.4)] \n",
            "\n",
            "[('excitation networks', 0.9301939638523566), ('squeeze', 0.06980607614764327)] \n",
            "\n",
            "[('deep neural networks', 0.5000000399999999), ('aggregated residual transformations', 0.4999999999999999)] \n",
            "\n",
            "[('max networks', 0.4493874773271151), ('recurrent sum', 0.4493874273271151), ('environments', 0.03374185511525659), ('decision', 0.03374179511525659), ('product', 0.033741745115256586)] \n",
            "\n",
            "[('trademark image retrieval', 0.8000001396341914), ('approach', 0.04000012007316175), ('learning', 0.04000009007316175), ('words', 0.04000006007316175), ('visual', 0.04000004007316175), ('hierarchy', 0.04000000007316175)] \n",
            "\n",
            "[('human preferences', 0.50000004), ('deep reinforcement', 0.5)] \n",
            "\n",
            "[('scene graph parsing', 0.42857145857142864), ('global context', 0.2857143557142857), ('neural motifs', 0.2857142857142857)] \n",
            "\n",
            "[('decoder scheme', 0.3174312165020752), ('latent space', 0.3174311565020752), ('illumination estimation', 0.3174311165020752), ('encoder', 0.023853480246887212), ('scene', 0.02385337024688721)] \n",
            "\n",
            "[('deep residual networks', 0.6000000300000001), ('identity mappings', 0.4)] \n",
            "\n",
            "[('social networks', 0.50000004), ('efficient referrals', 0.50000001)] \n",
            "\n",
            "[('semantic segmentation', 0.50000004), ('convolutional networks', 0.50000001)] \n",
            "\n",
            "[('multi - perspective business process anomaly classification', 1.00000002)] \n",
            "\n",
            "[('graph convolutional networks', 0.5825243418446602), ('joint embedding', 0.3883495145631066), ('structure', 0.029126243592233012)] \n",
            "\n",
            "[('speaker sensitive response evaluation model', 0.9999999999999997)] \n",
            "\n",
            "[('eclipse cdt code analysis', 0.6666666666666665), ('unit testing', 0.3333333833333333)] \n",
            "\n",
            "[('layer relu neural networks', 0.5594406394405593), ('spurious local minima', 0.41958041958041953), ('common', 0.020979060979020983)] \n",
            "\n",
            "[('multi - parametric mri', 0.4278996575923838), ('convolutional neural networks', 0.2955665324630542), ('co -', 0.24076550249283896), ('prostate cancer', 0.1970444349753695), ('detection', 0.014778405123152712)] \n",
            "\n",
            "[('trilinear attention sampling network', 0.6349201639374402), ('image recognition', 0.3174602069687202), ('details', 0.023810004546919842), ('devil', 0.023809974546919844)] \n",
            "\n",
            "[('conditional adversarial networks', 0.6000000700000001), ('image translation', 0.40000004), ('image', 0.2)] \n",
            "\n",
            "[('convolutional networks', 0.8694942247232974), ('dynamic', 0.06525294763835122), ('skipnet', 0.06525291763835123)] \n",
            "\n",
            "[('ambiguous images', 0.46509704192617835), ('probabilistic u', 0.4650969719261783), ('segmentation', 0.03490309807382164), ('net', 0.03490307807382163)] \n",
            "\n",
            "[('diachronic embedding projections', 0.5825243118446601), ('antisemitic language', 0.3883495245631066), ('france', 0.02912629359223301)] \n",
            "\n",
            "[('less evaluation', 0.8160738419637753), ('summarization', 0.061308842678741676), ('reference', 0.061308792678741675), ('highlight', 0.06130876267874168)] \n",
            "\n",
            "[('box adversarial attacks', 0.4651164090697675), ('effective initialization method', 0.4651163490697675), ('black', 0.023255923953488376), ('simple', 0.023255863953488377), ('paste', 0.023255833953488375)] \n",
            "\n",
            "[('accuracy', 0.33333339333333334), ('odds', 0.3333333733333333), ('robustness', 0.3333333333333333)] \n",
            "\n",
            "[('regression analysis', 0.32519474665675024), ('disease classification', 0.3251947166567502), ('topological descriptors', 0.32519466665675023), ('parkinson', 0.024416030029749514)] \n",
            "\n",
            "[('fast efficient hyperparameter tuning', 0.6666666666666665), ('policy gradients', 0.3333333833333333)] \n",
            "\n",
            "[('deep learning', 0.3333334233333333), ('pdf documents', 0.33333339333333334), ('similar tables', 0.3333333633333333)] \n",
            "\n",
            "[('bayesian sparsification methods', 0.5825242718446602), ('deep complex', 0.3883495545631066), ('networks', 0.02912629359223301)] \n",
            "\n",
            "[('professional photographs', 0.9301939638523566), ('web', 0.06980615614764327)] \n",
            "\n",
            "[('weak recist labels', 0.36809825950920255), ('ct lesion detection', 0.36809818950920253), ('dense masks', 0.245398843006135), ('retinanet', 0.01840491797546013)] \n",
            "\n",
            "[('big data', 0.50000003), ('gaussian processes', 0.5)] \n",
            "\n",
            "[('wide residual networks', 0.9999999999999999)] \n",
            "\n",
            "[('semantic image segmentation', 0.47619055619047623), ('atrous separable convolution', 0.4761905161904762), ('decoder', 0.023809543809523815), ('encoder', 0.023809523809523815)] \n",
            "\n",
            "[('time semantic segmentation', 0.5660378058490563), ('weight refinenet', 0.37735851056603775), ('real', 0.028301936792452836), ('light', 0.028301886792452834)] \n",
            "\n",
            "[('squares value iteration', 0.3750000700000001), ('frequentist regret bounds', 0.37500000000000006), ('randomized least', 0.25000004)] \n",
            "\n",
            "[('deep neural networks', 0.9090909590909092), ('sparsity', 0.04545457545454546), ('state', 0.045454555454545464)] \n",
            "\n",
            "[('traffic map movie forecasting', 0.650406520331913), ('team mie', 0.32520332016595654), ('lab', 0.024390349502130586)] \n",
            "\n",
            "[('shot video object segmentation', 1.0000000199999997)] \n",
            "\n",
            "[('datasets', 0.50000004), ('binary', 0.50000002)] \n",
            "\n",
            "[('morphologically rich languages', 0.5825243718446601), ('multilingual nmt', 0.3883495445631066), ('training', 0.02912621359223301)] \n",
            "\n",
            "[('chest abnormal imaging signs extraction', 0.9433962764150946), ('ernie', 0.028301916792452836), ('fine', 0.028301886792452834)] \n",
            "\n",
            "[('escape panic', 0.50000004), ('dynamical features', 0.50000001)] \n",
            "\n",
            "[('non - turing computations', 0.6349200739374402), ('hogarth space', 0.3174601069687202), ('times', 0.02381004454691984), ('malament', 0.023809994546919844)] \n",
            "\n",
            "[('semantic segmentation', 0.50000005), ('shot learning', 0.50000002)] \n",
            "\n",
            "[('regret minimizing sets', 0.5825243418446602), ('dynamic algorithm', 0.3883495345631066), ('k', 0.029126263592233012)] \n",
            "\n",
            "[('accented lexicon compression', 0.5825243218446602), ('sound rules', 0.3883495345631066), ('letter', 0.02912621359223301)] \n",
            "\n",
            "[('gravitational wave detection', 0.41958050958041954), ('frequency discriminator', 0.27972033972027976), ('squared time', 0.2797203097202797), ('chi', 0.020979030979020985)] \n",
            "\n",
            "[('neutral higgs particles', 0.42857145857142864), ('gluon collisions', 0.2857143757142857), ('pair production', 0.2857142857142857), ('gluon', 0.14285721285714284)] \n",
            "\n",
            "[('weak symmetry breaking', 0.41095895410958894), ('standard model', 0.27397276273972604), ('higgs boson', 0.273972722739726), ('electro', 0.020547975205479456), ('anatomy', 0.020547955205479456)] \n",
            "\n",
            "[('higgs boson', 0.9301939238523567), ('lhc', 0.06980612614764327)] \n",
            "\n",
            "[('quantum associative memory', 0.9999999999999999)] \n",
            "\n",
            "[('stabilizer circuits', 0.50000003), ('improved simulation', 0.5)] \n",
            "\n",
            "[('susy spectrum calculators', 0.4046639133314243), ('event generators', 0.24539891285942486), ('decay packages', 0.24539887285942485), ('susy les', 0.24539877285942474), ('accord', 0.01840493856230069)] \n",
            "\n",
            "[('lattice qcd', 0.9301939238523567), ('novices', 0.06980610614764327)] \n",
            "\n",
            "[('lep higgs statistics', 0.9090909490909092), ('lhc', 0.045454635454545464), ('challenges', 0.04545454545454546)] \n",
            "\n",
            "[('analytic confidence level calculations', 0.4999999999999999), ('fourier transform', 0.25000009), ('likelihood ratio', 0.25000006)] \n",
            "\n",
            "[('gravitational wave detection rates', 0.6666667166666665), ('double compact', 0.3333333333333333)] \n",
            "\n",
            "[('tight polygonal knots', 0.9523809723809525), ('ropelength', 0.04761904761904763)] \n",
            "\n",
            "[('quantile regression', 0.50000004), ('distributional reinforcement', 0.5)] \n",
            "\n",
            "[('pyramid scene', 0.9301939238523567), ('network', 0.06980610614764327)] \n",
            "\n",
            "[('reinforcement learning', 0.50000004), ('distributional perspective', 0.50000001)] \n",
            "\n",
            "[('incorporating user micro', 0.5217391304341128), ('item knowledge', 0.34782614695607544), ('recommendation', 0.026087126521962378), ('session', 0.026087096521962377), ('task', 0.02608706652196238), ('multi', 0.02608704652196238), ('behaviors', 0.026086996521962377)] \n",
            "\n",
            "[('data approximation', 0.50000003), ('topological grammars', 0.5)] \n",
            "\n",
            "[('deep bidirectional transformers', 0.3680982195092026), ('pre - training', 0.36809817950920254), ('language understanding', 0.245398873006135), ('bert', 0.018404907975460127)] \n",
            "\n",
            "[('conditional adversarial networks', 0.6000000700000001), ('image translation', 0.40000004), ('image', 0.2)] \n",
            "\n",
            "[('biomedical event extraction', 0.9523810223809526), ('pos', 0.04761905761904763)] \n",
            "\n",
            "[('proof method recommendation system', 0.89877487465423), ('hol', 0.03374180511525659), ('isabelle', 0.033741785115256585), ('pamper', 0.03374171511525659)] \n",
            "\n",
            "[('discretized logistic mixture likelihood', 0.650406560331913), ('other modifications', 0.3252033601659565), ('pixelcnn', 0.024390289502130586)] \n",
            "\n",
            "[('newborn eeg', 0.4650970119261783), ('quantitative features', 0.4650969819261783), ('matlab', 0.03490311807382163), ('neural', 0.034903038073821634)] \n",
            "\n",
            "[('natural language processing', 0.41095901410958896), ('error regularization', 0.273972682739726), ('selective prediction', 0.273972652739726), ('abstention', 0.020547975205479456), ('art', 0.020547955205479456)] \n",
            "\n",
            "[('knowledge base literals', 1.00000001)] \n",
            "\n",
            "[('plant diseases classification', 0.5000000399999999), ('deep interpretable architecture', 0.4999999999999999)] \n",
            "\n",
            "[('deep robot learning', 0.8695652873912978), ('rep', 0.04347831086956741), ('v', 0.04347829086956741), ('pyrep', 0.04347826086956741)] \n",
            "\n",
            "[('dynamic structural similarity', 0.9523809523809526), ('graphs', 0.04761908761904763)] \n",
            "\n",
            "[('tight differential privacy', 0.9523809623809526), ('fft', 0.04761910761904763)] \n",
            "\n",
            "[('reusable best practices', 0.2912622959223301), ('collective knowledge project', 0.2912621459223301), ('open apis', 0.1941748872815533), ('ml models', 0.1941748172815533), ('reproducible', 0.014563216796116505), ('portable', 0.014563196796116505)] \n",
            "\n",
            "[('additive margin softmax', 0.6000000000000001), ('face verification', 0.40000004)] \n",
            "\n",
            "[('stochastic kinetic treatment', 0.41958041958041953), ('macromolecular crowding', 0.27972037972027974), ('protein aggregation', 0.2797203197202797), ('effects', 0.020979100979020983)] \n",
            "\n",
            "[('directional contextual aggregating network', 0.5594405994405593), ('image semantic segmentation', 0.41958050958041954), ('bi', 0.020979040979020983)] \n",
            "\n",
            "[('imperceptible noises', 0.8694942547232974), ('tracker', 0.06525298763835123), ('attack', 0.06525294763835122)] \n",
            "\n",
            "[('thermodynamic neural network', 0.9999999999999999)] \n",
            "\n",
            "[('generator itemset mining', 0.41095903410958895), ('incomparability conditions', 0.273972652739726), ('solution dominance', 0.273972622739726), ('study', 0.020548055205479455), ('case', 0.020548035205479456)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('local polynomial image representations', 0.40395413662615626), ('distorted underwater images', 0.34280732052896684), ('compressive sensing', 0.1941746549165188), ('non -', 0.1941745549165188), ('combination', 0.014563762708702837), ('restoration', 0.014563662708702838)] \n",
            "\n",
            "[('margin distributions', 0.33333341333333333), ('deep networks', 0.3333333833333333), ('generalization gap', 0.3333333533333333)] \n",
            "\n",
            "[('aware visual compatibility prediction', 0.9638554222554806), ('context', 0.03614459774451953)] \n",
            "\n",
            "[('collaborative filtering', 0.50000004), ('deep autoencoders', 0.50000001)] \n",
            "\n",
            "[('graph convolutional neural networks', 0.9999999999999996)] \n",
            "\n",
            "[('scene character recognition', 0.5825242818446602), ('canonical forms', 0.38834957456310665), ('glyphs', 0.029126303592233012)] \n",
            "\n",
            "[('differentiable architecture search', 0.9523809723809525), ('darts', 0.04761904761904763)] \n",
            "\n",
            "[('large minibatch sgd', 0.8695652373912978), ('hour', 0.04347836086956741), ('imagenet', 0.04347833086956741), ('accurate', 0.04347826086956741)] \n",
            "\n",
            "[('factored generalized additive model', 0.4444444544444443), ('clinical decision support', 0.33333339333333334), ('operating room', 0.2222223322222222)] \n",
            "\n",
            "[('person re - identification', 0.5000000599999999), ('generative learning', 0.25000003), ('joint discriminative', 0.25)] \n",
            "\n",
            "[('open multilingual graph', 0.5825243118446601), ('general knowledge', 0.38834959456310664), ('conceptnet', 0.02912621359223301)] \n",
            "\n",
            "[('pddl problems', 0.4819161282799999), ('gym environments', 0.4819160982799999), ('pddlgym', 0.03616784344000001)] \n",
            "\n",
            "[('collaborative filtering', 0.50000004), ('deep autoencoders', 0.50000001)] \n",
            "\n",
            "[('aware deep metric learning', 0.9638554222554806), ('hardness', 0.03614459774451953)] \n",
            "\n",
            "[('prioritized experience replay', 1.00000001)] \n",
            "\n",
            "[('automatic machine', 0.9301939338523567), ('frameworks', 0.06980611614764327)] \n",
            "\n",
            "[('efficient probabilistic point', 0.3225806851612903), ('twist parameterization', 0.21505390344086028), ('gaussian filter', 0.21505387344086027), ('set registration', 0.2150538434408603), ('robust', 0.01612905225806452), ('filterreg', 0.01612903225806452)] \n",
            "\n",
            "[('local detection approach', 0.4623835614492874), ('named entity recognition', 0.41958049958041954), ('mention detection', 0.36532660345801565), ('fofe', 0.020979030979020985)] \n",
            "\n",
            "[('thompson sampling', 0.9301939538523567), ('tutorial', 0.06980608614764326)] \n",
            "\n",
            "[('binary classification', 0.9301939538523567), ('unfairness', 0.06980608614764326)] \n",
            "\n",
            "[('student curriculum', 0.9301939438523567), ('teacher', 0.06980607614764327)] \n",
            "\n",
            "[('multivariate information', 0.50000003), ('nonnegative decomposition', 0.5)] \n",
            "\n",
            "[('diffusion convolutional recurrent neural network', 0.6993006993006994), ('traffic forecasting', 0.27972036972027975), ('data', 0.020979080979020983)] \n",
            "\n",
            "[('effective malware family classification', 0.5479452854794518), ('novel feature extraction', 0.41095890410958896), ('fusion', 0.020548005205479454), ('selection', 0.020547985205479454)] \n",
            "\n",
            "[('image recognition', 0.9301939538523567), ('text', 0.06980607614764327)] \n",
            "\n",
            "[('unsupervised monocular depth estimation', 0.650406500331913), ('right consistency', 0.32520332016595654), ('left', 0.024390299502130588)] \n",
            "\n",
            "[('medical image segmentation', 0.42857150857142867), ('loss function', 0.28571433571428567), ('elastic interaction', 0.2857142957142857)] \n",
            "\n",
            "[('hyperparameter optimization', 0.46509705192617834), ('novel bandit', 0.4650969919261783), ('approach', 0.03490310807382163), ('hyperband', 0.034903038073821634)] \n",
            "\n",
            "[('comprehensive evaluation', 0.44938748732711514), ('shot learning', 0.4493874473271151), ('ugly', 0.03374187511525659), ('bad', 0.03374184511525659), ('good', 0.03374181511525659)] \n",
            "\n",
            "[('simple exponential family framework', 0.6504065103319131), ('shot learning', 0.32520333016595654), ('zero', 0.024390309502130586)] \n",
            "\n",
            "[('software engineering event modeling', 0.4444444444444443), ('temporal knowledge graphs', 0.33333341333333333), ('relative time', 0.2222222722222222)] \n",
            "\n",
            "[('cascade ranking models', 0.6000000300000001), ('joint optimization', 0.4)] \n",
            "\n",
            "[('flexible dual optimal inequalities', 0.49079758571884946), ('entity resolution', 0.24539888285942485), ('column generation', 0.24539878285942485), ('application', 0.01840499856230069)] \n",
            "\n",
            "[('deep neural networks', 0.9090909590909092), ('features', 0.04545457545454546), ('transferable', 0.045454555454545464)] \n",
            "\n",
            "[('deep nearest neighbor representations', 0.5714285814285716), ('differentiable boundary trees', 0.4285714885714287)] \n",
            "\n",
            "[('pretrained language models', 0.2500000399999999), ('inexpensive domain adaptation', 0.24999999999999994), ('covid-19 qa', 0.16666680666666667), ('biomedical ner', 0.16666677666666666), ('case studies', 0.16666674666666667)] \n",
            "\n",
            "[('openai remote rendering backend', 0.9638554222554806), ('orrb', 0.03614459774451953)] \n",
            "\n",
            "[('large annotated medical image', 0.6349200839374403), ('segmentation algorithms', 0.31746015696872015), ('evaluation', 0.02381004454691984), ('development', 0.023810024546919842)] \n",
            "\n",
            "[('spatial transformer networks', 0.5825243118446601), ('image registration', 0.3883496245631066), ('image', 0.1941747572815533), ('structure', 0.02912629359223301)] \n",
            "\n",
            "[('coherent point drift', 0.6000000500000001), ('set registration', 0.40000002), ('point', 0.2917921512697201)] \n",
            "\n",
            "[('generator versus segmentor', 0.5825242718446602), ('healthy synthesis', 0.38834957456310665), ('pseudo', 0.02912625359223301)] \n",
            "\n",
            "[('adaptive data representation', 0.4195804295804195), ('set registration', 0.27972035972027975), ('robust point', 0.2797203297202797), ('merging', 0.020979130979020984)] \n",
            "\n",
            "[('accurate point cloud registration', 0.6349200939374402), ('gaussian mixtures', 0.31746012696872017), ('trees', 0.023810014546919844), ('fast', 0.023809944546919842)] \n",
            "\n",
            "[('projective plane', 0.7689600921483595), ('maps', 0.057760096962910165), ('self', 0.05776007696291016), ('points', 0.057760056962910165), ('multipliers', 0.05776002696291017)] \n",
            "\n",
            "[('delay tcp', 0.8694941847232974), ('c2tcp', 0.06525296763835123), ('cellular', 0.06525291763835123)] \n",
            "\n",
            "[('accurate brain lesion segmentation', 0.43715857994535545), ('scale 3d cnn', 0.3278688824590164), ('efficient multi', 0.21857923497267756), ('crf', 0.016393532622950825)] \n",
            "\n",
            "[('scale video domain adaptation', 0.5594406194405593), ('temporal attentive alignment', 0.41958041958041953), ('large', 0.020979060979020983)] \n",
            "\n",
            "[('supervised sequence learning', 1.0000000199999999)] \n",
            "\n",
            "[('sequence models', 0.8694942547232974), ('sequence', 0.4347471323616487), ('vietnamese', 0.06525303763835123), ('-', 0.06525294763835122)] \n",
            "\n",
            "[('approximate curvature', 0.48191614827999996), ('neural networks', 0.48191608827999993), ('kronecker', 0.03616788344000001)] \n",
            "\n",
            "[('discretized logistic mixture likelihood', 0.650406560331913), ('other modifications', 0.3252033601659565), ('pixelcnn', 0.024390289502130586)] \n",
            "\n",
            "[('innovative adaptive kriging approach', 0.4444444544444443), ('efficient binary classification', 0.33333339333333334), ('mechanical problems', 0.2222223222222222)] \n",
            "\n",
            "[('object detectors', 0.8694942247232974), ('discriminability', 0.06525294763835122), ('transferability', 0.06525292763835122)] \n",
            "\n",
            "[('crop type mapping', 0.4878049480487806), ('time series dataset', 0.48780490804878057), ('breizhcrops', 0.02439024390243903)] \n",
            "\n",
            "[('decoupled deep neural network', 0.650406500331913), ('semantic segmentation', 0.32520333016595654), ('semi', 0.024390299502130588)] \n",
            "\n",
            "[('random network distillation', 0.9523809723809525), ('exploration', 0.04761904761904763)] \n",
            "\n",
            "[('attributed graph clustering', 0.5510110862735405), ('graph auto', 0.4353553758804141), ('encoder models', 0.3333333733333333)] \n",
            "\n",
            "[('adversarial noise', 0.33333339333333334), ('compressible signals', 0.3333333633333333), ('recovery guarantees', 0.3333333333333333)] \n",
            "\n",
            "[('english dependency treebank', 0.5825243218446602), ('prague czech', 0.3883495345631066), ('coreference', 0.02912621359223301)] \n",
            "\n",
            "[('fake news challenge stance detection task', 0.75000005), ('retrospective analysis', 0.25000001)] \n",
            "\n",
            "[('named entity recognition', 0.5000000499999999), ('finnish news corpus', 0.5000000099999999)] \n",
            "\n",
            "[('camera trap image review', 0.6666666966666666), ('efficient pipeline', 0.3333333333333333)] \n",
            "\n",
            "[('bayesian neural networks', 0.5825243318446601), ('approximate inference', 0.3883495445631066), ('loss', 0.02912621359223301)] \n",
            "\n",
            "[('neural text generation', 0.41958041958041953), ('biography domain', 0.27972037972027974), ('structured data', 0.2797203197202797), ('application', 0.020979090979020985)] \n",
            "\n",
            "[('efficient deployment', 0.9301940538523567), ('network', 0.06980615614764327)] \n",
            "\n",
            "[('model compression', 0.9301940638523567), ('efficacy', 0.06980617614764327)] \n",
            "\n",
            "[('alpha synaptic function', 0.36809822950920257), ('spiking neural networks', 0.36809818950920253), ('temporal coding', 0.245398773006135), ('backpropagation', 0.01840503797546013)] \n",
            "\n",
            "[('multilingual neural machine translation system', 0.8511803992648098), ('shot translation', 0.3745576544388282), ('zero', 0.023809613809523816), ('google', 0.023809523809523815)] \n",
            "\n",
            "[('bayesian hierarchical modelling', 0.5825243218446602), ('statistical comparison', 0.3883495145631066), ('classifiers', 0.029126243592233012)] \n",
            "\n",
            "[('local shrinkage priors', 0.5825243018446602), ('count data', 0.38834958456310664), ('global', 0.029126223592233012)] \n",
            "\n",
            "[('time series deconfounder', 0.41958041958041953), ('hidden confounders', 0.27972040972027973), ('treatment effects', 0.2797203297202797), ('time', 0.10764376318701385), ('presence', 0.020979130979020984)] \n",
            "\n",
            "[('aware citation recommendation', 0.5660378158490562), ('hybrid model', 0.37735852056603775), ('context', 0.028301946792452834), ('hybridcite', 0.028301886792452834)] \n",
            "\n",
            "[('deep unsupervised learning', 0.6000000000000001), ('nonequilibrium thermodynamics', 0.40000004)] \n",
            "\n",
            "[('large mini - batch object detector', 0.9756097860975611), ('megdet', 0.02439024390243906)] \n",
            "\n",
            "[('norm minimization', 0.32519476665675023), ('quadrature rules', 0.3251947066567502), ('sparse data', 0.32519466665675023), ('\\\\ell^p$-quasi', 0.02441608002974951)] \n",
            "\n",
            "[('particle swarm optimization clustering', 0.9638554322554806), ('tutorial', 0.03614460774451953)] \n",
            "\n",
            "[('private heavy hitters', 0.6000000300000001), ('lightweight techniques', 0.4)] \n",
            "\n",
            "[('meta vertex aggregation', 0.5504588255963303), ('keyword extraction', 0.3669725270642202), ('learning', 0.027523025779816523), ('rank', 0.027522955779816522), ('rakun', 0.027522935779816522)] \n",
            "\n",
            "[('gated shape cnns', 0.5825243118446601), ('semantic segmentation', 0.38834959456310664), ('gated', 0.14963528266417592), ('scnn', 0.02912623359223301)] \n",
            "\n",
            "[('deep neural networks', 0.37500004000000003), ('mean spectral normalization', 0.37500000000000006), ('embedded automation', 0.25000008)] \n",
            "\n",
            "[('orthogonal matching pursuit', 0.5000000499999999), ('scalable sparse subspace', 0.4999999999999999)] \n",
            "\n",
            "[('scalable elastic net subspace', 0.5594406194405593), ('active set algorithm', 0.4195804395804195), ('oracle', 0.020979020979020983)] \n",
            "\n",
            "[('conditional bert contextual augmentation', 0.9999999999999996)] \n",
            "\n",
            "[('deep model compression', 0.4878049380487806), ('aware network pruning', 0.4878048980487806), ('discrimination', 0.02439024390243903)] \n",
            "\n",
            "[('visual effects', 0.3174311965020752), ('ground segmentation', 0.3174311565020752), ('unsupervised meta', 0.3174310865020752), ('figure', 0.023853420246887212), ('learning', 0.023853400246887212)] \n",
            "\n",
            "[('super - resolution', 0.36809824950920256), ('time style transfer', 0.3680982095092025), ('perceptual losses', 0.245398773006135), ('real', 0.01840493797546013)] \n",
            "\n",
            "[('discrete architectural structures', 0.4285714985714287), ('robotic assembly', 0.2857143257142857), ('automated motion', 0.2857142857142857)] \n",
            "\n",
            "[('simplifying reinforcement learning research', 0.9638554222554806), ('mushroomrl', 0.03614459774451953)] \n",
            "\n",
            "[('dynamical stability perspective', 0.5504588455963303), ('global minima', 0.3669725170642202), ('learning', 0.02752303577981652), ('over', 0.027523005779816524), ('sgd', 0.027522945779816524)] \n",
            "\n",
            "[('roaring', 0.33333340333333333), ('bitmaps', 0.3333333833333333), ('smaller', 0.3333333633333333)] \n",
            "\n",
            "[('ultrasound tongue imaging', 0.36809827950920254), ('articulatory movement prediction', 0.36809823950920256), ('speech synthesis', 0.245398823006135), ('text', 0.01840491797546013)] \n",
            "\n",
            "[('community detection', 0.50000003), ('heat kernel', 0.5)] \n",
            "\n",
            "[('modular deep probabilistic programming', 0.9999999999999996)] \n",
            "\n",
            "[('graph convolutional networks', 0.6000000400000001), ('relational data', 0.40000001)] \n",
            "\n",
            "[('inverse imaging reconstruction', 0.5660378158490562), ('frequency domain', 0.37735854056603774), ('high', 0.028301916792452836), ('priors', 0.028301896792452836)] \n",
            "\n",
            "[('semantic expertise retrieval', 0.9090909490909092), ('efficient', 0.045454565454545466), ('unsupervised', 0.04545454545454546)] \n",
            "\n",
            "[('3d geometric constraints', 0.4026846837583893), ('monocular video', 0.26845646583892613), ('unsupervised learning', 0.2684563758389261), ('motion', 0.02013429818791947), ('ego', 0.02013427818791947), ('depth', 0.02013425818791947)] \n",
            "\n",
            "[('local explanations', 0.50000003), ('model agnostic', 0.5)] \n",
            "\n",
            "[('time series management systems', 0.9638554022554806), ('survey', 0.03614465774451953)] \n",
            "\n",
            "[('efficient neural architecture search', 0.6666666666666665), ('parameter sharing', 0.3333333833333333)] \n",
            "\n",
            "[('deep reinforcement', 1.0)] \n",
            "\n",
            "[('shape co - segmentation', 0.89877492465423), ('autoencoder', 0.03374176511525659), ('net', 0.03374173511525659), ('bae', 0.03374171511525659)] \n",
            "\n",
            "[('unsupervised defect segmentation', 0.5825242818446602), ('structural similarity', 0.38834957456310665), ('autoencoders', 0.029126303592233012)] \n",
            "\n",
            "[('time series classification', 0.5000000399999999), ('scalable dictionary classifiers', 0.4999999999999999)] \n",
            "\n",
            "[('stealthy cache attack', 0.9523810023809526), ('fast', 0.04761907761904763)] \n",
            "\n",
            "[('attention', 1.0)] \n",
            "\n",
            "[('benchmark time series database', 0.650406500331913), ('iot scenarios', 0.32520334016595653), ('benchmark', 0.11414174706313432), ('iotdb', 0.024390299502130588)] \n",
            "\n",
            "[('financial portfolio management problem', 0.6504065703319131), ('deep reinforcement', 0.3252032601659565), ('framework', 0.024390289502130586)] \n",
            "\n",
            "[('quantum internet software', 0.9090909690909091), ('simulator', 0.04545457545454546), ('simulaqron', 0.04545454545454546)] \n",
            "\n",
            "[('partial differential equations', 0.8695652973912977), ('algorithm', 0.04347831086956741), ('deep', 0.04347829086956741), ('dgm', 0.04347826086956741)] \n",
            "\n",
            "[('deep learning', 0.50000003), ('generative choreography', 0.5)] \n",
            "\n",
            "[('4d dynamic medical image', 0.5000000599999997), ('spatiotemporal volumetric interpolation network', 0.5000000099999998)] \n",
            "\n",
            "[('automatic neural network channel pruning', 0.7142857642857144), ('meta learning', 0.2857143057142857)] \n",
            "\n",
            "[('timescale spiking recurrent neural networks', 0.684931576849315), ('efficient computation', 0.273972622739726), ('multiple', 0.020547995205479456), ('effective', 0.020547945205479454)] \n",
            "\n",
            "[('variational adversarial active learning', 0.9999999999999996)] \n",
            "\n",
            "[('universal language model fine', 0.650406500331913), ('text classification', 0.32520332016595654), ('tuning', 0.024390299502130588)] \n",
            "\n",
            "[('lingual language model fine', 0.6349201239374402), ('efficient multi', 0.31746005696872015), ('tuning', 0.02381004454691984), ('multifit', 0.023809944546919842)] \n",
            "\n",
            "[('order logic programs', 0.9523809823809526), ('higher', 0.04761905761904763)] \n",
            "\n",
            "[('feature importance', 0.4650969919261783), ('unbiased measurement', 0.46509696192617833), ('methods', 0.034903128073821635), ('tree', 0.03490309807382164)] \n",
            "\n",
            "[('causal effect inference', 0.42857142857142866), ('variable models', 0.2857143557142857), ('deep latent', 0.2857143257142857)] \n",
            "\n",
            "[('causal sentence detection', 0.9523809823809526), ('transfer', 0.04761904761904763)] \n",
            "\n",
            "[('role factored tensor event embedding', 0.5362682338175156), ('conversational response re -', 0.4000000000000001), ('event causality', 0.23602031209482324)] \n",
            "\n",
            "[('cause pair extraction', 0.4195804395804195), ('emotion analysis', 0.27972037972027974), ('new task', 0.27972034972027976), ('emotion', 0.13986013986013987), ('texts', 0.020979150979020984)] \n",
            "\n",
            "[('fake news', 0.726864480857648), ('web', 0.054627319828470314), ('misinformation', 0.054627289828470316), ('spread', 0.054627239828470314), ('detecting', 0.054627199828470314), ('plugin', 0.05462717982847032)] \n",
            "\n",
            "[('worthy claims', 0.31743122650207517), ('multilingual check', 0.3174311965020752), ('cultural differences', 0.3174311565020752), ('checkthat', 0.02385339024688721), ('upv', 0.02385337024688721)] \n",
            "\n",
            "[('machine translation evaluation', 0.5825243518446601), ('contextual embeddings', 0.3883495645631066), ('evaluation', 0.1496352926641759), ('context', 0.029126243592233012)] \n",
            "\n",
            "[('domain question answering dataset', 0.6349201539374402), ('lingual open', 0.31746008696872013), ('cross', 0.023809974546919844), ('xqa', 0.023809944546919842)] \n",
            "\n",
            "[('decentralized artificial intelligence model network', 0.9708738164077669), ('daimon', 0.02912621359223301)] \n",
            "\n",
            "[('stage object detector', 0.5357144057142799), ('head r', 0.357142877142853), ('two', 0.026785814285716785), ('defense', 0.026785794285716785), ('cnn', 0.026785764285716787), ('light', 0.026785714285716786)] \n",
            "\n",
            "[('novel view synthesis', 0.23076938076923076), ('human motion imitation', 0.23076931076923077), ('liquid warping gan', 0.23076923076923075), ('appearance transfer', 0.15384627384615385), ('unified framework', 0.15384620384615386)] \n",
            "\n",
            "[('machine learning', 0.9301939538523567), ('justice', 0.06980608614764326)] \n",
            "\n",
            "[('unified network', 0.8160738519637752), ('text', 0.06130878267874167), ('fast', 0.06130876267874168), ('fots', 0.06130874267874167)] \n",
            "\n",
            "[('novel multiple classifier generation', 0.36363637363636364), ('individualized ensemble construction', 0.2727274027272727), ('fuzzy clustering', 0.18181828181818183), ('combination framework', 0.18181824181818182)] \n",
            "\n",
            "[('hierarchical importance', 0.9301939238523567), ('autoencoders', 0.06980610614764327)] \n",
            "\n",
            "[('environmental conditions', 0.283573713482768), ('covid-19 spread', 0.283573683482768), ('temporal study', 0.283573623482768), ('spain', 0.021325901364528065), ('catalonia', 0.021325881364528065), ('association', 0.021325801364528066), ('spatio', 0.021325741364528066), ('outcomes', 0.021325711364528068), ('choices', 0.021325681364528067), ('impact', 0.02132565136452807)] \n",
            "\n",
            "[('stochastic numerical methods', 0.42857145857142864), ('molecular sampling', 0.2857143557142857), ('rational construction', 0.2857142857142857)] \n",
            "\n",
            "[('principled trade', 0.8694941747232975), ('accuracy', 0.06525299763835123), ('robustness', 0.06525297763835122)] \n",
            "\n",
            "[('effective encoding', 0.3174311965020752), ('efficient training', 0.3174311665020752), ('graph parsing', 0.31743113650207516), ('transition', 0.02385339024688721), ('kÃ¸psala', 0.02385337024688721)] \n",
            "\n",
            "[('alternative surrogate loss', 0.5825242818446602), ('adversarial testing', 0.38834959456310664), ('pgd', 0.029126263592233012)] \n",
            "\n",
            "[('x - ray aperture photometry', 0.44843055327354264), ('newton archival observations', 0.26905844596412554), ('upper limit server', 0.26905831596412555), ('xmm', 0.013453044798206278)] \n",
            "\n",
            "[('quantum approximate optimization algorithm', 1.0000000099999995)] \n",
            "\n",
            "[('data', 0.33333340333333333), ('net', 0.3333333533333333), ('pde', 0.3333333333333333)] \n",
            "\n",
            "[('non - decomposable objectives', 0.6666666966666666), ('scalable learning', 0.3333333333333333)] \n",
            "\n",
            "[('multi - modal transformers', 0.606023917883172), ('answer questions', 0.30301198394158596), ('caption', 0.022741132043810564), ('paint', 0.022741112043810564), ('lxmert', 0.022741092043810564), ('x', 0.022741072043810564)] \n",
            "\n",
            "[('relaxed pruning', 0.25000009), ('label rules', 0.25000006), ('expressive multi', 0.25000003), ('efficient discovery', 0.25)] \n",
            "\n",
            "[('graph classification', 0.33333340333333333), ('lehman procedure', 0.3333333733333333), ('persistent weisfeiler', 0.3333333433333333)] \n",
            "\n",
            "[('label noise reduction', 0.41224101957471837), ('label embedding', 0.25000009999999995), ('heterogeneous partial', 0.25000007), ('entity typing', 0.25000004)] \n",
            "\n",
            "[('urban autonomous driving', 0.4761905761904762), ('exploring data aggregation', 0.4761904761904762), ('vision', 0.023809593809523816), ('policy', 0.023809563809523815)] \n",
            "\n",
            "[('graph structured prediction energy networks', 0.9999999999999997)] \n",
            "\n",
            "[('recommender systems', 0.4819161282799999), ('deep learning', 0.4819160982799999), ('wide', 0.03616784344000001)] \n",
            "\n",
            "[('interaction model', 0.46509705192617834), ('end neighborhood', 0.4650970119261783), ('end', 0.23254849096308916), ('recommendation', 0.03490318807382163), ('knowledge', 0.03490315807382163)] \n",
            "\n",
            "[('variational inference', 0.8694942347232975), ('variance', 0.06525296763835123), ('bias', 0.06525294763835122)] \n",
            "\n",
            "[('shot refinement neural network', 0.650406520331913), ('object detection', 0.32520332016595654), ('single', 0.024390249502130586)] \n",
            "\n",
            "[('conditional adversarial networks', 0.6000000700000001), ('image translation', 0.40000004), ('image', 0.2)] \n",
            "\n",
            "[('spiking neural networks', 0.6000000300000001), ('minibatch processing', 0.4)] \n",
            "\n",
            "[('multitask learners', 0.50000004), ('language models', 0.5)] \n",
            "\n",
            "[('qa websites', 0.46509702192617836), ('subjective features', 0.4650969719261783), ('bert', 0.034903128073821635), ('questions', 0.03490307807382163)] \n",
            "\n",
            "[('non - detection methodology', 0.3292182169958847), ('hierarchical bayesian framework', 0.24691364024691356), ('kilonova population properties', 0.24691359024691356), ('event analyses', 0.16460923349794238), ('single', 0.012345839012345683)] \n",
            "\n",
            "[('content adaptive resampler', 0.5825243318446601), ('image downscaling', 0.3883495245631066), ('upscaling', 0.02912625359223301)] \n",
            "\n",
            "[('stochastic chebyshev gradient descent', 0.6666666666666665), ('spectral optimization', 0.3333333833333333)] \n",
            "\n",
            "[('efficient softmax approximation', 0.9523809523809526), ('gpus', 0.04761908761904763)] \n",
            "\n",
            "[('end dialect recognition', 0.37500011000000005), ('convolutional neural networks', 0.37500000000000006), ('language embeddings', 0.25000004), ('end', 0.09632699419129756)] \n",
            "\n",
            "[('region proposal networks', 0.3614459031325301), ('time object detection', 0.36144586313253013), ('faster r', 0.24096385542168675), ('real', 0.01807234915662651), ('cnn', 0.01807231915662651)] \n",
            "\n",
            "[('neural unsupervised machine translation', 0.9638554422554806), ('phrase', 0.03614459774451953)] \n",
            "\n",
            "[('molecular dynamics', 0.46509703192617835), ('graining auto', 0.4650969819261783), ('encoders', 0.034903088073821635), ('coarse', 0.034903038073821634)] \n",
            "\n",
            "[('generative pre - training', 0.6666667066666665), ('language understanding', 0.3333333433333333)] \n",
            "\n",
            "[('semantic segmentation', 0.46509705192617834), ('dilated convolution', 0.4650969919261783), ('backbone', 0.03490310807382163), ('fastfcn', 0.034903038073821634)] \n",
            "\n",
            "[('computation efficient backbone network', 0.5369128016778526), ('time object detection', 0.4026846837583893), ('real', 0.020134328187919468), ('gpu', 0.02013425818791947), ('energy', 0.02013423818791947)] \n",
            "\n",
            "[('population average treatment effects', 0.9302216217581679), ('noncompliance', 0.03488927412091613), ('experiments', 0.03488925412091613)] \n",
            "\n",
            "[('camera model selection', 0.9523809723809525), ('uncertainty', 0.04761904761904763)] \n",
            "\n",
            "[('fashionable modelling', 0.9301939238523567), ('flux', 0.06980610614764327)] \n",
            "\n",
            "[('extended technical report', 0.2690584559641256), ('workload characterization', 0.17937232730941693), ('resource usage', 0.17937226730941694), ('datacenter operations', 0.17937223730941695), ('holistic analysis', 0.17937220730941694), ('energy', 0.013453014798206278)] \n",
            "\n",
            "[('lhc precision era', 0.4878049480487806), ('parton density access', 0.4878048980487806), ('lhapdf6', 0.02439024390243903)] \n",
            "\n",
            "[('chatbot models', 0.50000003), ('deep learning', 0.5)] \n",
            "\n",
            "[('strong redundancy properties', 0.5825243318446601), ('effective qbf', 0.3883495345631066), ('qratpre+', 0.02912621359223301)] \n",
            "\n",
            "[('temporal video similarity learning', 0.9302216817581679), ('spatio', 0.03488924412091613), ('visil', 0.03488919412091613)] \n",
            "\n",
            "[('conditional generative adversarial networks', 0.9302216717581678), ('identity', 0.03488922412091613), ('face', 0.03488919412091613)] \n",
            "\n",
            "[('gaussian copula', 0.32519476665675023), ('synthetic population', 0.32519473665675025), ('unified framework', 0.3251946966567502), ('sync', 0.024416000029749512)] \n",
            "\n",
            "[('convolutional neural networks', 0.42857142857142866), ('language processing', 0.2857143657142857), ('tree structures', 0.2857143257142857)] \n",
            "\n",
            "[('neural networks', 0.8694941847232974), ('fragile', 0.06525296763835123), ('interpretation', 0.06525291763835123)] \n",
            "\n",
            "[('recurrent neural networks', 0.42857150857142867), ('drug discovery', 0.28571433571428567), ('molecule libraries', 0.2857143057142857)] \n",
            "\n",
            "[('multi - scale deep network', 0.5000000899999999), ('depth map prediction', 0.3000000000000001), ('single image', 0.20000005)] \n",
            "\n",
            "[('mode normalization', 1.0)] \n",
            "\n",
            "[('robotic reinforcement learning', 0.4878049380487806), ('backward reachability curriculum', 0.4878048980487806), ('barc', 0.02439024390243903)] \n",
            "\n",
            "[('audio signal preprocessing methods', 0.43715849994535544), ('deep neural networks', 0.32786893245901644), ('music tagging', 0.21857935497267755), ('comparison', 0.016393452622950826)] \n",
            "\n",
            "[('many streaming processes', 0.4878049380487806), ('autonomous knowledge transfer', 0.4878048980487806), ('atl', 0.02439024390243903)] \n",
            "\n",
            "[('deep neural networks', 0.8695652973912977), ('training', 0.04347832086956741), ('framework', 0.04347829086956741), ('autoassist', 0.04347826086956741)] \n",
            "\n",
            "[('cosine series envelope', 0.32258080516129034), ('analytic signal', 0.21505388344086027), ('vocal training', 0.21505383344086026), ('interactive tools', 0.21505380344086028), ('time', 0.01612905225806452), ('real', 0.01612903225806452)] \n",
            "\n",
            "[('universal style transfer', 0.6000000600000001), ('form solution', 0.40000003)] \n",
            "\n",
            "[('depthwise convolutional kernels', 0.9523809823809526), ('mixconv', 0.04761904761904763)] \n",
            "\n",
            "[('ros multi - ontology references services', 0.5454545554545457), ('application prototyping issues', 0.2727273827272727), ('owl reasoners', 0.18181826181818184)] \n",
            "\n",
            "[('3d point clouds', 0.4285714885714287), ('geometric primitives', 0.2857143157142857), ('supervised fitting', 0.2857142857142857)] \n",
            "\n",
            "[('level convolutional networks', 0.5825242918446601), ('text classification', 0.38834957456310665), ('character', 0.02912621359223301)] \n",
            "\n",
            "[('positive integer', 0.8694941747232975), ('palindromes', 0.06525299763835123), ('sum', 0.06525296763835123)] \n",
            "\n",
            "[('neural networks', 0.50000005), ('training time', 0.50000002)] \n",
            "\n",
            "[('knowledge transfer', 0.9301939738523567), ('net2net', 0.06980607614764327)] \n",
            "\n",
            "[('saliency guided data augmentation strategy', 0.6993007293006994), ('better regularization', 0.27972036972027975), ('saliencymix', 0.020979020979020983)] \n",
            "\n",
            "[('permutation invariant graph neural network', 0.8732853348556358), ('graph classification', 0.377558781848273), ('pinet', 0.02439029001754598)] \n",
            "\n",
            "[('simple non - perturbative resummation schemes', 0.6857141764743879), ('case study', 0.22857150215812927), ('dimensions', 0.0171431162734966), ('theory', 0.0171430662734966), ('scalar', 0.0171430262734966), ('field', 0.0171429762734966), ('mean', 0.0171429562734966)] \n",
            "\n",
            "[('scenario creator', 1.00000003)] \n",
            "\n",
            "[('maven dependency graph', 0.7095880283066978), ('maven central', 0.3693572359494654), ('temporal graph', 0.36935717594946543), ('representation', 0.02912631359223301)] \n",
            "\n",
            "[('relu networks', 0.33333340333333333), ('certified robustness', 0.3333333733333333), ('fast computation', 0.3333333433333333)] \n",
            "\n",
            "[('pointwise common change', 0.48780492804878056), ('multivariate redundant information', 0.4878048880487806), ('surprisal', 0.02439033390243903)] \n",
            "\n",
            "[('hand keypoint detection', 0.5825242718446602), ('single images', 0.3883495545631066), ('multiview', 0.029126283592233012)] \n",
            "\n",
            "[('latent embedding optimization', 0.9090909490909092), ('learning', 0.045454565454545466), ('meta', 0.04545454545454546)] \n",
            "\n",
            "[('audio set classification', 0.42857142857142866), ('probabilistic perspective', 0.2857143657142857), ('attention model', 0.2857143257142857)] \n",
            "\n",
            "[('multi - level attention model', 0.6993006993006994), ('audio classification', 0.27972035972027975), ('weakly', 0.020979080979020983)] \n",
            "\n",
            "[('deep video completion', 0.5825243218446602), ('peel networks', 0.3883495345631066), ('onion', 0.02912621359223301)] \n",
            "\n",
            "[('network embedding', 0.50000004), ('training methods', 0.50000001)] \n",
            "\n",
            "[('central server free federated learning', 0.5464480874316942), ('sided trust social networks', 0.43715854994535547), ('single', 0.016393502622950824)] \n",
            "\n",
            "[('execute', 1.00000002)] \n",
            "\n",
            "[('incremental improvement', 0.9301939538523567), ('yolov3', 0.06980607614764327)] \n",
            "\n",
            "[('deep person re - identification', 0.50000008), ('batch normalization neck', 0.3000000400000001), ('strong baseline', 0.20000001)] \n",
            "\n",
            "[('abstract scene graphs', 0.48780501804878057), ('image caption generation', 0.4878049780487806), ('control', 0.024390323902439028)] \n",
            "\n",
            "[('perfect ground truth data', 0.4444445244444443), ('misaligned cadaster maps', 0.3333333733333333), ('noisy supervision', 0.2222222222222222)] \n",
            "\n",
            "[('sequence attentional neural machine translation', 0.9708738264077669), ('tree', 0.02912621359223301)] \n",
            "\n",
            "[('wide residual networks', 0.9999999999999999)] \n",
            "\n",
            "[('time convolutional neural networks', 0.6349200939374402), ('gender classification', 0.31746012696872017), ('emotion', 0.023810014546919844), ('real', 0.023809944546919842)] \n",
            "\n",
            "[('iterative feature refinement network', 0.89877489465423), ('mri', 0.033741825115256585), ('net', 0.03374173511525659), ('ifr', 0.03374171511525659)] \n",
            "\n",
            "[('hamiltonian sdes', 0.31003224727728235), ('diffusion processes', 0.3100321872772823), ('spectral density', 0.31003206727728233), ('illustration', 0.023301426056050907), ('abc', 0.023301346056050908), ('measure', 0.02330131605605091)] \n",
            "\n",
            "[('region proposal networks', 0.3614459031325301), ('time object detection', 0.36144586313253013), ('faster r', 0.24096385542168675), ('real', 0.01807234915662651), ('cnn', 0.01807231915662651)] \n",
            "\n",
            "[('localizable features', 0.32519475665675024), ('strong classifiers', 0.32519472665675025), ('regularization strategy', 0.3251946866567502), ('cutmix', 0.024416000029749512)] \n",
            "\n",
            "[('sample model hypotheses', 0.9090909890909091), ('ransac', 0.04545457545454546), ('neural', 0.04545454545454546)] \n",
            "\n",
            "[('corrupted image', 0.8160739019637753), ('self', 0.06130881267874167), ('clean', 0.06130878267874167), ('noisy', 0.06130874267874167)] \n",
            "\n",
            "[('camera localization', 0.4819161282799999), ('differentiable ransac', 0.4819160982799999), ('dsac', 0.03616784344000001)] \n",
            "\n",
            "[('dimensional heterogeneous media', 0.5660378358490562), ('continuum diffusion', 0.37735854056603774), ('models', 0.028301916792452836), ('advection', 0.028301886792452834)] \n",
            "\n",
            "[('batch normalization', 0.9301939938523567), ('things', 0.06980608614764326)] \n",
            "\n",
            "[('gated graph sequence neural networks', 0.9999999999999997)] \n",
            "\n",
            "[('directed gated graph network', 0.6201293225682378), ('shot learning', 0.3100647512841188), ('few', 0.02326878204921443), ('labeling', 0.023268712049214428), ('edge', 0.023268692049214428)] \n",
            "\n",
            "[('neural variational inference', 0.5825242718446602), ('belief networks', 0.38834957456310665), ('learning', 0.02912625359223301)] \n",
            "\n",
            "[('dynamic dedicated path protection', 0.6504065703319131), ('exact algorithm', 0.3252032901659565), ('efficient', 0.024390269502130586)] \n",
            "\n",
            "[('stochastic optimization', 0.8694942147232975), ('method', 0.06525294763835122), ('adam', 0.06525291763835123)] \n",
            "\n",
            "[('permutohedral lattice', 0.8160738519637752), ('convolutions', 0.061308792678741675), ('specific', 0.06130877267874167), ('task', 0.061308752678741675)] \n",
            "\n",
            "[('iris recognition system', 0.5660377858490563), ('convolutional networks', 0.37735859056603777), ('efficient', 0.028301916792452836), ('resource', 0.028301896792452836)] \n",
            "\n",
            "[('multi - scale', 0.5660378058490563), ('radar classification', 0.37735863056603774), ('rnns', 0.028302006792452834), ('size', 0.028301896792452836)] \n",
            "\n",
            "[('label distribution matters', 0.4711780048621563), ('supervised relation extraction', 0.42857153857142866), ('label noise', 0.28571430571428574)] \n",
            "\n",
            "[('large minibatch sgd', 0.8695652373912978), ('hour', 0.04347836086956741), ('imagenet', 0.04347833086956741), ('accurate', 0.04347826086956741)] \n",
            "\n",
            "[('neural architecture search', 0.3333333333333333), ('biological knowledge', 0.2222223222222222), ('predictive power', 0.2222222922222222), ('joint optimization', 0.22222226222222222)] \n",
            "\n",
            "[('counterfactual risk assessments', 0.9090909090909092), ('fairness', 0.04545461545454546), ('evaluation', 0.04545458545454546)] \n",
            "\n",
            "[('global image enhancement', 0.4878049380487806), ('neural curve layers', 0.4878048980487806), ('curl', 0.02439024390243903)] \n",
            "\n",
            "[('fiber analysis', 0.4650971019261783), ('mask r', 0.4650970119261783), ('fiber', 0.23254848096308917), ('image', 0.03490314807382163), ('cnn', 0.03490305807382164)] \n",
            "\n",
            "[('incremental improvement', 0.9301939538523567), ('yolov3', 0.06980607614764327)] \n",
            "\n",
            "[('neural machine translation', 0.9523809523809526), ('translate', 0.24448794476582153), ('align', 0.04761911761904763)] \n",
            "\n",
            "[('local binary convolutional neural networks', 0.9999999999999997)] \n",
            "\n",
            "[('knee point identification', 0.5825242718446602), ('off utility', 0.38834958456310664), ('trade', 0.029126263592233012)] \n",
            "\n",
            "[('ternary neural networks', 1.00000001)] \n",
            "\n",
            "[('unorganized 3d points', 0.380101934654065), ('point atrous convolution', 0.380101894654065), ('deep hierarchical encoder', 0.36809817950920254), ('decoder', 0.018404967975460127)] \n",
            "\n",
            "[('autonomous racing', 0.8694942147232975), ('trajectories', 0.06525294763835122), ('deepracing', 0.06525291763835123)] \n",
            "\n",
            "[('sequential monte carlo', 0.6000000100000001), ('surrogate likelihoods', 0.40000005)] \n",
            "\n",
            "[('nonlinear partial differential equations', 0.606023947883172), ('deep learning', 0.30301192394158594), ('solutions', 0.022741192043810563), ('data', 0.022741162043810565), ('part', 0.022741122043810565), ('physics', 0.022741072043810564)] \n",
            "\n",
            "[('order logic', 0.48191613827999996), ('neural networks', 0.48191608827999993), ('first', 0.03616788344000001)] \n",
            "\n",
            "[('causal hybrid automata recovery', 0.650406520331913), ('dynamic analysis', 0.32520332016595654), ('charda', 0.024390249502130586)] \n",
            "\n",
            "[('attention', 1.0)] \n",
            "\n",
            "[('large quantum circuits', 0.5825243018446602), ('continuous parameters', 0.38834958456310664), ('optimization', 0.029126223592233012)] \n",
            "\n",
            "[('external commonsense knowledge', 0.5000000499999999), ('event representation learning', 0.4999999999999999)] \n",
            "\n",
            "[('different representation learning strategies', 0.5201285455713041), ('deep music representation', 0.4414763121439917), ('comparative analysis', 0.25000011)] \n",
            "\n",
            "[('graph neural network', 0.5825242718446602), ('traffic forecasting', 0.3883495545631066), ('survey', 0.02912629359223301)] \n",
            "\n",
            "[('shot refinement neural network', 0.650406520331913), ('object detection', 0.32520332016595654), ('single', 0.024390249502130586)] \n",
            "\n",
            "[('threshold issuance selective disclosure credentials', 0.9174312126605505), ('ledgers', 0.027523045779816523), ('applications', 0.027523015779816522), ('coconut', 0.027522935779816522)] \n",
            "\n",
            "[('semi - discrete optimal transport', 0.6134969825153375), ('semi - discrete optimization', 0.530941130131211), ('neural architecture search', 0.36809829950920253), ('framework', 0.018405027975460127)] \n",
            "\n",
            "[('weakly supervised cell instance segmentation', 0.7142857142857144), ('detection response', 0.2857143657142857)] \n",
            "\n",
            "[('common semantic space', 0.5504587255963304), ('lingual meta', 0.3669725670642202), ('embeddings', 0.02752305577981652), ('cross', 0.027523005779816524), ('monolingual', 0.027522985779816524)] \n",
            "\n",
            "[('deep action recognition', 0.5504587455963303), ('new dataset', 0.3669726070642202), ('framework', 0.02752303577981652), ('adversarial', 0.027523015779816522), ('privacy', 0.027522935779816522)] \n",
            "\n",
            "[('world depth super - resolution', 0.5376344686021504), ('benchmark dataset', 0.21505388344086027), ('accurate real', 0.21505379344086029), ('baseline', 0.01612918225806452), ('fast', 0.01612904225806452)] \n",
            "\n",
            "[('hierarchical navigable small world graphs', 0.602409718554217), ('nearest neighbor search', 0.3614458231325301), ('robust', 0.01807230915662651), ('efficient', 0.01807228915662651)] \n",
            "\n",
            "[('3d keypoint estimation', 0.37500004000000003), ('unsupervised domain adaptation', 0.37500000000000006), ('view consistency', 0.25000008)] \n",
            "\n",
            "[('audio source separation', 0.8695653173912977), ('u', 0.04347832086956741), ('net', 0.04347828086956741), ('j', 0.04347826086956741)] \n",
            "\n",
            "[('multi - view images', 0.5369128516778525), ('aware 3d reconstruction', 0.40268460375838927), ('single', 0.020134308187919468), ('context', 0.02013424818791947), ('pix2vox', 0.02013422818791947)] \n",
            "\n",
            "[('hybrid bilinear models', 0.4878049380487806), ('epileptic seizure classification', 0.4878048780487806), ('symmetric', 0.02439028390243903)] \n",
            "\n",
            "[('bayesian online changepoint detection', 0.9999999999999996)] \n",
            "\n",
            "[('positive unlabeled dataset', 0.41958048958041955), ('image classification', 0.27972038972027974), ('relevant counter', 0.27972028972027974), ('examples', 0.020979060979020983)] \n",
            "\n",
            "[('non - parallel text', 0.5921478471359768), ('cross - alignment', 0.4407334093521277), ('style transfer', 0.25)] \n",
            "\n",
            "[('multi - stage dynamic generative adversarial networks', 0.7393360845881483), ('generate time', 0.2216937063407655), ('lapse videos', 0.20000005)] \n",
            "\n",
            "[('stellar model atmospheres', 0.5504587755963303), ('transit parameters', 0.3669726070642202), ('biases', 0.027523045779816523), ('exoplanets', 0.027522965779816524), ('darkening', 0.027522945779816524)] \n",
            "\n",
            "[('deep neural networks', 0.3278689624590164), ('adversarial robustness', 0.21857931497267757), ('probability estimation', 0.21857928497267756), ('output codes', 0.21857925497267755), ('error', 0.016393442622950824)] \n",
            "\n",
            "[('shot learning', 0.9301939638523566), ('networks', 0.06980608614764326)] \n",
            "\n",
            "[('multi - sense word embeddings', 0.7142857442857145), ('probabilistic fasttext', 0.2857142857142857)] \n",
            "\n",
            "[('temporal skip connections', 0.5825243318446601), ('visual planning', 0.3883495445631066), ('self', 0.02912621359223301)] \n",
            "\n",
            "[('improved residual networks', 0.5825242718446602), ('video recognition', 0.38834957456310665), ('image', 0.02912625359223301)] \n",
            "\n",
            "[('lda', 0.3333333833333333), ('guide', 0.3333333633333333), ('hitchhiker', 0.3333333433333333)] \n",
            "\n",
            "[('complex datasets', 0.8160738419637753), ('pdes', 0.061308792678741675), ('discovery', 0.06130877267874167), ('data', 0.06130874267874167)] \n",
            "\n",
            "[('efficient cnn architecture design', 0.5000000599999999), ('practical guidelines', 0.25000003), ('shufflenet v2', 0.25)] \n",
            "\n",
            "[('directed acyclic graphs', 0.42857145857142864), ('discrete data', 0.2857143557142857), ('penalized estimation', 0.2857142857142857)] \n",
            "\n",
            "[('c++ metaprogramming', 0.33333340333333333), ('stokes problems', 0.3333333733333333), ('linear solvers', 0.3333333433333333)] \n",
            "\n",
            "[('moment closure approximations', 0.9090909390909092), ('julia', 0.04545461545454546), ('momentclosure.jl', 0.04545454545454546)] \n",
            "\n",
            "[('programs', 0.50000005), ('deepcoder', 0.5)] \n",
            "\n",
            "[('Î±$-divergence minimization', 0.8694942047232974), ('box', 0.06525293763835123), ('black', 0.06525291763835123)] \n",
            "\n",
            "[('minimal achievable sufficient statistic learning', 0.9999999999999997)] \n",
            "\n",
            "[('label progression', 0.44589614563486013), ('label refinery', 0.4458960756348601), ('imagenet classification', 0.40000004)] \n",
            "\n",
            "[('dynamic 3d reconstruction', 0.3614458731325301), ('combining point tracking', 0.3614458031325301), ('part detection', 0.24096391542168674), ('vehicles', 0.01807241915662651), ('carfusion', 0.01807228915662651)] \n",
            "\n",
            "[('iterative joint image demosaicking', 0.5714285714285715), ('residual denoising network', 0.42857150857142867)] \n",
            "\n",
            "[('neural network hyper', 0.3125000400000001), ('weight decay', 0.20833355333333334), ('batch size', 0.20833349333333334), ('disciplined approach', 0.20833334333333334), ('momentum', 0.015625190000000008), ('rate', 0.015625140000000006), ('part', 0.015625100000000006), ('parameters', 0.015625080000000006)] \n",
            "\n",
            "[('music audio tagging', 0.5825243418446602), ('end learning', 0.3883495545631066), ('end', 0.1941747572815533), ('scale', 0.02912632359223301)] \n",
            "\n",
            "[('meta knowledge graph information', 0.4819273846622921), ('shot relations', 0.24096381733114597), ('hop reasoning', 0.240963767331146), ('few', 0.018072735337708003), ('multi', 0.018072685337708002)] \n",
            "\n",
            "[('object code sequences', 0.35502972579881664), ('arbitrary binary files', 0.35502968579881666), ('cpu architecture', 0.2366864405325444), ('endianness', 0.017751559289940832), ('detection', 0.017751509289940834), ('usable', 0.017751489289940834)] \n",
            "\n",
            "[('diffusion convolutional recurrent neural network', 0.591716026331361), ('scale traffic forecasting', 0.35502971579881665), ('large', 0.017751589289940833), ('partitioning', 0.017751499289940832), ('graph', 0.017751479289940832)] \n",
            "\n",
            "[('research papers', 0.4493875373271151), ('seeking questions', 0.4493874773271151), ('answers', 0.03374179511525659), ('information', 0.033741745115256586), ('dataset', 0.03374172511525659)] \n",
            "\n",
            "[('mandarin tts models', 0.5660378258490563), ('phonological knowledge', 0.37735853056603774), ('tones', 0.028301956792452836), ('phonetic', 0.028301906792452834)] \n",
            "\n",
            "[('approximate moment condition models', 0.6666666966666666), ('sensitivity analysis', 0.3333333333333333)] \n",
            "\n",
            "[('multi - scale time', 0.2826855623374355), ('deep convolutional neural networks', 0.28268551233743555), ('fusion devices', 0.14134293616871776), ('disruption prediction', 0.14134290616871778), ('series classification', 0.14134285616871778), ('application', 0.010600836818975395)] \n",
            "\n",
            "[('neural networks', 0.8694942147232975), ('rates', 0.06525293763835123), ('cyclical', 0.06525291763835123)] \n",
            "\n",
            "[('efficient neural network robustness certification', 0.6250000000000001), ('general activation functions', 0.3750000600000001)] \n",
            "\n",
            "[('general strategy', 0.31861317076017115), ('generative replay', 0.31861310076017113), ('continual learning', 0.2857143857142857), ('feedback connections', 0.2857143157142857)] \n",
            "\n",
            "[('convolutional neural networks', 0.37500004000000003), ('skin lesions classification', 0.37500000000000006), ('clinical images', 0.25000008)] \n",
            "\n",
            "[('generating structured queries', 0.42857144857142865), ('reinforcement learning', 0.2857143757142857), ('natural language', 0.2857143457142857)] \n",
            "\n",
            "[('complex models', 0.25000011), ('variational inference', 0.25000008), ('gradient descent', 0.25000005), ('simple natural', 0.25000002)] \n",
            "\n",
            "[('deep association metric', 0.5825243418446602), ('simple online', 0.3883495145631066), ('realtime', 0.029126243592233012)] \n",
            "\n",
            "[('variate normal distributions', 0.6955718325224596), ('spatial variational auto', 0.5850329404453936), ('matrix', 0.028301946792452834), ('encoding', 0.028301926792452834)] \n",
            "\n",
            "[('attention', 1.0)] \n",
            "\n",
            "[('aspect level sentiment classification', 0.5714285714285715), ('deep memory network', 0.42857147857142863)] \n",
            "\n",
            "[('generative adversarial networks', 0.9999999999999999)] \n",
            "\n",
            "[('efficient nonparametric statistical inference', 0.4444444444444443), ('population feature importance', 0.3333333833333333), ('shapley values', 0.22222231222222222)] \n",
            "\n",
            "[('linguistic cues', 0.4493875373271151), ('fake news', 0.4493874573271151), ('semantic', 0.03374180511525659), ('satire', 0.03374177511525659), ('nuances', 0.03374172511525659)] \n",
            "\n",
            "[('sequence decoder', 0.50000004), ('morphological analysis', 0.5)] \n",
            "\n",
            "[('mobilenetv3', 1.00000002)] \n",
            "\n",
            "[('semantic localization', 0.302976318063488), ('time metric', 0.30297628806348803), ('source library', 0.302976238063488), ('mapping', 0.022768018952384003), ('real', 0.022767938952384003), ('open', 0.022767888952384005), ('kimera', 0.022767858952384003)] \n",
            "\n",
            "[('non - linear system identification', 0.5679299268150328), ('power system dynamics', 0.33834246037938676), ('informed neural networks', 0.2955665224630542), ('physics', 0.014778325123152712)] \n",
            "\n",
            "[('non - invasive brain stimulation', 0.5540756650126984), ('personalized deep brain structures', 0.4429699254082011), ('semantic segmentation', 0.1970443849753695), ('end', 0.014778325123152712)] \n",
            "\n",
            "[('accurate peak location technique', 0.43715855994535546), ('sun envelope method', 0.3278688724590164), ('xanes measurements', 0.21857937497267757), ('automatic', 0.016393512622950825)] \n",
            "\n",
            "[('improved femur fracture classification', 0.4819274046622921), ('prior knowledge', 0.24096379733114598), ('scheduling data', 0.240963767331146), ('uncertainty', 0.018072765337708), ('curriculum', 0.018072625337708002)] \n",
            "\n",
            "[('model knowledge', 1.00000001)] \n",
            "\n",
            "[('positive characteristic', 0.33333340333333333), ('tate classes', 0.3333333733333333), ('effective obstruction', 0.3333333333333333)] \n",
            "\n",
            "[('biomedical image segmentation', 0.5660378058490563), ('convolutional networks', 0.37735853056603774), ('net', 0.028301906792452834), ('u', 0.028301886792452834)] \n",
            "\n",
            "[('cross - network node classification', 0.704588580527687), ('adversarial deep network embedding', 0.558395290477085)] \n",
            "\n",
            "[('population scale clustering', 0.5504587555963303), ('ancestry inferencing', 0.3669725770642202), ('bio', 0.027523015779816522), ('networks', 0.027522955779816522), ('convolutional', 0.027522935779816522)] \n",
            "\n",
            "[('aware seq2seq', 0.46509705192617834), ('text generation', 0.4650970019261783), ('structure', 0.03490310807382163), ('table', 0.034903038073821634)] \n",
            "\n",
            "[('deep neural networks', 0.6000000000000001), ('bot detection', 0.40000004)] \n",
            "\n",
            "[('unsupervised data augmentation', 0.6000000000000001), ('consistency training', 0.40000004)] \n",
            "\n",
            "[('temporal action detection', 0.5504588055963303), ('graph localization', 0.36697253706422023), ('sub', 0.027522975779816522), ('tad', 0.027522955779816522), ('g', 0.027522935779816522)] \n",
            "\n",
            "[('wise temporal convolutional networks', 0.5000000299999999), ('action localization', 0.25000008), ('deep concept', 0.25)] \n",
            "\n",
            "[('bayesian neural networks', 0.37500008000000007), ('cosmic microwave background', 0.37500004000000003), ('parameters estimation', 0.25)] \n",
            "\n",
            "[('cooperative data rate prediction', 0.4907975557188495), ('g networks', 0.24539888285942485), ('future mobile', 0.24539883285942485), ('vehicular', 0.01840499856230069)] \n",
            "\n",
            "[('fundamental physics', 0.726864450857648), ('society', 0.05462725982847032), ('machine', 0.054627199828470314), ('algorithms', 0.05462717982847032), ('science', 0.054627159828470315), ('non)-neutrality', 0.05462713982847032)] \n",
            "\n",
            "[('sanskrit language', 0.4346991273215265), ('neural compound', 0.4346990173215265), ('splitting', 0.03265058133923674), ('generation', 0.03265056133923674), ('sandhi', 0.03265054133923674), ('word', 0.03265052133923674)] \n",
            "\n",
            "[('deep convolutional neural networks', 0.4907976257188495), ('learnable parameters', 0.24539882285942485), ('gabor filters', 0.24539879285942484), ('gabornet', 0.01840490856230069)] \n",
            "\n",
            "[('computer vision', 0.50000005), ('inception architecture', 0.50000002)] \n",
            "\n",
            "[('formal controller synthesis', 0.42857142857142866), ('genetic programming', 0.2857143557142857), ('hybrid systems', 0.2857143257142857)] \n",
            "\n",
            "[('attention', 1.0)] \n",
            "\n",
            "[('public land disposal', 0.40268460375838927), ('counterfactual prediction', 0.2684565158389261), ('matrix completion', 0.2684564858389261), ('application', 0.02013431818791947), ('building', 0.02013424818791947), ('state', 0.02013422818791947)] \n",
            "\n",
            "[('micro - pcbs', 0.5504588555963303), ('sufficient diversity', 0.36697253706422023), ('classification', 0.02752305577981652), ('perspective', 0.027523025779816523), ('importance', 0.027522955779816522)] \n",
            "\n",
            "[('deep learning', 0.3333334333333333), ('model uncertainty', 0.33333340333333333), ('bayesian approximation', 0.3333333633333333)] \n",
            "\n",
            "[('nonparametric bayes package', 0.5825243018446602), ('julia language', 0.38834959456310664), ('gaussianprocesses.jl', 0.02912621359223301)] \n",
            "\n",
            "[('box adversarial attacks', 0.5660377558490562), ('limited queries', 0.3773585505660378), ('information', 0.028301976792452836), ('black', 0.028301886792452834)] \n",
            "\n",
            "[('memory aware synapses', 0.9999999999999999)] \n",
            "\n",
            "[('lightweight probabilistic deep networks', 0.9999999999999996)] \n",
            "\n",
            "[('probabilistic graphical model explanations', 0.5479452454794518), ('graph neural networks', 0.41095899410958897), ('explainer', 0.020547965205479454), ('pgm', 0.020547945205479454)] \n",
            "\n",
            "[('consistent adversarial networks', 0.4878049780487806), ('image translation', 0.3625943989772371), ('unpaired image', 0.36259434897723714), ('cycle', 0.024390323902439028)] \n",
            "\n",
            "[('systematic crosstalk mitigation', 0.5660377358490563), ('aware compilation', 0.37735858056603777), ('frequency', 0.028301956792452836), ('qubits', 0.028301936792452836)] \n",
            "\n",
            "[('optimization method', 0.7689600221483595), ('rates', 0.057760136962910165), ('adaptive', 0.05776011696291016), ('gradient', 0.05776003696291016), ('eve', 0.057760006962910164)] \n",
            "\n",
            "[('implicit feedback', 0.48191613827999996), ('bayesian personalized', 0.4819160982799999), ('bpr', 0.03616784344000001)] \n",
            "\n",
            "[('shot classification', 0.32519475665675024), ('multiple domains', 0.3251947066567502), ('universal representation', 0.32519466665675023), ('few', 0.024416070029749513)] \n",
            "\n",
            "[('human motion status', 0.5504587455963303), ('wall radar', 0.3669725970642202), ('uwb', 0.027523025779816523), ('ir', 0.027523005779816524), ('dataset', 0.027522945779816524)] \n",
            "\n",
            "[('accelerated parameter estimation', 0.9523809523809526), ('dale$Ï‡$', 0.04761908761904763)] \n",
            "\n",
            "[('deep bayesian optimization', 0.9523809523809526), ('graphs', 0.04761909761904763)] \n",
            "\n",
            "[('comparisons', 0.50000008), ('localization', 0.50000005)] \n",
            "\n",
            "[('generative adversarial imitation learning', 0.9999999999999996)] \n",
            "\n",
            "[('quo vadis submission', 0.9523809623809526), ('traffic4cast', 0.04761909761904763)] \n",
            "\n",
            "[('machine learning approach', 0.36809822950920257), ('human artists', 0.2735388419802136), ('artistic style', 0.2735387019802136), ('robotic painting', 0.245398803006135), ('brushstroke', 0.018405027975460127)] \n",
            "\n",
            "[('demystifying brain tumour segmentation networks', 0.6993006993006994), ('uncertainty analysis', 0.27972035972027975), ('interpretability', 0.020979080979020983)] \n",
            "\n",
            "[('deep learning', 0.50000004), ('poker probabilities', 0.50000001)] \n",
            "\n",
            "[('end speech recognition', 0.7688669647344761), ('deep speech', 0.6075012113294177), ('end', 0.16136571340505848), ('mandarin', 0.03488386093023257), ('english', 0.034883840930232564)] \n",
            "\n",
            "[('non - linear independent components estimation', 0.975609776097561), ('nice', 0.02439024390243906)] \n",
            "\n",
            "[('deep neural networks', 0.3278689624590164), ('gradient norm equality', 0.32786892245901644), ('modularized statistical framework', 0.3278688824590164), ('comprehensive', 0.016393452622950826)] \n",
            "\n",
            "[('normalized wasserstein distance', 0.3278688524590164), ('domain adaptation', 0.21857935497267755), ('adversarial learning', 0.21857932497267757), ('mixture distributions', 0.21857927497267757), ('applications', 0.016393512622950825)] \n",
            "\n",
            "[('higher order multivariate cumulants', 0.36363639363636363), ('small target detection', 0.2727273527272727), ('hyperspectral images', 0.18181830181818182), ('band selection', 0.18181818181818182)] \n",
            "\n",
            "[('autoregressive recurrent networks', 0.5825243218446602), ('probabilistic forecasting', 0.3883495345631066), ('deepar', 0.02912621359223301)] \n",
            "\n",
            "[('resource machine translation', 0.5217393004341128), ('transformer network', 0.3478261269560754), ('low', 0.02608710652196238), ('performance', 0.02608708652196238), ('efficiency', 0.026087056521962377), ('speed', 0.026087036521962377), ('auto', 0.026086956521962378)] \n",
            "\n",
            "[('context dependent semantic parsing', 0.9638554022554806), ('survey', 0.03614465774451953)] \n",
            "\n",
            "[('aware knowledge graph embeddings', 0.6504065303319131), ('link prediction', 0.32520333016595654), ('hierarchy', 0.024390259502130588)] \n",
            "\n",
            "[('embodied visual exploration', 1.0000000299999998), ('exploration', 0.2567881143628736)] \n",
            "\n",
            "[('weight normalization family', 0.7410645773170302), ('weight decay', 0.449387507327115), ('regularizer', 0.03374187511525659), ('l_2', 0.03374185511525659), ('disharmony', 0.03374173511525659)] \n",
            "\n",
            "[('attentive sentence embedding', 0.6000000400000001), ('structured self', 0.40000001)] \n",
            "\n",
            "[('learning deconvolution network', 0.6000000000000001), ('semantic segmentation', 0.40000004)] \n",
            "\n",
            "[('offline reinforcement learning', 0.9523809723809525), ('behavior', 0.04761904761904763)] \n",
            "\n",
            "[('wasserstein generative adversarial network', 0.5263158194736842), ('dose pet image', 0.39473699210526314), ('low', 0.0197369721052632), ('wgan', 0.019736942105263198), ('pt', 0.019736922105263198), ('parameter', 0.0197368421052632)] \n",
            "\n",
            "[('parsing expression grammars', 0.9523809823809526), ('recursion', 0.04761905761904763)] \n",
            "\n",
            "[('fast autoaugment', 1.0)] \n",
            "\n",
            "[('human atrial cells', 0.9090909690909091), ('unidentifiability', 0.04545457545454546), ('complexity', 0.045454555454545464)] \n",
            "\n",
            "[('structural equation models', 0.5504587455963303), ('simulation study', 0.3669726070642202), ('specification', 0.02752303577981652), ('miss', 0.027523015779816522), ('robust', 0.027522945779816524)] \n",
            "\n",
            "[('deep networks', 0.3174311865020752), ('fast adaptation', 0.3174311565020752), ('agnostic meta', 0.3174311065020752), ('learning', 0.023853420246887212), ('model', 0.02385337024688721)] \n",
            "\n",
            "[('shot neural machine translation', 0.6504065503319131), ('effective strategies', 0.3252032501659565), ('zero', 0.024390279502130588)] \n",
            "\n",
            "[('recurrent conditional gans', 0.4878049780487806), ('time series generation', 0.4878049380487806), ('medical', 0.02439028390243903)] \n",
            "\n",
            "[('automatic de - identification', 0.3940887399507389), ('dutch medical text', 0.2955666224630542), ('pattern matching method', 0.2955665324630542), ('deduce', 0.014778325123152712)] \n",
            "\n",
            "[('robust person re - identification', 0.7142857142857144), ('feature uncertainty', 0.2857143557142857)] \n",
            "\n",
            "[('linear storage cost', 0.41095900410958897), ('sequential data', 0.273972632739726), ('hierarchical encoding', 0.273972602739726), ('sub', 0.020548025205479454), ('compact', 0.020548005205479454)] \n",
            "\n",
            "[('shot object detection', 0.5504588155963303), ('rich features', 0.3669724870642202), ('single', 0.027523015779816522), ('speed', 0.027522995779816522), ('high', 0.027522975779816522)] \n",
            "\n",
            "[('variational divergence minimization', 0.4878049680487806), ('generative neural samplers', 0.48780492804878056), ('gan', 0.02439026390243903)] \n",
            "\n",
            "[('natural language toolkit', 0.9523809823809526), ('nltk', 0.04761904761904763)] \n",
            "\n",
            "[('support vector machine', 0.35502968579881666), ('convolutional neural network', 0.35502961579881664), ('image classification', 0.2366865605325444), ('svm', 0.01775161928994083), ('cnn', 0.017751549289940834), ('architecture', 0.017751489289940834)] \n",
            "\n",
            "[('deep forest', 1.0)] \n",
            "\n",
            "[('neural network architecture combining gated recurrent unit', 0.5051025376396165), ('network traffic data', 0.23587319946197274), ('support vector machine', 0.20979032979020978), ('intrusion detection', 0.13986032986013988), ('svm', 0.010489670489510492), ('gru', 0.010489600489510491)] \n",
            "\n",
            "[('signal processing', 0.46509704192617835), ('matrix completion', 0.4650969919261783), ('perspective', 0.03490309807382164), ('survey', 0.034903048073821635)] \n",
            "\n",
            "[('operator splitting method', 0.3550297457988167), ('box adversarial examples', 0.3550296457988167), ('free optimization', 0.23668652053254438), ('gradient', 0.017751589289940833), ('black', 0.017751519289940832), ('design', 0.017751499289940832)] \n",
            "\n",
            "[('online continual object detection', 0.6666666866666665), ('real world', 0.33333341333333333)] \n",
            "\n",
            "[('non - parallel many', 0.4024832456718879), ('star generative adversarial networks', 0.38834966456303455), ('many voice conversion', 0.2912622459222759), ('vc', 0.014563126796206693), ('stargan', 0.014563106796206694)] \n",
            "\n",
            "[('better storytellers', 0.50000006), ('language models', 0.50000003)] \n",
            "\n",
            "[('deep reinforcement learning', 0.3278689524590164), ('local map information', 0.32786891245901645), ('uav path planning', 0.3278688524590164), ('global', 0.016393482622950824)] \n",
            "\n",
            "[('implicit feature interactions', 0.5660377858490563), ('recommender systems', 0.37735858056603777), ('explicit', 0.028301916792452836), ('xdeepfm', 0.028301886792452834)] \n",
            "\n",
            "[('deep neural net', 0.4761905661904762), ('music mood detection', 0.4761904761904762), ('lyrics', 0.023809593809523816), ('audio', 0.023809573809523817)] \n",
            "\n",
            "[('image search', 0.33333340333333333), ('global features', 0.3333333733333333), ('deep local', 0.3333333433333333)] \n",
            "\n",
            "[('network design spaces', 0.6000000100000001), ('visual recognition', 0.40000005)] \n",
            "\n",
            "[('deep multi - view', 0.650406500331913), ('optimal cca', 0.32520333016595654), ('task', 0.024390309502130586)] \n",
            "\n",
            "[('generalized forensic framework', 0.8695652373912978), ('learning', 0.043478370869567406), ('deepfakes', 0.04347833086956741), ('tar', 0.04347826086956741)] \n",
            "\n",
            "[('continuous signed distance functions', 0.6666666966666666), ('shape representation', 0.33333341333333333)] \n",
            "\n",
            "[('intensive nlp tasks', 0.5660378058490563), ('augmented generation', 0.37735851056603775), ('knowledge', 0.028301936792452836), ('retrieval', 0.028301886792452834)] \n",
            "\n",
            "[('quantum state preparation', 0.60000008), ('conquer algorithm', 0.40000005)] \n",
            "\n",
            "[('chess piece recognition', 0.5660377558490562), ('neural networks', 0.37735858056603777), ('support', 0.028301956792452836), ('chessboard', 0.028301886792452834)] \n",
            "\n",
            "[('visual features', 0.33333339333333334), ('unsupervised learning', 0.3333333633333333), ('deep clustering', 0.3333333333333333)] \n",
            "\n",
            "[('unsupervised video generation', 0.4424792739056288), ('generative adversarial networks', 0.44247923390562877), ('image manifolds', 0.2857143157142857)] \n",
            "\n",
            "[('mutual information neural estimation', 0.9638554222554806), ('mine', 0.03614459774451953)] \n",
            "\n",
            "[('contrastive predictive coding', 0.9523809823809526), ('representation', 0.04761904761904763)] \n",
            "\n",
            "[('adversarial attacks', 0.48191613827999996), ('peer wisdom', 0.4819161082799999), ('peernets', 0.03616784344000001)] \n",
            "\n",
            "[('persian keyphrase generation', 0.6000000000000001), ('sequence models', 0.40000008000000004), ('sequence', 0.20000004000000002)] \n",
            "\n",
            "[('agnostic visiolinguistic representations', 0.5825243218446602), ('language tasks', 0.3883496445631066), ('task', 0.19417478728155332), ('vision', 0.029126303592233012)] \n",
            "\n",
            "[('weight sharing outperform random architecture search', 0.9523809623774537), ('tunas', 0.023809633811273188), ('investigation', 0.023809613811273188)] \n",
            "\n",
            "[('programs', 0.50000005), ('deepcoder', 0.5)] \n",
            "\n",
            "[('manifold regularization', 0.9301939538523567), ('causal', 0.06980607614764327)] \n",
            "\n",
            "[('realistic danmaku generation', 0.5825243018446602), ('sequential gans', 0.38834958456310664), ('keiki', 0.02912621359223301)] \n",
            "\n",
            "[('open vocabulary neural machine translation', 0.5555555655555555), ('character models', 0.2222223222222222), ('hybrid word', 0.2222222922222222)] \n",
            "\n",
            "[('linear equations', 0.4819161282799999), ('qubo formulations', 0.48191607827999994), ('system', 0.036167873440000005)] \n",
            "\n",
            "[('factored approximate fisher matrix', 0.6504065303319131), ('convolution layers', 0.32520333016595654), ('kronecker', 0.024390259502130588)] \n",
            "\n",
            "[('promoted listings', 0.4346991373215265), ('rate prediction', 0.4346991073215265), ('etsy', 0.03265064133923674), ('click', 0.03265055133923674), ('approach', 0.03265053133923674), ('ensemble', 0.03265050133923674)] \n",
            "\n",
            "[('time object detection', 0.2870814897129187), ('convolutional neural networks', 0.2870814297129187), ('autonomous driving', 0.19138774980861245), ('low power', 0.19138761980861244), ('real', 0.014354196985645936), ('small', 0.014354106985645937), ('squeezedet', 0.014354066985645937)] \n",
            "\n",
            "[('deep generative networks', 0.6000000000000001), ('sequence prediction', 0.40000004)] \n",
            "\n",
            "[('multimodal distribution alignment', 0.5000000399999999), ('hierarchical optimal transport', 0.4999999999999999)] \n",
            "\n",
            "[('negative casimir entropies', 0.4285714885714287), ('dissipative origins', 0.2857143157142857), ('disentangling geometric', 0.2857142857142857)] \n",
            "\n",
            "[('vocabulary neural machine translation', 0.5594406294405594), ('latent morphology model', 0.4195804295804195), ('open', 0.020979070979020985)] \n",
            "\n",
            "[('medical image segmentation', 0.5504588155963303), ('faster encoders', 0.3669725270642202), ('transformer', 0.027523015779816522), ('unet', 0.027522955779816522), ('levit', 0.027522935779816522)] \n",
            "\n",
            "[('rem sleep behaviour disorder', 0.6666666866666665), ('minimal sensors', 0.33333340333333333)] \n",
            "\n",
            "[('convolutional neural networks', 0.5825242718446602), ('rank regularization', 0.38834957456310665), ('low', 0.02912625359223301)] \n",
            "\n",
            "[('vector space', 0.33333339333333334), ('word representations', 0.3333333633333333), ('efficient estimation', 0.3333333333333333)] \n",
            "\n",
            "[('lung nodule classification', 0.42857142857142866), ('global networks', 0.2857143557142857), ('deep local', 0.2857143257142857)] \n",
            "\n",
            "[('multi - ingredient pizza image generator', 0.7361963490184047), ('conditional stylegans', 0.245398873006135), ('mpg', 0.01840490797546013)] \n",
            "\n",
            "[('evaluation plan', 0.48191614827999996), ('distance challenge', 0.4819161182799999), ('voices', 0.03616785344000001)] \n",
            "\n",
            "[('aware neural language models', 0.9638554222554806), ('character', 0.03614459774451953)] \n",
            "\n",
            "[('smoothed inference', 0.8694941647232974), ('models', 0.06525297763835122), ('adversarially', 0.06525294763835122)] \n",
            "\n",
            "[('deep neural networks', 0.5825243318446601), ('selective sampling', 0.3883495145631066), ('training', 0.02912625359223301)] \n",
            "\n",
            "[('neural multisensory scene inference', 0.9999999999999996)] \n",
            "\n",
            "[('approximate', 0.33333339333333334), ('hard', 0.3333333533333333), ('tetris', 0.3333333333333333)] \n",
            "\n",
            "[('independent secret sharing', 0.4195804395804195), ('bell nonlocality', 0.27972037972027974), ('stronger form', 0.27972034972027976), ('device', 0.020979020979020983)] \n",
            "\n",
            "[('organic solar cell screening', 0.5714286414285715), ('ensemble neural networks', 0.42857145857142864)] \n",
            "\n",
            "[('compact semantic segmentation architectures', 0.650406560331913), ('automatic search', 0.3252032801659565), ('template', 0.024390249502130586)] \n",
            "\n",
            "[('3d point cloud generation', 0.5594405794405594), ('continuous normalizing flows', 0.41958048958041955), ('pointflow', 0.020979020979020983)] \n",
            "\n",
            "[('implicit spectral densities', 0.9523809723809525), ('backpropagation', 0.04761904761904763)] \n",
            "\n",
            "[('dynamic distribution adaptation', 0.9523809823809526), ('transfer', 0.04761904761904763)] \n",
            "\n",
            "[('fast waveform generation model', 0.4099533099633461), ('multi - resolution spectrogram', 0.39408880991202816), ('generative adversarial networks', 0.3479228907024837), ('parallel', 0.014778325219929259)] \n",
            "\n",
            "[('continuous dropout', 1.0)] \n",
            "\n",
            "[('modern neural networks', 0.9523809823809526), ('calibration', 0.04761905761904763)] \n",
            "\n",
            "[('excitation networks', 0.9301939638523566), ('squeeze', 0.06980607614764327)] \n",
            "\n",
            "[('talks', 0.50000003), ('suggestions', 0.5)] \n",
            "\n",
            "[('mobile network data analytics', 0.4819274946622921), ('time monitor', 0.24096374733114598), ('accurate real', 0.240963717331146), ('client', 0.018072715337708004), ('falcon', 0.018072625337708002)] \n",
            "\n",
            "[('non consistency', 0.4493875373271151), ('linear predictor', 0.4493874273271151), ('solutions', 0.03374185511525659), ('values', 0.03374180511525659), ('data', 0.03374177511525659)] \n",
            "\n",
            "[('partition averaging', 0.46509703192617835), ('document embeddings', 0.4650970019261783), ('sif', 0.03490305807382164), ('p', 0.034903038073821634)] \n",
            "\n",
            "[('android malware family classification', 0.650406500331913), ('resource consumption', 0.32520331016595655), ('time', 0.024390339502130588)] \n",
            "\n",
            "[('grid tagging scheme', 0.5660377358490563), ('opinion extraction', 0.37735859056603777), ('fine', 0.028301956792452836), ('aspect', 0.028301926792452834)] \n",
            "\n",
            "[('vertex model', 0.46509702192617836), ('stochastic six', 0.4650969919261783), ('asep', 0.034903138073821637), ('convergence', 0.034903038073821634)] \n",
            "\n",
            "[('language generation', 0.8160738719637752), ('biases', 0.06130882267874167), ('babysitter', 0.061308792678741675), ('woman', 0.061308752678741675)] \n",
            "\n",
            "[('stage object detection', 0.9090909690909091), ('convolutional', 0.04545457545454546), ('fcos', 0.04545454545454546)] \n",
            "\n",
            "[('nilpotent orbit', 0.4328890891804843), ('nilpotent matrices', 0.4328889891804843), ('jordan type', 0.3883495645631066), ('maximum', 0.02912629359223301)] \n",
            "\n",
            "[('fully convolutional networks', 0.5000000499999999), ('retinal vessel segmentation', 0.4999999999999999)] \n",
            "\n",
            "[('bidirectional attention flow', 0.6000000000000001), ('machine comprehension', 0.40000004)] \n",
            "\n",
            "[('efficient policy gradient', 0.7187092250288406), ('policy critic', 0.5678532426663802), ('off', 0.03260880565217461), ('sample', 0.03260873565217461), ('prop', 0.032608715652174616), ('q', 0.03260869565217461)] \n",
            "\n",
            "[('predicates', 0.25000008), ('parsing', 0.25000004), ('gapping', 0.25000002), ('sentences', 0.25)] \n",
            "\n",
            "[('sparse neural networks', 0.9523809623809526), ('regularization', 0.04761912761904763)] \n",
            "\n",
            "[('natural language', 0.32519474665675024), ('video collections', 0.3251947166567502), ('temporal localization', 0.32519466665675023), ('moments', 0.024416030029749514)] \n",
            "\n",
            "[('classifier', 0.50000002), ('mapper', 0.5)] \n",
            "\n",
            "[('discrete latent variable', 0.4761905661904762), ('dialogue generation model', 0.4761905261904762), ('-', 0.023809553809523817), ('plato', 0.023809523809523815)] \n",
            "\n",
            "[('biomedical domain', 0.50000008), ('causal precedence', 0.50000004)] \n",
            "\n",
            "[('hermite polynomial activations', 0.29126228592233006), ('avoiding overconfident predictions', 0.2912622459223301), ('supervised learning', 0.19417483728155333), ('accurate pseudo', 0.1941747672815533), ('semi', 0.014563166796116505), ('labels', 0.014563146796116505)] \n",
            "\n",
            "[('non - euclidean geometry', 0.6349201439374402), ('c++ libraries', 0.31746005696872015), ('manipulations', 0.023809994546919844), ('moebinv', 0.023809944546919842)] \n",
            "\n",
            "[('deep convolutional sparse', 0.5825242718446602), ('image fusion', 0.38834957456310665), ('networks', 0.02912625359223301)] \n",
            "\n",
            "[('wavelet convolutional neural networks', 0.6666666666666665), ('texture classification', 0.3333333833333333)] \n",
            "\n",
            "[('stereo videos', 0.4493875273271151), ('optical flow', 0.4493874673271151), ('depth', 0.033741785115256585), ('learning', 0.03374173511525659), ('joint', 0.03374171511525659)] \n",
            "\n",
            "[('conditional variational autoencoders', 0.36809825950920255), ('neural dialog models', 0.3680982195092026), ('level diversity', 0.245398803006135), ('discourse', 0.01840491797546013)] \n",
            "\n",
            "[('deep automatic music transcription', 0.6504065703319131), ('score alignment', 0.3252032901659565), ('audio', 0.024390249502130586)] \n",
            "\n",
            "[('spatiotemporal convolutional sequence', 0.5660377558490562), ('weather forecasting', 0.37735858056603777), ('network', 0.028301956792452836), ('stconvs2s', 0.028301886792452834)] \n",
            "\n",
            "[('critic architecture', 0.9301939538523567), ('option', 0.06980608614764326)] \n",
            "\n",
            "[('3d mri brain tumor segmentation', 0.7142857142857144), ('autoencoder regularization', 0.2857143457142857)] \n",
            "\n",
            "[('vector machines', 0.8160738019637753), ('r', 0.061308832678741675), ('gpu', 0.06130881267874167), ('rgtsvm', 0.06130874267874167)] \n",
            "\n",
            "[('term mobile traffic forecasting', 0.43715848994535544), ('temporal neural networks', 0.3278689524590164), ('deep spatio', 0.21857930497267755), ('long', 0.016393442622950824)] \n",
            "\n",
            "[('deterministic centroid initialisation', 0.410958964109589), ('means variations', 0.273972722739726), ('empirical comparison', 0.273972612739726), ('k', 0.020548045205479454), ('stochastic', 0.020547985205479454)] \n",
            "\n",
            "[('maximum weight online matching', 0.9638554022554806), ('deadlines', 0.03614464774451953)] \n",
            "\n",
            "[('differential functional graphical models', 0.6666666966666666), ('direct estimation', 0.3333333333333333)] \n",
            "\n",
            "[('predictive uncertainty', 0.8694942547232974), ('uncertainty', 0.43474714236164874), ('shift', 0.06525304763835123), ('model', 0.06525295763835123)] \n",
            "\n",
            "[('sketching low degree polynomial kernels', 0.6250000400000001), ('tight dimensionality reduction', 0.37500000000000006)] \n",
            "\n",
            "[('non - convex sgd', 0.650406560331913), ('variance reduction', 0.3252032801659565), ('momentum', 0.024390249502130586)] \n",
            "\n",
            "[('rank gaussian copula processes', 0.5479452854794518), ('dimensional multivariate forecasting', 0.41095892410958895), ('low', 0.020548005205479454), ('high', 0.020547945205479454)] \n",
            "\n",
            "[('eeg brain signals', 0.3278689624590164), ('geometrical shape reconstruction', 0.32786892245901644), ('task generative adversarial', 0.3278688724590164), ('multi', 0.016393442622950824)] \n",
            "\n",
            "[('stochastic computer model', 0.5660377858490563), ('deterministic approximation', 0.37735859056603777), ('available', 0.028302016792452835), ('output', 0.028301906792452834)] \n",
            "\n",
            "[('hard attention models', 0.8695652673912978), ('vision', 0.04347835086956741), ('accuracy', 0.04347829086956741), ('saccader', 0.04347826086956741)] \n",
            "\n",
            "[('stochastic classifiers deterministic', 1.0000000199999999)] \n",
            "\n",
            "[('distribution detection', 0.50000007), ('likelihood ratios', 0.5)] \n",
            "\n",
            "[('sequence neural networks', 0.37500011000000005), ('industrial control systems', 0.37500003000000004), ('anomaly detection', 0.25), ('sequence', 0.09632699419129756)] \n",
            "\n",
            "[('multi - criteria dimensionality reduction', 0.9433962264150946), ('fairness', 0.028301966792452834), ('applications', 0.028301946792452834)] \n",
            "\n",
            "[('shot classification', 0.46509704192617835), ('visual representations', 0.4650969719261783), ('few', 0.03490309807382164), ('language', 0.03490307807382163)] \n",
            "\n",
            "[('rare event simulation', 0.4195804495804195), ('squares regression', 0.27972036972027975), ('variational approach', 0.27972027972027974), ('least', 0.020979090979020985)] \n",
            "\n",
            "[('aerial images', 0.32519474665675024), ('road networks', 0.3251947166567502), ('automatic extraction', 0.3251946866567502), ('roadtracer', 0.024416000029749512)] \n",
            "\n",
            "[('problem', 0.33333341333333333), ('vehicle', 0.33333339333333334), ('heuristics', 0.3333333733333333)] \n",
            "\n",
            "[('probabilistic future frame synthesis', 0.4444444744444443), ('cross convolutional networks', 0.33333341333333333), ('visual dynamics', 0.2222222222222222)] \n",
            "\n",
            "[('reliable capsule networks', 0.5825243018446602), ('nlp applications', 0.38834959456310664), ('scalable', 0.029126223592233012)] \n",
            "\n",
            "[('lipschitz constrained convolutional networks', 0.6666667066666665), ('gradient attenuation', 0.3333333433333333)] \n",
            "\n",
            "[('chinese definition modeling', 0.9523809823809526), ('sememes', 0.04761905761904763)] \n",
            "\n",
            "[('fast approximate natural gradient descent', 0.9433962264150946), ('eigenbasis', 0.028301986792452834), ('kronecker', 0.028301956792452836)] \n",
            "\n",
            "[('gravitational waves', 0.48191613827999996), ('deep learning', 0.48191607827999994), ('parameters', 0.03616788344000001)] \n",
            "\n",
            "[('scalable bayesian deep learning', 0.869398054643053), ('adam', 0.03265060133923674), ('perturbation', 0.03265058133923674), ('weight', 0.03265056133923674), ('fast', 0.03265049133923674)] \n",
            "\n",
            "[('satellite constellation optimization', 0.3000000400000001), ('heterogeneous quantum computing', 0.3000000000000001), ('clique problem', 0.20000013), ('weighted k', 0.20000010000000001)] \n",
            "\n",
            "[('deep face representations', 0.5825243618446602), ('cosine logits', 0.3883495545631066), ('adacos', 0.02912621359223301)] \n",
            "\n",
            "[('vision models', 0.8694942147232975), ('generalization', 0.06525294763835122), ('language', 0.06525292763835122)] \n",
            "\n",
            "[('quantum jumps', 0.8694942247232974), ('duration', 0.06525295763835123), ('upper', 0.06525291763835123)] \n",
            "\n",
            "[('video emotion recognition', 0.41095900410958897), ('integrating micro-', 0.273972632739726), ('mimamo net', 0.273972602739726), ('motion', 0.020548025205479454), ('macro', 0.020548005205479454)] \n",
            "\n",
            "[('subspace attack', 0.4221231462056444), ('box attacks', 0.3252033701659565), ('efficient black', 0.32520334016595653), ('promising subspaces', 0.3252032901659565), ('query', 0.024390319502130588)] \n",
            "\n",
            "[('untrained network priors', 0.4285714885714287), ('inverse imaging', 0.2857143157142857), ('algorithmic guarantees', 0.2857142857142857)] \n",
            "\n",
            "[('convergence rate guarantees', 0.5660378058490563), ('gibbs sampling', 0.37735853056603774), ('minibatching', 0.028301906792452834), ('poisson', 0.028301886792452834)] \n",
            "\n",
            "[('vae models', 1.00000003)] \n",
            "\n",
            "[('realistic eyes', 0.4650970119261783), ('consistent generation', 0.4650969819261783), ('style', 0.03490311807382163), ('content', 0.034903038073821634)] \n",
            "\n",
            "[('rich 3d model repository', 0.9302216617581679), ('information', 0.03488922412091613), ('shapenet', 0.03488919412091613)] \n",
            "\n",
            "[('graph convolutional networks', 0.6000000400000001), ('relational data', 0.40000001)] \n",
            "\n",
            "[('face recognition', 0.46509702192617836), ('unified embedding', 0.4650969919261783), ('clustering', 0.034903128073821635), ('facenet', 0.034903038073821634)] \n",
            "\n",
            "[('cosmic linear anisotropy', 0.5504587255963304), ('approximation schemes', 0.3669725870642202), ('ii', 0.027523025779816523), ('class', 0.027523005779816524), ('system', 0.027522985779816524)] \n",
            "\n",
            "[('dialogue level evaluation', 0.9090909590909092), ('turn', 0.04545457545454546), ('dynaeval', 0.04545454545454546)] \n",
            "\n",
            "[('manhattan room layout reconstruction', 0.4733688330096304), ('art methods', 0.23668461650481523), ('comparative study', 0.23668452650481522), ('state', 0.01775425132691311), ('image', 0.01775419132691311), ('single', 0.01775417132691311)] \n",
            "\n",
            "[('pano stretch data augmentation', 0.43715855994535546), ('learning room layout', 0.3278688724590164), ('1d representation', 0.21857929497267756), ('horizonnet', 0.016393442622950824)] \n",
            "\n",
            "[('anomalous sound detection', 0.5504588255963303), ('machine operating', 0.3669725470642202), ('miniature', 0.027522985779816524), ('dataset', 0.027522965779816524), ('toyadmos', 0.027522935779816522)] \n",
            "\n",
            "[('graph neural networks', 0.5825243218446602), ('generating explanations', 0.3883495345631066), ('gnnexplainer', 0.02912621359223301)] \n",
            "\n",
            "[('variational quantum eigensolvers', 0.6000000300000001), ('collective optimization', 0.4)] \n",
            "\n",
            "[('integrable speech synthesis toolkit', 0.650406560331913), ('fairseq s^2', 0.3252032501659565), ('scalable', 0.024390289502130586)] \n",
            "\n",
            "[('deep exploration', 0.9301939238523567), ('dqn', 0.06980611614764327)] \n",
            "\n",
            "[('new backbone', 0.8160738019637753), ('cnn', 0.06130885267874167), ('capability', 0.061308832678741675), ('cspnet', 0.06130874267874167)] \n",
            "\n",
            "[('semantic question similarity', 0.5660377958490562), ('nsurl-2019 task', 0.37735851056603775), ('arabic', 0.028301986792452834), ('tha3aroon', 0.028301886792452834)] \n",
            "\n",
            "[('deep neural networks', 0.5000000499999999), ('small batch training', 0.5000000099999999)] \n",
            "\n",
            "[('incremental improvement', 0.9301939538523567), ('yolov3', 0.06980607614764327)] \n",
            "\n",
            "[('scale local planar guidance', 0.5369128216778526), ('monocular depth estimation', 0.4026846837583893), ('multi', 0.02013427818791947), ('small', 0.02013425818791947), ('big', 0.02013423818791947)] \n",
            "\n",
            "[('robot learning benchmark', 0.7967343743334878), ('learning environment', 0.6296133369802288), ('rlbench', 0.03614457831325302)] \n",
            "\n",
            "[('nonlinear partial differential equations', 0.606023947883172), ('deep learning', 0.30301192394158594), ('discovery', 0.022741192043810563), ('data', 0.022741162043810565), ('part', 0.022741122043810565), ('physics', 0.022741072043810564)] \n",
            "\n",
            "[('supervised learning', 0.4493874773271151), ('simplifying semi', 0.4493874473271151), ('confidence', 0.03374181511525659), ('consistency', 0.03374179511525659), ('fixmatch', 0.03374171511525659)] \n",
            "\n",
            "[('deep learning', 0.9301939538523567), ('calibration', 0.06980608614764326)] \n",
            "\n",
            "[('graph data structure', 0.42857146857142864), ('parallel mining', 0.2857143657142857), ('blockchain performance', 0.2857142957142857)] \n",
            "\n",
            "[('invariant convolutional layers', 0.6000000500000001), ('learnable scatternet', 0.40000001)] \n",
            "\n",
            "[('pointer networks', 1.0)] \n",
            "\n",
            "[('smart contract vulnerabilities', 0.9999999999999999)] \n",
            "\n",
            "[('student networks', 0.4819161282799999), ('free learning', 0.4819160982799999), ('data', 0.03616784344000001)] \n",
            "\n",
            "[('unlabeled compression', 0.8694941847232974), ('cloud', 0.06525297763835122), ('positive', 0.06525291763835123)] \n",
            "\n",
            "[('other symbolic sequences', 0.5660378058490563), ('finding structure', 0.37735849056603776), ('genome', 0.028301936792452836), ('text', 0.028301916792452836)] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6VgfzUCP2v6"
      },
      "source": [
        "titles = papers_df['paper_titles']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HywzQQzCSSKw"
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "counts = count_vectorizer.fit_transform(titles)\n",
        "tfidf_vectorizer = TfidfTransformer().fit(counts)\n",
        "tfidf_titles = tfidf_vectorizer.transform(counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-48gJ7RSbUj"
      },
      "source": [
        "X = tfidf_titles\n",
        "clustering = AffinityPropagation().fit(X)\n",
        "clustering \n",
        "\n",
        "content_affinity_clusters = list(clustering.labels_)\n",
        "content_affinity_clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3LAdDhBSgO0"
      },
      "source": [
        "papers_df['title_cluster'] = content_affinity_clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDiB5feFSlG-"
      },
      "source": [
        "papers_cluster11 = papers_df.loc[papers_df['title_cluster']==11,['paper_title']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goly4DOUSvRN"
      },
      "source": [
        "dict(sorted(papers_cluster11.values.tolist())) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}